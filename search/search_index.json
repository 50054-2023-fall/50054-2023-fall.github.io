{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Compiler Design and Program Analysis Click here to refer to the course info","title":"Home"},{"location":"#welcome-to-compiler-design-and-program-analysis","text":"Click here to refer to the course info","title":"Welcome to Compiler Design and Program Analysis"},{"location":"advanced_static_analysis/","text":"50.054 - Advanced Topics in Static Analysis Learning Outcomes Apply Path Sensitive Analysis to Sign Analysis. Apply Static Analysis to detect software security loopholes. Recall that sign analysis The Sign Analysis that we developed in the previous class has some limitation. // PA1 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ] 3: ifn t goto 6 // s3 = s2 4: x <- x - 1 // s4 = s3[ x -> s3(x) -- + ] 5: goto 2 // s5 = s4 6: y <- Math.sqrt(x) // s6 = s3 7: r_ret <- y 8: ret The monotonic equations in the comments are defined based on the following lattice. graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] And the abstract value operators are defined as -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) By converting the equation system into monotonic function f1((s0, s1, s2, s3, s4, s5, s6)) = ( [ x -> top, t -> top, input -> top ], s0[ x -> s0(input) ], lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ], s2, s3[ x -> s3(x) -- + ], s4, s3 ) when we apply the fixed point algorithm to the f1 and the VarSign lattice, we have the following solution s0 = [ x -> top, t -> top, input -> top ], s1 = [ x -> top, t -> top, input -> top ], s2 = [ x -> top, t -> top, input -> top ], s3 = [ x -> top, t -> top, input -> top ], s4 = [ x -> top, t -> top, input -> top ], s5 = [ x -> top, t -> top, input -> top ], s6 = [ x -> top, t -> top, input -> top ] At label 6, x 's sign is \\(\\top\\) . Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. Could it be due to the problem of how the abstract operators -- and >== are defined? No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as +0 and -0 Let's say we adjust the lattice mermaid graph A[\"\u22a4\"]---A1[+0] A[\"\u22a4\"]---A2[-0] A2---B[-] A2---C[0] A1---C[0] A1---D[+] B---E C---E D---E[\u22a5] * and the abstract operators -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + + \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) It does not help, as it might give t a more precise abtract value but it does not help to improve the result of x The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition x>=0 and the path of going out of the while loop is only valid under the condition x < 0 . Path sensitive analysis via assertion Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of PA1 in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop. // SIMP2 x = input; while x >= 0 { assert x >= 0; x = x - 1; } assert x < 0; y = Math.sqrt(x); return y; As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions // PA2 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ] 3: ifn t goto 7 // s3 = s2 4: assert x >= 0 // s4 = s3[ x -> gte(s3(x), 0) ] 5: x <- x - 1 // s5 = s4[ x -> s4(x) -- + ] 6: goto 2 // s6 = s5 7: assert x < 0 // s7 = s3[ x -> lt(s3(x), 0) ] 8: y <- Math.sqrt(x) // s8 = s7 9: r_ret <- y 10: ret We could add the following monotonic function synthesis case case \\(l: assert\\ t\\ >=\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\) case \\(l: assert\\ t\\ <\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\) Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs. gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) To show that the above definitions of gte and lt are sound. We can consider the range notation of the abstract values. $$ \\begin{array}{rcl} \\top & = & [-\\infty, +\\infty] \\ +0 & = & [0, +\\infty] \\ -0 & = & [-\\infty, 0] \\ \\ + & = & [1, +\\infty] \\ \\ - & = & [-\\infty, -1] \\ 0 & = & [0, 0] \\ \\bot & = & [+\\infty, -\\infty] \\end{array} $$ \\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\) . \\(\\bot\\) is an empty range. We can think of gte as \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] Similiarly we can think of lt as \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] where \\(+\\infty - 1 = +\\infty\\) With the adjusted monotonic equations, we can now define the monotonic function f2 as follows f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( [ x -> top, t -> top, input -> top ] s0[ x -> s0(input) ], lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ], s2, s3[ x -> gte(s3(x), 0) ], s4[ x -> s4(x) -- + ], s5, s3[ x -> lt(s3(x), 0) ], s7 ) By applying the fixed point algorithm to f2 we find the following solution s0 = [ x -> top, t -> top, input -> top ] s1 = [ x -> top, t -> top, input -> top ] s2 = [ x -> top, t -> +0, input -> top ] s3 = [ x -> top, t -> +0, input -> top ] s4 = [ x -> +0, t -> +0, input -> top ] s5 = [ x -> top, t -> +0, input -> top ] s6 = [ x -> -, t -> +0, input -> top] s7 = [ x -> -, t -> +0, input -> top] which detects that the sign of x at instruction 8 is - . Information Flow Analysis One widely applicable static analysis is information flow analysis. The information flow in a program describes how data are evaluated and propogated in the program via variables and operations. The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds. Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection. High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure. Tainted Flow IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id; try { Statement statement = dbconnection.createStatement(); ResultSet res = statement.executeQuery( query ); // access sensitive resource } The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. When the id is \"' OR 'a'='a'; delete from customer_data; --\" , the malicious user gains the login access and deletes all records from the customer_data table. This can be prevented by using a prepared statement. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\"; try { PreparedStatement pstmt = connection.prepareStatement( query ); pstmt.setString(1, id); // pstmt is sanitized before being used. ResultSet results = pstmt.executeQuery(); } One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple Let's recast the above into SIMP, we would have the vulunerable code as id = input(); query = \"select \" + id; exec(query); return id; We assume that we've extended SIMP to support string values and string concatenation. The input is a function that prompts the user for input. The exec function is a database builtin function. The following version fixed the vulnerability, assume the sanitize function, sanitizes the input. id = input(); query = \"select \"; query = sanitize(query, id) exec(query); return id; To increase the level of compexlity, let's add some control flow to the example. id = input(); query = \"select \" + id; while (id == \"\") { id = input(); query = sanitize(\"select \", id) } exec(query); return id; In the above, it is not directly clear that the exec() is given a sanitized query. The manual check becomes exponentially hard as the code size grows. We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. graph tainted --- clean --- bot(\"\u22a5\") We rewrite the above SIMP program into the following PA equivalent. 1: id <- input() 2: query <- \"select \" + id 3: b <- id == \"\" 4: ifn b goto 5: id <- input() 6: query <- sanitize(\"select\", id) 7: goto 3 8: _ <- exec(query) 9: r_ret <- id 10: ret We define the equation generation rules as follows, \\[ join(s_i) = \\bigsqcup pred(s_i) \\] where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\) case \\(l: t \\leftarrow input()\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\) case \\(l: t \\leftarrow sanitize(src_1, src_2)\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & clean \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] We inline the equations as comments in the PA code. // s0 = [id -> bot, query -> bot, b -> bot] 1: id <- input() // s1 = s0[ id -> tainted ] 2: query <- \"select \" + id // s2 = s1[ query -> lub(clean, s1(id)) ] 3: b <- id == \"\" // s3 = lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ] 4: ifn b goto 8 // s4 = s3 5: id <- input() // s5 = s4[ id -> tainted ] 6: query <- sanitize(\"select\", id) // s6 = s5[ query -> clean ] 7: goto 3 // s7 = s6 8: exec(query) // s8 = s4 9: r_ret <- id // s9 = s8 10: ret By applying the above we have the followning monotonic function. f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = ( [id -> bot, query -> bot, b -> bot], s0[ id -> tainted], s1[ query -> lub(clean, s1(id)) ], lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ], s3, s4[ id -> tainted ], s5[ query -> clean ], s6, s4, s8 ) By applying the fixed point algorithm, we find the following solution for the monotonic equations. s0 = [id -> bot, query -> bot, b -> bot] s1 = [id -> tainted, query -> bot, b -> bot] s2 = [id -> tainted, query -> tainted, b -> bot] s3 = [id -> tainted, query -> tainted, b -> tainted ] s4 = [id -> tainted, query -> tainted, b -> tainted ] s5 = [id -> tainted, query -> tainted, b -> tainted ] s6 = [id -> tainted, query -> clean, b -> tainted ] s7 = [id -> tainted, query -> clean, b -> tainted ] s8 = [id -> tainted, query -> tainted, b -> tainted ] s9 = [id -> tainted, query -> tainted, b -> tainted ] which says that the use of variable query at instruction 8 is risky as query may be tainted at this point. Sensitive Information Disclosure In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); pstmt.setString(1, username); pstmt.setString(2, password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table user is having a lower security level, since it is accessed by the database users. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); String hashed_password = hash(password); // the hash serves as a declassification operation. pstmt.setString(1, username); pstmt.setString(2, hashed_password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password. The hash function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except, We will have a different lattice for security level. graph secret --- confidential --- bot(\"\u22a5\") where secret has a higher level of security than confidential . Instead of using sanitize to increase the level of security, we use hash (or declassify ) to lower the level of security.","title":"50.054 - Advanced Topics in Static Analysis"},{"location":"advanced_static_analysis/#50054-advanced-topics-in-static-analysis","text":"","title":"50.054 - Advanced Topics in Static Analysis"},{"location":"advanced_static_analysis/#learning-outcomes","text":"Apply Path Sensitive Analysis to Sign Analysis. Apply Static Analysis to detect software security loopholes.","title":"Learning Outcomes"},{"location":"advanced_static_analysis/#recall-that-sign-analysis","text":"The Sign Analysis that we developed in the previous class has some limitation. // PA1 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ] 3: ifn t goto 6 // s3 = s2 4: x <- x - 1 // s4 = s3[ x -> s3(x) -- + ] 5: goto 2 // s5 = s4 6: y <- Math.sqrt(x) // s6 = s3 7: r_ret <- y 8: ret The monotonic equations in the comments are defined based on the following lattice. graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] And the abstract value operators are defined as -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) By converting the equation system into monotonic function f1((s0, s1, s2, s3, s4, s5, s6)) = ( [ x -> top, t -> top, input -> top ], s0[ x -> s0(input) ], lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ], s2, s3[ x -> s3(x) -- + ], s4, s3 ) when we apply the fixed point algorithm to the f1 and the VarSign lattice, we have the following solution s0 = [ x -> top, t -> top, input -> top ], s1 = [ x -> top, t -> top, input -> top ], s2 = [ x -> top, t -> top, input -> top ], s3 = [ x -> top, t -> top, input -> top ], s4 = [ x -> top, t -> top, input -> top ], s5 = [ x -> top, t -> top, input -> top ], s6 = [ x -> top, t -> top, input -> top ] At label 6, x 's sign is \\(\\top\\) . Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. Could it be due to the problem of how the abstract operators -- and >== are defined? No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as +0 and -0 Let's say we adjust the lattice mermaid graph A[\"\u22a4\"]---A1[+0] A[\"\u22a4\"]---A2[-0] A2---B[-] A2---C[0] A1---C[0] A1---D[+] B---E C---E D---E[\u22a5] * and the abstract operators -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + + \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) It does not help, as it might give t a more precise abtract value but it does not help to improve the result of x The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition x>=0 and the path of going out of the while loop is only valid under the condition x < 0 .","title":"Recall that sign analysis"},{"location":"advanced_static_analysis/#path-sensitive-analysis-via-assertion","text":"Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of PA1 in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop. // SIMP2 x = input; while x >= 0 { assert x >= 0; x = x - 1; } assert x < 0; y = Math.sqrt(x); return y; As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions // PA2 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ] 3: ifn t goto 7 // s3 = s2 4: assert x >= 0 // s4 = s3[ x -> gte(s3(x), 0) ] 5: x <- x - 1 // s5 = s4[ x -> s4(x) -- + ] 6: goto 2 // s6 = s5 7: assert x < 0 // s7 = s3[ x -> lt(s3(x), 0) ] 8: y <- Math.sqrt(x) // s8 = s7 9: r_ret <- y 10: ret We could add the following monotonic function synthesis case case \\(l: assert\\ t\\ >=\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\) case \\(l: assert\\ t\\ <\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\) Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs. gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) To show that the above definitions of gte and lt are sound. We can consider the range notation of the abstract values. $$ \\begin{array}{rcl} \\top & = & [-\\infty, +\\infty] \\ +0 & = & [0, +\\infty] \\ -0 & = & [-\\infty, 0] \\ \\ + & = & [1, +\\infty] \\ \\ - & = & [-\\infty, -1] \\ 0 & = & [0, 0] \\ \\bot & = & [+\\infty, -\\infty] \\end{array} $$ \\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\) . \\(\\bot\\) is an empty range. We can think of gte as \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] Similiarly we can think of lt as \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] where \\(+\\infty - 1 = +\\infty\\) With the adjusted monotonic equations, we can now define the monotonic function f2 as follows f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( [ x -> top, t -> top, input -> top ] s0[ x -> s0(input) ], lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ], s2, s3[ x -> gte(s3(x), 0) ], s4[ x -> s4(x) -- + ], s5, s3[ x -> lt(s3(x), 0) ], s7 ) By applying the fixed point algorithm to f2 we find the following solution s0 = [ x -> top, t -> top, input -> top ] s1 = [ x -> top, t -> top, input -> top ] s2 = [ x -> top, t -> +0, input -> top ] s3 = [ x -> top, t -> +0, input -> top ] s4 = [ x -> +0, t -> +0, input -> top ] s5 = [ x -> top, t -> +0, input -> top ] s6 = [ x -> -, t -> +0, input -> top] s7 = [ x -> -, t -> +0, input -> top] which detects that the sign of x at instruction 8 is - .","title":"Path sensitive analysis via assertion"},{"location":"advanced_static_analysis/#information-flow-analysis","text":"One widely applicable static analysis is information flow analysis. The information flow in a program describes how data are evaluated and propogated in the program via variables and operations. The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds. Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection. High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure.","title":"Information Flow Analysis"},{"location":"advanced_static_analysis/#tainted-flow","text":"IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id; try { Statement statement = dbconnection.createStatement(); ResultSet res = statement.executeQuery( query ); // access sensitive resource } The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. When the id is \"' OR 'a'='a'; delete from customer_data; --\" , the malicious user gains the login access and deletes all records from the customer_data table. This can be prevented by using a prepared statement. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\"; try { PreparedStatement pstmt = connection.prepareStatement( query ); pstmt.setString(1, id); // pstmt is sanitized before being used. ResultSet results = pstmt.executeQuery(); } One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple Let's recast the above into SIMP, we would have the vulunerable code as id = input(); query = \"select \" + id; exec(query); return id; We assume that we've extended SIMP to support string values and string concatenation. The input is a function that prompts the user for input. The exec function is a database builtin function. The following version fixed the vulnerability, assume the sanitize function, sanitizes the input. id = input(); query = \"select \"; query = sanitize(query, id) exec(query); return id; To increase the level of compexlity, let's add some control flow to the example. id = input(); query = \"select \" + id; while (id == \"\") { id = input(); query = sanitize(\"select \", id) } exec(query); return id; In the above, it is not directly clear that the exec() is given a sanitized query. The manual check becomes exponentially hard as the code size grows. We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. graph tainted --- clean --- bot(\"\u22a5\") We rewrite the above SIMP program into the following PA equivalent. 1: id <- input() 2: query <- \"select \" + id 3: b <- id == \"\" 4: ifn b goto 5: id <- input() 6: query <- sanitize(\"select\", id) 7: goto 3 8: _ <- exec(query) 9: r_ret <- id 10: ret We define the equation generation rules as follows, \\[ join(s_i) = \\bigsqcup pred(s_i) \\] where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\) case \\(l: t \\leftarrow input()\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\) case \\(l: t \\leftarrow sanitize(src_1, src_2)\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & clean \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] We inline the equations as comments in the PA code. // s0 = [id -> bot, query -> bot, b -> bot] 1: id <- input() // s1 = s0[ id -> tainted ] 2: query <- \"select \" + id // s2 = s1[ query -> lub(clean, s1(id)) ] 3: b <- id == \"\" // s3 = lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ] 4: ifn b goto 8 // s4 = s3 5: id <- input() // s5 = s4[ id -> tainted ] 6: query <- sanitize(\"select\", id) // s6 = s5[ query -> clean ] 7: goto 3 // s7 = s6 8: exec(query) // s8 = s4 9: r_ret <- id // s9 = s8 10: ret By applying the above we have the followning monotonic function. f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = ( [id -> bot, query -> bot, b -> bot], s0[ id -> tainted], s1[ query -> lub(clean, s1(id)) ], lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ], s3, s4[ id -> tainted ], s5[ query -> clean ], s6, s4, s8 ) By applying the fixed point algorithm, we find the following solution for the monotonic equations. s0 = [id -> bot, query -> bot, b -> bot] s1 = [id -> tainted, query -> bot, b -> bot] s2 = [id -> tainted, query -> tainted, b -> bot] s3 = [id -> tainted, query -> tainted, b -> tainted ] s4 = [id -> tainted, query -> tainted, b -> tainted ] s5 = [id -> tainted, query -> tainted, b -> tainted ] s6 = [id -> tainted, query -> clean, b -> tainted ] s7 = [id -> tainted, query -> clean, b -> tainted ] s8 = [id -> tainted, query -> tainted, b -> tainted ] s9 = [id -> tainted, query -> tainted, b -> tainted ] which says that the use of variable query at instruction 8 is risky as query may be tainted at this point.","title":"Tainted Flow"},{"location":"advanced_static_analysis/#sensitive-information-disclosure","text":"In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); pstmt.setString(1, username); pstmt.setString(2, password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table user is having a lower security level, since it is accessed by the database users. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); String hashed_password = hash(password); // the hash serves as a declassification operation. pstmt.setString(1, username); pstmt.setString(2, hashed_password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password. The hash function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except, We will have a different lattice for security level. graph secret --- confidential --- bot(\"\u22a5\") where secret has a higher level of security than confidential . Instead of using sanitize to increase the level of security, we use hash (or declassify ) to lower the level of security.","title":"Sensitive Information Disclosure"},{"location":"code_generation/","text":"50.054 - Code Generation Learning Outcomes Name the difference among the target code platforms Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly Handle register spilling Implement the target code generation to JVM bytecode given a Pseudo Assembly Program Recap Compiler Pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C For Target Code Generation, we consider some IR as input, the target code (executable) as the output. Instruction Selection Instruction selection is a process of choosing the target platform on which the language to be executed. There are mainly 3 kinds of target platforms. 3-address instruction RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly 2-address instruction CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86 1-address instruction Stack machine. E.g. JVM Assembly code vs Machine code Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format. 3-address instruction In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction. For instance in 3 address instruction, we have instructions that look like x <- 1 y <- 2 r <- x + y where r , x and y are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, load x 1 load y 2 add r x y The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.) 2-address instruction In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add x and y and store the result in r , we have to write load x 1 load y 2 add x y in the 3rd instruction we add the values stored in registers x and y . The sum will be stored in x . In the last statement, we move the result from x to r . As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex. 1-address instruction In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. For example for the same program, we need t9o encode it in 1-address instruction as follows push 1 push 2 add store r In the first instruction, we push the constant 1 to the left operand register (or the 1st register). In the second instruction, we push the constant 2 to the right oeprand register (the 2nd register). In the 3rd instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2nd register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable r The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex. From PA to 3-address target platform In this section, we consider generating code for a target platform that using 3-address instruciton. Register Allocation Problem Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register rret . Such an assumption is no longer valid in the code generation phase. We face two major constraints. Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation. The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation. For example, the following PA program // PA1 1: x <- inpput 2: y <- x + 1 3: z <- y + 1 4: w <- y * z 5: rret <- w 6: ret has to be translated into 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r3 <- r1 * r2 5: rret <- r3 6: ret assuming we have 4 other registers r0 , r1 , r2 and r3 , besides rret . We can map the PA variables {x : r0, y : r1, z : r2, w : r3} When we only have 3 other registers excluding rret we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: x <- r0 5: r0 <- r1 * r2 6: rret <- r0 7: ret The above program will work within the hardware constraint (3 extra registers besides rret ). Now the register allocation, {x : r0, y : r1, z : r2} is only valid for instructions 1-4 and the alloction for instructions 5-7 is {w : r0, y : r1, z: r2} . As we can argue, we could avoid the offloading by mapping w to rret since it is the one being retured. 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: rret <- r1 * r2 5: ret However this option is not always possible, as the following the w might not be returned variable in some other examples. We could also avoid the offloading by exploiting the liveness analysis, that x is not live from instruction 3 onwards, hence we should not even save the result of r0 to the temporary variable x . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r0 <- r1 * r2 5: rret <- r0 6: ret However this option is not always possible, as in some other situation x is needed later. The Register Allocation Problem is then define as follows. Given a program \\(p\\) , and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized. Interference Graph To solve the register allocation problem, we define a data structure called the interference graph. Two temporary variables are interferring each other when they are both \"live\" at the same time in a program. In the following we include the liveness analysis result as the comments in the program PA1 . // PA1 1: x <- inpput // {input} 2: y <- x + 1 // {x} 3: z <- y + 1 // {y} 4: w <- y * z // {y,z} 5: rret <- w // {w} 6: ret // {} We conclude that y and z are interfering each other. Hence they should not be sharing the same register. graph TD; input x y --- z w From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the rret register. For example we annotate the graph with the mapped registers r0 and r1 graph TD; input[\"input(r0)\"] x[\"x(r0)\"] y[\"y(r0)\"] --- z[\"z(r1)\"] w[\"w(r0)\"] And we can generate the following output 1: r0 <- inpput 2: r0 <- r0 + 1 3: r1 <- r0 + 1 4: r0 <- r0 * r1 5: rret <- r0 6: ret Graph Coloring Problem From the above example, we find that we can recast the register allocation problem into a graph coloring problem. The graph coloring problem is defined as follows. Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. Unfortunately, this problem is NP-complete in general. No efficient algorithm is known. Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists. Chordal Graph A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n > 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle. For example, the following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 v2 --- v4 is chordal, because of \\((v_2,v_4)\\) . The following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 is not chordal, or chordless . It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time. An Example Consider the following PA program with the variable liveness result as comments // PA2 1: a <- 0 // {} 2: b <- 1 // {a} 3: c <- a + b // {a, b} 4: d <- b + c // {b, c} 5: a <- c + d // {c, d} 6: e <- 2 // {a} 7: d <- a + e // {a, e} 8: r_ret <- e + d // {e, d} 9: ret We observe the interference graph graph TD a --- b --- c --- d a --- e --- d and find that it is chordless. SSA saves the day! With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph. For example, if we apply SSA conversion to PA2 We have the following // PA_SSA2 1: a1 <- 0 // {} 2: b1 <- 1 // {a1} 3: c1 <- a1 + b1 // {a1, b1} 4: d1 <- b1 + c1 // {b1, c1} 5: a2 <- c1 + d1 // {c1, d1} 6: e1 <- 2 // {a2} 7: d2 <- a2 + e1 // {a2, e1} 8: r_ret <- e1 + d2 // {e1, d2} 9: ret The liveness analysis algorithm can be adapted to SSA with the following adjustment. We define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j) \\] where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\) . \\[ \\begin{array}{rcl} \\Theta_{i,j} & = & \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] The monotonic functions can be defined by the following cases. case \\(l: \\overline{\\phi}\\ ret\\) , \\(s_l = \\{\\}\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) Now the interference graph of the PA_SSA2 is as follows graph TD; a1 --- b1 --- c1 --- d1 a2 --- e1 --- d2 which is chordal. Coloring Interference Graph generated from SSA According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order. In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.) Therefore we can color the above graph as follows, graph TD; a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\") a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\") From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form. Given that the program interference graph is chordal, the register allocation can be computed in polymomial type. Instead of using building the interference graph, we consider using the live range table of an SSA program, In the following table (of PA_SSA2 ), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An * in a cell (x, l) represent variable x is live at program location l . var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 At any point, (any column), the number of * denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is 2 (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling. Register Spilling However register spilling is avoidable due to program complexity and limit of hardware. Let's consider another example // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x} 3: z <- x * x // {x,y} 4: w <- y * x // {x,y,z} 5: u <- z + w // {z,w} 6: r_ret <- u // {u} 7: ret // {} The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis. var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * From the live range table able, we find that at peak i.e. instruction 4 , there are 3 live variables currently. We would need three registers for the allocation. What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction 4 , by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here. Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point. Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live. // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x(3)} 3: z <- x * x // {x(3),y(4)} 4: w <- y * x // {x(4),y(4),z(5)} 5: u <- z + w // {z(5),w(5)} 6: r_ret <- u // {u(6)} 7: ret // {} From the above results, we can conclude that at instruction 4 , we should sacrifice the live variable z , because z is marked live at label 5 which is needed in the instruction one-hop away in the CFG, compared to x and y which are marked live at label 4 . In other words, z is not as urgently needed compared to x and y . var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label 3 , variable is z is some register, either r0 or r1 , assuming in the target code operation * can use the same register for both operands and the result. We encounter another problem. To spill z (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 3: ?? <- r0 * r0 // what register should hold the result of x * x, before spilling it to `z`? where the comments indicate what happens after the label instruction is excuted. There are two option here ?? is r1 . It implies that we need to spill r1 to y first after instruction 2 and then spill r1 to z after instruction 3 , and load y back to r1 after instruction 3 before instruction 4. ?? is r0 . It implies that we need to spill r0 to z first after instruction 2 and then spill r0 to z after instruction 3 , and load x back to r0 after instruction 3 before instruction 4. In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling ( z in this example) is not needed until then. Now let's say we pick the first option, the register allocation continues var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 where - indicates taht z is being spilled from r1 before label 4 and it needs to be loaded back to r1 before label 5 . And the complete code of PA3_REG is as follows // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 y <- r1 // temporarily save y 3: r1 <- r0 * r0 // z is r1 z <- r1 // spill to z r1 <- y // y is r1 4: r0 <- r1 * r0 // w is r0 (x,y are dead afterwards) r1 <- z // z is r1 5: r1 <- r1 + r0 // u is r1 (z,w are dead afterwards) 6: r_ret <- r1 7: ret In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case). As an exercise, work out what if we save x temporarily instead of y at label 2 . Register allocation for phi assignments What remains to address is the treatment of the phi assignments. Let's consider a slightly bigger example. // PA4 1: x <- input // {input} 2: s <- 0 // {x} 3: c <- 0 // {s,x} 4: b <- c < x // {c,s,x} 5: ifn b goto 9 // {b,c,s,x} 6: s <- c + s // {c,s,x} 7: c <- c + 1 // {c,s,x} 8: goto 4 // {c,s,x} 9: r_ret <- s // {s} 10: ret // {} In the above we find a sum program with liveness analysis results included as comments. Let's convert it into SSA. // PA_SSA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(4),x1(4)} 4: c2 <- phi(3:c1, 8:c3) s2 <- phi(3:s1, 8:s3) b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(4),x1(4)} 8: goto 4 // {c3(4),s3(4),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} We put the liveness analysis results as comments. There are a few options of handling phi assignments. Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code Ensure the variables in the phi assignments sharing the same registers. Let's consider the first approach Conservative approach When we translate the SSA back to PA // PA_SSA_PA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(3.1),x1(4)} 3.1: c2 <- c1 s2 <- s1 // {s1(3.1),x1(4),c1(3.1)} 4: b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(7.1),x1(4)} 7.1: c2 <- c3 s2 <- s3 // {s3(7.1),x1(4),c3(7.1)} 8: goto 4 // {c2(4),s2(6,9),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers. var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 At the peak of the live variables, i.e. instruction 5 , we realize that x1 is live but not urgently needed until 4 which is 5-hop away from the current location. Hence we spill it from register r1 to the temporary variable to free up r1 . Registers are allocated by the next available in round-robin manner. // PA4_REG1 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r2 <- r0 + r2 // s3 is r2 7: r0 <- r0 + 1 // c3 is r0 // c2 is r0 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- r2 // 10: ret // What if at instruction 7 , we allocate r1 to s3 instead of r2 ? Thanks to some indeterminism, we could have a slightly different register allocation as follows var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 // PA4_REG2 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r1 <- r0 + r2 // s3 is r1 7: r2 <- r0 + 1 // c3 is r2 7.1: r0 <- r2 // c2 is r0 r2 <- r1 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- s2 10: ret In this case we have to introduce some additional register shuffling at 7.1 . Compared to PA4_REG1 , this result is less efficient. Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. What we could construct the live range table as follow. var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 Although from the above we find c1 seems to be always dead, but it is not, because its value is merged into c2 in label 4 . This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level. We also take note we want to c1 and c3 to share the same register, and s1 and s3 to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach PA4_REG1 . Note that this approach is not guanranteed to produce more efficient results than the conversvative approach. Summary so far To sum up the code generation process from PA to 3-address target could be carried out as follows, Convert the PA program into a SSA. Perform Liveness Analysis on the SSA. Generate the live range table based on the liveness analysis results. Allocate registers based on the live range table. Detect potential spilling. Depending on the last approach, either convert SSA back to PA and generate the target code according to the live range table, or generate the target code from SSA with register coalesced for the phi assignment operands. Further Reading for SSA-based Register Allocation https://compilers.cs.uni-saarland.de/papers/ssara.pdf https://dl.acm.org/doi/10.1145/512529.512534 JVM bytecode (reduced set) In this section, we consider the generated JVM codes from PA. \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) & jis & ::= & [] \\mid ji\\ jis\\\\ (\\tt JVM\\ Instruction) & ji & ::= & ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\ & & & \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) & n & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt constant) & c & ::= & -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767 \\end{array} \\] As mentioned, JVM has 3 registers a register for the first operand and result a register for the second operand a register for controlling the state of the stack operation (we can't used.) Technically speaking we only have 2 registers. An Example of JVM byte codes is illustrated as follows Supposed we have a PA program as follows, 1: x <- input 2: s <- 0 3: c <- 0 4: b <- c < x 5: ifn b goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: _ret_r <- s 10: ret For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as input to 1 , x to 2 , s to 3 , c to 4 (and b to 5 , though b is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. 4 to l1 and 9 to l2 . iload 1 // push the content of input to register 0 istore 2 // pop register 0's content to x, sipush 0 // push the value 0 to register 0 istore 3 // pop register 0 to s sipush 0 // push the value 0 to register 0 istore 4 // pop register 0 to c ilabel l1 // mark label l1 iload 4 // push the content of c to register 0 iload 2 // push the content of x to register 1 if_icmpge l2 // if register 0 >= register 1 jump, // regardless of the comparison pop both registers iload 4 // push the content of c to register 0 iload 3 // push the content of s to register 1 iadd // sum up the r0 and r1 and result remains in register 0 istore 3 // pop register 0 to s iload 4 // push the content of c to register 0 sipush 1 // push a constant 1 to register 1 iadd istore 4 // pop register 0 to c igoto l1 ilabel l2 iload 3 // push the content of s to register 0 ireturn JVM bytecode operational semantics To describe the operational semantics of JVM bytecodes, we define the following meta symbols. \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) & J & \\subseteq & jis \\\\ (\\tt JVM\\ Environment) & \\Delta & \\subseteq & n \\times c \\\\ (\\tt JVM\\ Stack) & S & = & \\_,\\_ \\mid c,\\_ \\mid c,c \\end{array} \\] An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom ( \\(r_0\\) ) and the right slot is the top ( \\(r_1\\) ). \\(\\_\\) denotes that a slot is vacant. We can decribe the operational semantics of JVM byte codes using the follow rule form \\[ J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] \\(J\\) is the entire program, it is required when we process jumps and conditional jump, the rule rewrites a configuration \\((L\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\) , where \\(\\Delta\\) and \\(\\Delta'\\) are the local environments, \\(S\\) and \\(S'\\) are the stacks, \\(jis\\) and \\(jis'\\) are the currrent and next set of instructions to be processed. $$ \\begin{array}{rc} (\\tt sjLoad1) & J \\vdash (\\Delta, _, _, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), _, jis) \\ \\ (\\tt sjLoad2) & J \\vdash (\\Delta, c, _, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\ \\ (\\tt sjPush1) & J \\vdash (\\Delta, _, _, sipush\\ c;jis) \\longrightarrow (\\Delta, c, _, jis) \\ \\ (\\tt sjPush2) & J \\vdash (\\Delta, c_0, _, sipush\\ c_2;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} $$ The rules \\((\\tt sjLoad1)\\) and \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers. The rules \\((\\tt sjPush1)\\) and \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. \\[ \\begin{array}{rc} (\\tt sjLabel) & J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation. \\[ \\begin{array}{rc} (\\tt sjStore) & J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} (\\tt sjAdd) & J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\ (\\tt sjSub) & J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\ (\\tt sjMul) & J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis) \\end{array} \\] The rules \\((\\tt sjAdd)\\) , \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty. \\[ \\begin{array}{rc} (\\tt sjGoto) & J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\ (\\tt sjCmpNE1) & \\begin{array}{c} c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) & \\begin{array}{c} c_0 = c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ (\\tt sjCmpGE1) & \\begin{array}{c} c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) & \\begin{array}{c} c_0 \\lt c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ \\end{array} \\] The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\) . Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) & = & error \\\\ codeAfterLabel(ilabel\\ l;jis, l') & = & \\left \\{ \\begin{array}{lc} jis & l == l' \\\\ codeAfterLabel(jis, l') & {\\tt otherwise} \\end{array} \\right . \\\\ codeAfterLabel(ji; jis, l) & = & codeAfterLabel(jis, l) \\end{array} \\] The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\) . The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 \\lt c_1\\) . Conversion from PA to JVM bytecodes A simple conversion from PA to JVM bytecodes can be described using the following deduction system. Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels. We have three types of rules. \\(M, L \\vdash lis \\Rightarrow jis\\) , convert a sequence of PA labeled isntructions to a sequence of JVM bytecode instructions. \\(M \\vdash s \\Rightarrow jis\\) , convert a PA operand into a sequence of JVM bytecode instructions. \\(L \\vdash l \\Rightarrow jis\\) , convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton. Converting PA labeled instructions $$ \\begin{array}{rl} {\\tt (jMove)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\ \\hline M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2 \\end{array} \\ \\end{array} $$ The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection. \\[ \\begin{array}{rl} {\\tt (jEq)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3 \\end{array} \\\\ \\\\ {\\tt (jLThan)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 < s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions. \\[ \\begin{array}{rl} {\\tt (jAdd)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jSub)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jMul)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jAdd)\\) , \\((\\tt jSub)\\) and \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM. \\[ \\begin{array}{rl} {\\tt (jGoto)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M,L \\vdash lis \\Rightarrow jis_1 \\\\ \\hline M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jReturn)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\ \\hline M, L \\vdash l_1:rret \\leftarrow s; l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn] \\end{array} \\\\ \\end{array} \\] The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial. Converting PA Operands \\[ \\begin{array}{rl} {\\tt (jConst)} & M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\ {\\tt (jVar)} & M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\ \\end{array} \\] Converting PA Labels \\[ \\begin{array}{rl} {\\tt (jLabel1)} & \\begin{array}{c} l \\not \\in L \\\\ \\hline L \\vdash l \\Rightarrow [] \\end{array} \\\\ \\\\ {\\tt (jLabel2)} & \\begin{array}{c} l \\in L \\\\ \\hline L \\vdash l \\Rightarrow [ilabel\\ l] \\end{array} \\end{array} \\] Optimizing JVM bytecode Though it is limited, there is room to opimize the JVM bytecode. For example, From the following SIMP program r = (1 + 2) * 3 we generate the following PA code via the Maximal Munch 1: t <- 1 + 2 2: r <- t * 3 In turn if we apply the above PA to JVM bytecode conversion sipush 1 sipush 2 iadd istore 2 // 2 is t iload 2 sipush 3 imul istore 3 // 3 is r As observe, the istore 2 followed by iload 2 are rundandant, because t is not needed later (dead). sipush 1 sipush 2 iadd sipush 3 imul istore 3 // 3 is r This can either be done via Liveness analysis on PA level or Generate JVM byte code directly from SIMP. This requires the expression of SIMP assignment to be left nested. The conversion is beyond the scope of this module. Further Reading for JVM bytecode generation https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf Summary for JVM bytecode generation To generate JVM bytecode w/o optimization can be done via deduction system To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.","title":"50.054 - Code Generation"},{"location":"code_generation/#50054-code-generation","text":"","title":"50.054 - Code Generation"},{"location":"code_generation/#learning-outcomes","text":"Name the difference among the target code platforms Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly Handle register spilling Implement the target code generation to JVM bytecode given a Pseudo Assembly Program","title":"Learning Outcomes"},{"location":"code_generation/#recap-compiler-pipeline","text":"graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C For Target Code Generation, we consider some IR as input, the target code (executable) as the output.","title":"Recap Compiler Pipeline"},{"location":"code_generation/#instruction-selection","text":"Instruction selection is a process of choosing the target platform on which the language to be executed. There are mainly 3 kinds of target platforms. 3-address instruction RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly 2-address instruction CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86 1-address instruction Stack machine. E.g. JVM","title":"Instruction Selection"},{"location":"code_generation/#assembly-code-vs-machine-code","text":"Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format.","title":"Assembly code vs Machine code"},{"location":"code_generation/#3-address-instruction","text":"In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction. For instance in 3 address instruction, we have instructions that look like x <- 1 y <- 2 r <- x + y where r , x and y are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, load x 1 load y 2 add r x y The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.)","title":"3-address instruction"},{"location":"code_generation/#2-address-instruction","text":"In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add x and y and store the result in r , we have to write load x 1 load y 2 add x y in the 3rd instruction we add the values stored in registers x and y . The sum will be stored in x . In the last statement, we move the result from x to r . As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex.","title":"2-address instruction"},{"location":"code_generation/#1-address-instruction","text":"In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. For example for the same program, we need t9o encode it in 1-address instruction as follows push 1 push 2 add store r In the first instruction, we push the constant 1 to the left operand register (or the 1st register). In the second instruction, we push the constant 2 to the right oeprand register (the 2nd register). In the 3rd instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2nd register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable r The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex.","title":"1-address instruction"},{"location":"code_generation/#from-pa-to-3-address-target-platform","text":"In this section, we consider generating code for a target platform that using 3-address instruciton.","title":"From PA to 3-address target platform"},{"location":"code_generation/#register-allocation-problem","text":"Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register rret . Such an assumption is no longer valid in the code generation phase. We face two major constraints. Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation. The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation. For example, the following PA program // PA1 1: x <- inpput 2: y <- x + 1 3: z <- y + 1 4: w <- y * z 5: rret <- w 6: ret has to be translated into 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r3 <- r1 * r2 5: rret <- r3 6: ret assuming we have 4 other registers r0 , r1 , r2 and r3 , besides rret . We can map the PA variables {x : r0, y : r1, z : r2, w : r3} When we only have 3 other registers excluding rret we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: x <- r0 5: r0 <- r1 * r2 6: rret <- r0 7: ret The above program will work within the hardware constraint (3 extra registers besides rret ). Now the register allocation, {x : r0, y : r1, z : r2} is only valid for instructions 1-4 and the alloction for instructions 5-7 is {w : r0, y : r1, z: r2} . As we can argue, we could avoid the offloading by mapping w to rret since it is the one being retured. 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: rret <- r1 * r2 5: ret However this option is not always possible, as the following the w might not be returned variable in some other examples. We could also avoid the offloading by exploiting the liveness analysis, that x is not live from instruction 3 onwards, hence we should not even save the result of r0 to the temporary variable x . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r0 <- r1 * r2 5: rret <- r0 6: ret However this option is not always possible, as in some other situation x is needed later. The Register Allocation Problem is then define as follows. Given a program \\(p\\) , and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized.","title":"Register Allocation Problem"},{"location":"code_generation/#interference-graph","text":"To solve the register allocation problem, we define a data structure called the interference graph. Two temporary variables are interferring each other when they are both \"live\" at the same time in a program. In the following we include the liveness analysis result as the comments in the program PA1 . // PA1 1: x <- inpput // {input} 2: y <- x + 1 // {x} 3: z <- y + 1 // {y} 4: w <- y * z // {y,z} 5: rret <- w // {w} 6: ret // {} We conclude that y and z are interfering each other. Hence they should not be sharing the same register. graph TD; input x y --- z w From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the rret register. For example we annotate the graph with the mapped registers r0 and r1 graph TD; input[\"input(r0)\"] x[\"x(r0)\"] y[\"y(r0)\"] --- z[\"z(r1)\"] w[\"w(r0)\"] And we can generate the following output 1: r0 <- inpput 2: r0 <- r0 + 1 3: r1 <- r0 + 1 4: r0 <- r0 * r1 5: rret <- r0 6: ret","title":"Interference Graph"},{"location":"code_generation/#graph-coloring-problem","text":"From the above example, we find that we can recast the register allocation problem into a graph coloring problem. The graph coloring problem is defined as follows. Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. Unfortunately, this problem is NP-complete in general. No efficient algorithm is known. Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists.","title":"Graph Coloring Problem"},{"location":"code_generation/#chordal-graph","text":"A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n > 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle. For example, the following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 v2 --- v4 is chordal, because of \\((v_2,v_4)\\) . The following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 is not chordal, or chordless . It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time.","title":"Chordal Graph"},{"location":"code_generation/#an-example","text":"Consider the following PA program with the variable liveness result as comments // PA2 1: a <- 0 // {} 2: b <- 1 // {a} 3: c <- a + b // {a, b} 4: d <- b + c // {b, c} 5: a <- c + d // {c, d} 6: e <- 2 // {a} 7: d <- a + e // {a, e} 8: r_ret <- e + d // {e, d} 9: ret We observe the interference graph graph TD a --- b --- c --- d a --- e --- d and find that it is chordless.","title":"An Example"},{"location":"code_generation/#ssa-saves-the-day","text":"With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph. For example, if we apply SSA conversion to PA2 We have the following // PA_SSA2 1: a1 <- 0 // {} 2: b1 <- 1 // {a1} 3: c1 <- a1 + b1 // {a1, b1} 4: d1 <- b1 + c1 // {b1, c1} 5: a2 <- c1 + d1 // {c1, d1} 6: e1 <- 2 // {a2} 7: d2 <- a2 + e1 // {a2, e1} 8: r_ret <- e1 + d2 // {e1, d2} 9: ret The liveness analysis algorithm can be adapted to SSA with the following adjustment. We define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j) \\] where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\) . \\[ \\begin{array}{rcl} \\Theta_{i,j} & = & \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] The monotonic functions can be defined by the following cases. case \\(l: \\overline{\\phi}\\ ret\\) , \\(s_l = \\{\\}\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) Now the interference graph of the PA_SSA2 is as follows graph TD; a1 --- b1 --- c1 --- d1 a2 --- e1 --- d2 which is chordal.","title":"SSA saves the day!"},{"location":"code_generation/#coloring-interference-graph-generated-from-ssa","text":"According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order. In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.) Therefore we can color the above graph as follows, graph TD; a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\") a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\") From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form. Given that the program interference graph is chordal, the register allocation can be computed in polymomial type. Instead of using building the interference graph, we consider using the live range table of an SSA program, In the following table (of PA_SSA2 ), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An * in a cell (x, l) represent variable x is live at program location l . var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 At any point, (any column), the number of * denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is 2 (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling.","title":"Coloring Interference Graph generated from SSA"},{"location":"code_generation/#register-spilling","text":"However register spilling is avoidable due to program complexity and limit of hardware. Let's consider another example // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x} 3: z <- x * x // {x,y} 4: w <- y * x // {x,y,z} 5: u <- z + w // {z,w} 6: r_ret <- u // {u} 7: ret // {} The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis. var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * From the live range table able, we find that at peak i.e. instruction 4 , there are 3 live variables currently. We would need three registers for the allocation. What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction 4 , by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here. Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point. Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live. // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x(3)} 3: z <- x * x // {x(3),y(4)} 4: w <- y * x // {x(4),y(4),z(5)} 5: u <- z + w // {z(5),w(5)} 6: r_ret <- u // {u(6)} 7: ret // {} From the above results, we can conclude that at instruction 4 , we should sacrifice the live variable z , because z is marked live at label 5 which is needed in the instruction one-hop away in the CFG, compared to x and y which are marked live at label 4 . In other words, z is not as urgently needed compared to x and y . var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label 3 , variable is z is some register, either r0 or r1 , assuming in the target code operation * can use the same register for both operands and the result. We encounter another problem. To spill z (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 3: ?? <- r0 * r0 // what register should hold the result of x * x, before spilling it to `z`? where the comments indicate what happens after the label instruction is excuted. There are two option here ?? is r1 . It implies that we need to spill r1 to y first after instruction 2 and then spill r1 to z after instruction 3 , and load y back to r1 after instruction 3 before instruction 4. ?? is r0 . It implies that we need to spill r0 to z first after instruction 2 and then spill r0 to z after instruction 3 , and load x back to r0 after instruction 3 before instruction 4. In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling ( z in this example) is not needed until then. Now let's say we pick the first option, the register allocation continues var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 where - indicates taht z is being spilled from r1 before label 4 and it needs to be loaded back to r1 before label 5 . And the complete code of PA3_REG is as follows // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 y <- r1 // temporarily save y 3: r1 <- r0 * r0 // z is r1 z <- r1 // spill to z r1 <- y // y is r1 4: r0 <- r1 * r0 // w is r0 (x,y are dead afterwards) r1 <- z // z is r1 5: r1 <- r1 + r0 // u is r1 (z,w are dead afterwards) 6: r_ret <- r1 7: ret In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case). As an exercise, work out what if we save x temporarily instead of y at label 2 .","title":"Register Spilling"},{"location":"code_generation/#register-allocation-for-phi-assignments","text":"What remains to address is the treatment of the phi assignments. Let's consider a slightly bigger example. // PA4 1: x <- input // {input} 2: s <- 0 // {x} 3: c <- 0 // {s,x} 4: b <- c < x // {c,s,x} 5: ifn b goto 9 // {b,c,s,x} 6: s <- c + s // {c,s,x} 7: c <- c + 1 // {c,s,x} 8: goto 4 // {c,s,x} 9: r_ret <- s // {s} 10: ret // {} In the above we find a sum program with liveness analysis results included as comments. Let's convert it into SSA. // PA_SSA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(4),x1(4)} 4: c2 <- phi(3:c1, 8:c3) s2 <- phi(3:s1, 8:s3) b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(4),x1(4)} 8: goto 4 // {c3(4),s3(4),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} We put the liveness analysis results as comments. There are a few options of handling phi assignments. Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code Ensure the variables in the phi assignments sharing the same registers. Let's consider the first approach","title":"Register allocation for phi assignments"},{"location":"code_generation/#conservative-approach","text":"When we translate the SSA back to PA // PA_SSA_PA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(3.1),x1(4)} 3.1: c2 <- c1 s2 <- s1 // {s1(3.1),x1(4),c1(3.1)} 4: b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(7.1),x1(4)} 7.1: c2 <- c3 s2 <- s3 // {s3(7.1),x1(4),c3(7.1)} 8: goto 4 // {c2(4),s2(6,9),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers. var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 At the peak of the live variables, i.e. instruction 5 , we realize that x1 is live but not urgently needed until 4 which is 5-hop away from the current location. Hence we spill it from register r1 to the temporary variable to free up r1 . Registers are allocated by the next available in round-robin manner. // PA4_REG1 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r2 <- r0 + r2 // s3 is r2 7: r0 <- r0 + 1 // c3 is r0 // c2 is r0 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- r2 // 10: ret // What if at instruction 7 , we allocate r1 to s3 instead of r2 ? Thanks to some indeterminism, we could have a slightly different register allocation as follows var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 // PA4_REG2 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r1 <- r0 + r2 // s3 is r1 7: r2 <- r0 + 1 // c3 is r2 7.1: r0 <- r2 // c2 is r0 r2 <- r1 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- s2 10: ret In this case we have to introduce some additional register shuffling at 7.1 . Compared to PA4_REG1 , this result is less efficient.","title":"Conservative approach"},{"location":"code_generation/#register-coalesced-approach-ensure-the-variables-in-the-phi-assignments-sharing-the-same-registers","text":"Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. What we could construct the live range table as follow. var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 Although from the above we find c1 seems to be always dead, but it is not, because its value is merged into c2 in label 4 . This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level. We also take note we want to c1 and c3 to share the same register, and s1 and s3 to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach PA4_REG1 . Note that this approach is not guanranteed to produce more efficient results than the conversvative approach.","title":"Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers"},{"location":"code_generation/#summary-so-far","text":"To sum up the code generation process from PA to 3-address target could be carried out as follows, Convert the PA program into a SSA. Perform Liveness Analysis on the SSA. Generate the live range table based on the liveness analysis results. Allocate registers based on the live range table. Detect potential spilling. Depending on the last approach, either convert SSA back to PA and generate the target code according to the live range table, or generate the target code from SSA with register coalesced for the phi assignment operands.","title":"Summary so far"},{"location":"code_generation/#further-reading-for-ssa-based-register-allocation","text":"https://compilers.cs.uni-saarland.de/papers/ssara.pdf https://dl.acm.org/doi/10.1145/512529.512534","title":"Further Reading for SSA-based Register Allocation"},{"location":"code_generation/#jvm-bytecode-reduced-set","text":"In this section, we consider the generated JVM codes from PA. \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) & jis & ::= & [] \\mid ji\\ jis\\\\ (\\tt JVM\\ Instruction) & ji & ::= & ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\ & & & \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) & n & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt constant) & c & ::= & -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767 \\end{array} \\] As mentioned, JVM has 3 registers a register for the first operand and result a register for the second operand a register for controlling the state of the stack operation (we can't used.) Technically speaking we only have 2 registers. An Example of JVM byte codes is illustrated as follows Supposed we have a PA program as follows, 1: x <- input 2: s <- 0 3: c <- 0 4: b <- c < x 5: ifn b goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: _ret_r <- s 10: ret For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as input to 1 , x to 2 , s to 3 , c to 4 (and b to 5 , though b is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. 4 to l1 and 9 to l2 . iload 1 // push the content of input to register 0 istore 2 // pop register 0's content to x, sipush 0 // push the value 0 to register 0 istore 3 // pop register 0 to s sipush 0 // push the value 0 to register 0 istore 4 // pop register 0 to c ilabel l1 // mark label l1 iload 4 // push the content of c to register 0 iload 2 // push the content of x to register 1 if_icmpge l2 // if register 0 >= register 1 jump, // regardless of the comparison pop both registers iload 4 // push the content of c to register 0 iload 3 // push the content of s to register 1 iadd // sum up the r0 and r1 and result remains in register 0 istore 3 // pop register 0 to s iload 4 // push the content of c to register 0 sipush 1 // push a constant 1 to register 1 iadd istore 4 // pop register 0 to c igoto l1 ilabel l2 iload 3 // push the content of s to register 0 ireturn","title":"JVM bytecode (reduced set)"},{"location":"code_generation/#jvm-bytecode-operational-semantics","text":"To describe the operational semantics of JVM bytecodes, we define the following meta symbols. \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) & J & \\subseteq & jis \\\\ (\\tt JVM\\ Environment) & \\Delta & \\subseteq & n \\times c \\\\ (\\tt JVM\\ Stack) & S & = & \\_,\\_ \\mid c,\\_ \\mid c,c \\end{array} \\] An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom ( \\(r_0\\) ) and the right slot is the top ( \\(r_1\\) ). \\(\\_\\) denotes that a slot is vacant. We can decribe the operational semantics of JVM byte codes using the follow rule form \\[ J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] \\(J\\) is the entire program, it is required when we process jumps and conditional jump, the rule rewrites a configuration \\((L\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\) , where \\(\\Delta\\) and \\(\\Delta'\\) are the local environments, \\(S\\) and \\(S'\\) are the stacks, \\(jis\\) and \\(jis'\\) are the currrent and next set of instructions to be processed. $$ \\begin{array}{rc} (\\tt sjLoad1) & J \\vdash (\\Delta, _, _, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), _, jis) \\ \\ (\\tt sjLoad2) & J \\vdash (\\Delta, c, _, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\ \\ (\\tt sjPush1) & J \\vdash (\\Delta, _, _, sipush\\ c;jis) \\longrightarrow (\\Delta, c, _, jis) \\ \\ (\\tt sjPush2) & J \\vdash (\\Delta, c_0, _, sipush\\ c_2;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} $$ The rules \\((\\tt sjLoad1)\\) and \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers. The rules \\((\\tt sjPush1)\\) and \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. \\[ \\begin{array}{rc} (\\tt sjLabel) & J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation. \\[ \\begin{array}{rc} (\\tt sjStore) & J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} (\\tt sjAdd) & J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\ (\\tt sjSub) & J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\ (\\tt sjMul) & J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis) \\end{array} \\] The rules \\((\\tt sjAdd)\\) , \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty. \\[ \\begin{array}{rc} (\\tt sjGoto) & J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\ (\\tt sjCmpNE1) & \\begin{array}{c} c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) & \\begin{array}{c} c_0 = c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ (\\tt sjCmpGE1) & \\begin{array}{c} c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) & \\begin{array}{c} c_0 \\lt c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ \\end{array} \\] The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\) . Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) & = & error \\\\ codeAfterLabel(ilabel\\ l;jis, l') & = & \\left \\{ \\begin{array}{lc} jis & l == l' \\\\ codeAfterLabel(jis, l') & {\\tt otherwise} \\end{array} \\right . \\\\ codeAfterLabel(ji; jis, l) & = & codeAfterLabel(jis, l) \\end{array} \\] The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\) . The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 \\lt c_1\\) .","title":"JVM bytecode operational semantics"},{"location":"code_generation/#conversion-from-pa-to-jvm-bytecodes","text":"A simple conversion from PA to JVM bytecodes can be described using the following deduction system. Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels. We have three types of rules. \\(M, L \\vdash lis \\Rightarrow jis\\) , convert a sequence of PA labeled isntructions to a sequence of JVM bytecode instructions. \\(M \\vdash s \\Rightarrow jis\\) , convert a PA operand into a sequence of JVM bytecode instructions. \\(L \\vdash l \\Rightarrow jis\\) , convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton.","title":"Conversion from PA to JVM bytecodes"},{"location":"code_generation/#converting-pa-labeled-instructions","text":"$$ \\begin{array}{rl} {\\tt (jMove)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\ \\hline M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2 \\end{array} \\ \\end{array} $$ The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection. \\[ \\begin{array}{rl} {\\tt (jEq)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3 \\end{array} \\\\ \\\\ {\\tt (jLThan)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 < s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions. \\[ \\begin{array}{rl} {\\tt (jAdd)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jSub)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jMul)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jAdd)\\) , \\((\\tt jSub)\\) and \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM. \\[ \\begin{array}{rl} {\\tt (jGoto)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M,L \\vdash lis \\Rightarrow jis_1 \\\\ \\hline M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jReturn)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\ \\hline M, L \\vdash l_1:rret \\leftarrow s; l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn] \\end{array} \\\\ \\end{array} \\] The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial.","title":"Converting PA labeled instructions"},{"location":"code_generation/#converting-pa-operands","text":"\\[ \\begin{array}{rl} {\\tt (jConst)} & M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\ {\\tt (jVar)} & M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\ \\end{array} \\]","title":"Converting PA Operands"},{"location":"code_generation/#converting-pa-labels","text":"\\[ \\begin{array}{rl} {\\tt (jLabel1)} & \\begin{array}{c} l \\not \\in L \\\\ \\hline L \\vdash l \\Rightarrow [] \\end{array} \\\\ \\\\ {\\tt (jLabel2)} & \\begin{array}{c} l \\in L \\\\ \\hline L \\vdash l \\Rightarrow [ilabel\\ l] \\end{array} \\end{array} \\]","title":"Converting PA Labels"},{"location":"code_generation/#optimizing-jvm-bytecode","text":"Though it is limited, there is room to opimize the JVM bytecode. For example, From the following SIMP program r = (1 + 2) * 3 we generate the following PA code via the Maximal Munch 1: t <- 1 + 2 2: r <- t * 3 In turn if we apply the above PA to JVM bytecode conversion sipush 1 sipush 2 iadd istore 2 // 2 is t iload 2 sipush 3 imul istore 3 // 3 is r As observe, the istore 2 followed by iload 2 are rundandant, because t is not needed later (dead). sipush 1 sipush 2 iadd sipush 3 imul istore 3 // 3 is r This can either be done via Liveness analysis on PA level or Generate JVM byte code directly from SIMP. This requires the expression of SIMP assignment to be left nested. The conversion is beyond the scope of this module.","title":"Optimizing JVM bytecode"},{"location":"code_generation/#further-reading-for-jvm-bytecode-generation","text":"https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf","title":"Further Reading for JVM bytecode generation"},{"location":"code_generation/#summary-for-jvm-bytecode-generation","text":"To generate JVM bytecode w/o optimization can be done via deduction system To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.","title":"Summary for JVM bytecode generation"},{"location":"dynamic_semantics/","text":"50.054 - Dynamic Semantics Learning Outcomes Explain the small step operational semantics of a programming language. Explain the big step operational semantics of a programming language. Formalize the run-time behavior of a programming language using small step operational semantics. Formalize the run-time behavior of a programming language using big step operational semantics. Recall that by formalizing the dynamic semantics of a program we are keen to find out How does the program get executed? What does the program compute / return? Operational Semantics Operational Semantics specifies how a program get executed. For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\) -reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\) . The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule. As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed. To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases. Small-Step Operational Semantics of SIMP Let's try to formalize the Operational Semantics of SIMP language, $$ \\begin{array}{rccl} (\\tt SIMP\\ Environment) & \\Delta & \\subseteq & (X \\times c) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\) , i.e. \\(\\{ X \\mid (X,c) \\in \\Delta \\}\\) . We assume for all \\(X \\in dom(\\Delta)\\) , there exists only one entry of \\((X,c) \\in \\Delta\\) . Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\) , an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\) . We define the operational semantics of SIMP with two sets of rules. The first set of rules deal with expression. Small Step Operational Semantics of SIMP Expression The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\) . \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) $$ The ${\\tt (sVar)}$ rule looks up the value of variable $X$ from the memory environment. If the variable is not found, it gets stuck and an error is returned. $$ \\begin{array}{rc} {\\tt (sOp1)} & \\begin{array}{c} \\Delta \\vdash E_1 \\longrightarrow E_1' \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} & \\begin{array}{c} \\Delta \\vdash E_2 \\longrightarrow E_2' \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} & \\begin{array}{c} C_3 = C_1 \\ OP\\ C_2 \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3 \\end{array} \\end{array} \\] The above three rules handle the binary operation expression. \\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step. \\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step. \\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values. \\[ \\begin{array}{rc} {\\tt (sParen1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline \\Delta \\vdash (E) \\longrightarrow (E') \\end{array} \\\\ \\\\ {\\tt (sParen2)} & \\begin{array}{c} \\Delta \\vdash (c) \\longrightarrow c \\end{array} \\end{array} \\] The rules \\({\\tt (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses. Small Step Operational Semantics of SIMP statement The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . The pair of a environment and a statement is called a program configuration. \\[ \\begin{array}{cc} {\\tt (sAssign1)} & \\begin{array}{c} \\Delta\\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, X = E;) \\longrightarrow (\\Delta, X = E';) \\end{array} \\end{array} \\] $$ \\begin{array}{cc} {\\tt (sAssign2)} & \\begin{array}{c} \\Delta' = \\Delta \\oplus (X, C) \\ \\hline (\\Delta, X = C;) \\longrightarrow (\\Delta', nop) \\end{array} \\end{array} $$ The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements. \\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step. \\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\) . The statement of the resulting configuration a \\(nop\\) . \\[ \\begin{array}{cc} {\\tt (sIf1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} & (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} & (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] The rules \\({\\tt (sIf1)}\\) , \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement. \\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step. \\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\) , it proceeds to evaluate the statements in the then clauses. \\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\) , it proceeds to evaluate the statements in the else clauses. \\[ \\begin{array}{cc} {\\tt (sWhile)} & (\\Delta, while\\ E\\ \\{\\overline{S}\\} ) \\longrightarrow (\\Delta, if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement. In the then branch, we unroll the while loop body once followed by the while loop. In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used. \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} & \\begin{array}{c} S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S') \\\\ \\hline (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S}) \\end{array} \\end{array} \\] The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements. \\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\) . \\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\) . It evalues \\(S\\) by one step. For example, {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # using (sSeq) {(input,1)}, x = input ---> # (sAssign1) {(input,1)}, x = 1 ---> # (sAssign2) {(input, 1), (x,1)}, nop ---> {(input,1), (x,1)}, nop; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sNopSeq) {(input,1), (x,1)}, s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,0)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,0)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,0)}, 0 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,0)}, 0 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,0)}, true ---> {(input,1), (x,1), (s,0), (c,0)}, if true { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf2) {(input,1), (x,1), (s,0), (c,0)}, s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, s = c + s ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + s ---> # (sOp1) 0 + s ---> # (sOp2) 0 + 0 ---> # (sOp3) 0 {(input,1), (x,1), (s,0), (c,0)}, s = 0 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,0)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1 ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + 1 ---> # (sOp1) 0 + 1 ---> # (SOp3) 1 {(input,1), (x,1), (s,0), (c,1)}, c = 1 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,1)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,1)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,1)}, 1 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,1)}, 1 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,1)}, false ---> {(input,1), (x,1), (s,0), (c,1)}, if false { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf3) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)} return s; At last the derivation stop at the return statement. We can return the value 0 as result. Big Step Operational Semantics Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program. Big Step Operational Semantics for SIMP expressions We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\) , which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\) . We consider the following three rules $$ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C $$ In case that the expression is a constant, we return the constant itself. \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] In case that the expression is a variable \\(X\\) , we return the value associated with \\(X\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} {\\tt (bOp)} & \\begin{array}{c} \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~ C_1\\ OP\\ C_2 = C_3 \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3 \\end{array} \\end{array} \\] in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values. \\[ \\begin{array}{rc} {\\tt (bParen)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\\\ \\hline \\Delta \\vdash (E) \\Downarrow C \\end{array} \\end{array} \\] the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses. Big Step Operational Semantics for SIMP statements We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\) , which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\) . Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations. We consider the following rules $$ \\begin{array}{rc} {\\tt (bAssign)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\ \\hline (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C) \\end{array} \\end{array} $$ In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment. \\[ \\begin{array}{rc} {\\tt (bIf1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false ~~~~~~ (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2 \\end{array} \\end{array} \\] In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\) , otherwise evaluate \\(\\overline{S_2}\\) . \\[ \\begin{array}{rc} {\\tt (bWhile1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta \\end{array} \\end{array} \\] In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\) , otherwise, we exit the while loop and return the existing memory environment. \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] In case that the statement is a nop statement, there is no change to the memory environment. \\[ \\begin{array}{rc} {\\tt (bSeq)} & \\begin{array}{c} (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta'' \\\\ \\hline (\\Delta, S \\overline{S}) \\Downarrow \\Delta'' \\end{array} \\end{array} \\] In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements. For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively. {(input,1),(x,1)} |- 0 \u21d3 0 (bConst) ---------------------(bAssign) [sub tree 1] {(input,1), (x,1)}, s = 0; {(input,1)} |- input \u21d3 1 (bVar) \u21d3 {(input,1), (x,1), (s,0)} ---------------- (bAssign) -------------------------------------------(bSeq) {(input,1)}, {(input,1), (x,1)}, x = input; s = 0; \u21d3 {(input,1), (x,1)} c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} ---------------------------------------------------------------------------- (bSeq) {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where sub derivation [sub tree 1] is as follows {(input,1), (x,1), (s,0)} [sub tree 2] -------------------- (bReturn) |- 0 \u21d3 0 (bConst) {(input,1), (x,1), (s,0), (c,1)}, return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} --------------------------(bAssign) -------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, {(input,1), (x,1), (s,0), (c,0)}, c = 0; while c < x {s = c + s; c = c + 1;} \u21d3 return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} {(input,1),(x,1),(s,0),(c,0)} ---------------------------------------------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 2] is {(input,1), (x,1), (s,0), (c,0)} |- c \u21d3 0 (bVar) {(input,1), (x,1), (s,0), (c,0)} |- x \u21d3 1 (bVar) 0 < 1 == true [sub tree 3] [sub tree 4] -------------------------------- (bOp) ---------------------------------- (bSeq) {(input,1), (x,1), (s,0), (c,0)} {(input,1), (x,1), (s,0), (c,0)}, |- c < x \u21d3 true s = c + s; c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} ----------------------------------------------------------------------- (bWhile1) {(input,1), (x,1), (s,0), (c,0)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 3] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- s \u21d3 0 (bVar) c + s == 0 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + s \u21d3 0 ------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, s = c + s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 0)} where [sub tree 4] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- 1 \u21d3 1 (bConst) c + 1 == 1 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + 1 \u21d3 1 -------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} [sub tree 5] --------------------------------------------------------------------- (bSeq) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where [sub tree 5] is {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- c \u21d3 1 {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- x \u21d3 1 1 < 1 == false ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 1)} |- c < x \u21d3 false ---------------------------------------------------- (bWhile2) {(input, 1), (x, 1), (s, 0), (c, 1)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} Quick Summary: Small step vs Big Step operational semantics Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating Formal Results We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\) . Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP) Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\) . Proof of this lemma requires some knowledge which will be discussed in the upcoming classes. Operational Semantics of Pseudo Assembly Next we consider the operational semantics of pseudo assembly. Let's define the environments required for the rules. \\[ \\begin{array}{rccl} (\\tt PA\\ Program) & P & \\subseteq & (l \\times li) \\\\ (\\tt PA\\ Environment) & L & \\subseteq & (t \\times c) \\cup (r \\times c) \\end{array} \\] We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values. Small Step Operational Semantics of Pseudo Assembly The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) , which reads, given a PA program \\(P\\) , the current program context \\((L,li)\\) is evaluated to \\((L', li')\\) . Note that we use a memory environment and program label instruction pair to denote a program context. \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) $$ In the ${\\tt (pConst)}$ rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as $c$ and move on to the next instruction. $$ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1)) \\] $${\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) $$ In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction. \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1)) \\end{array} \\end{array} \\] The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment. \\[ \\begin{array}{rc} {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l')) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1)) \\end{array} \\end{array} \\] The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction. \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction. Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error. For example, let \\(P\\) be 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret and \\(input = 1\\) . We have the following derivation P |- {(input,1)}, 1: x <- input ---> # (pTempVar) P |- {(input,1), (x,1)}, 2: s <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0)}, 3: c <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0), (c,0)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---> # (pIfn0) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s <- c + s ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c <- c + 1 ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---> # (pGoto) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---> # (pIfnNot0) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret <- s ---> # (pTempVar) P |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret Formal Results Definition: Consistency of the memory environments Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\) ), iff \\(\\forall (x,v) \\in \\Delta\\) , \\((x,conv(v)) \\in L\\) , and \\(\\forall (y,u) \\in L\\) , \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\) . Lemma: Correctness of the Maximal Munch Algorithm Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\) . Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\) . Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\) Proof Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time. What about big step operational semantics of Pseudo Assembly? As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se. If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.) https://link.springer.com/article/10.1007/BF00264536 Denotational Semantics (Optional Materials) Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit. In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain. Syntactic Domains In many cases, the syntactic domains are defined by the grammar rules. For SIMP program, we have the following syntactic domains. \\(S\\) denotes the domain of all valid single statement \\(E\\) denotes the domain of all valid expressions \\(\\overline{S}\\) denotes the domain of all valid sequence statements \\(OP\\) denotes the domain of all valid operators. \\(C\\) denotes the domain of all constant values. \\(X\\) denotes the domain of all variables. Semantic Domains \\(Int\\) denotes the set of all integers values \\(Bool\\) denotes the set of \\(\\{true, false\\}\\) Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\) . Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\) . Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\) . Denotational Semantics for SIMP expressions The denotational semantics for the SIMP expression is defined by the following semantic functions. Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\) \\[ \\begin{array}{lll} {\\mathbb E}\\llbracket \\cdot \\rrbracket\\ :\\ E &\\rightarrow&\\ \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}\\llbracket X \\rrbracket & = & \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}\\llbracket c \\rrbracket & = & \\lambda\\sigma. c \\\\ {\\mathbb E}\\llbracket E_1\\ OP\\ E_2 \\rrbracket & = &\\lambda\\sigma. {\\mathbb E}\\llbracket E_1\\rrbracket\\sigma\\ \\llbracket OP \\rrbracket\\ {\\mathbb E}\\llbracket E_2\\rrbracket\\sigma\\\\\\ \\end{array} \\] The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value. Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\(\\llbracket + \\rrbracket\\) gives us the sum operation among two integers. Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}\\llbracket E\\rrbracket\\sigma\\) is the short hand for \\(({\\mathbb E}\\llbracket E\\rrbracket)(\\sigma)\\) As we observe, \\({\\mathbb E}\\llbracket \\cdot \\rrbracket\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\) , returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains. Denotational Semantics for SIMP statements To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs. Let \\(\\bot\\) be a special element, called undefined , that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define $$ \\begin{array}{rcl} f \\circ_\\bot g & = & \\lambda \\sigma. \\left [ \\begin{array}{cc} \\bot & g(\\sigma) = \\bot \\ f(g(\\sigma)) & otherwise \\end{array} \\right . \\end{array} $$ which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements. $$ \\begin{array}{lll} {\\mathbb S}\\llbracket \\cdot \\rrbracket : \\overline{S} & \\rightarrow\\ & \\Sigma \\ \\rightarrow \\ \\Sigma \\cup { \\bot } \\ {\\mathbb S} \\llbracket nop \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket return\\ X \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket X = E \\rrbracket& = & \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}\\llbracket E \\rrbracket\\sigma) \\ {\\mathbb S} \\llbracket S \\overline{S} \\rrbracket& = & {\\mathbb S} \\llbracket \\overline{S} \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket S \\rrbracket\\ {\\mathbb S} \\llbracket if \\ E\\ {\\overline{S_1}} \\ else\\ {\\overline{S_2} } \\rrbracket& = & \\lambda\\sigma. \\left [ \\begin{array}{cc} {\\mathbb S} \\llbracket \\overline{S_1} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ {\\mathbb S} \\llbracket \\overline{S_2} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & fix(F) \\ {\\tt where}\\ & & F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence. In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\) . In case of sequence statements, the semantic function returns a \\(\\bot\\) -function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have $$ \\begin{array}{lll} {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & \\lambda\\sigma. \\left { \\begin{array}{cc} ({\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}}\\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} \\llbracket while \\ E\\ \\{\\overline{S}\\} \\rrbracket\\) For example, let \\(\\sigma = \\{ (input, 1)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket x=input; s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma \\\\ = & ({\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket x=input \\rrbracket) (\\sigma) \\\\ = & {\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_1 \\\\ = & ({\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket s=0 \\rrbracket) (\\sigma_1) \\\\ = & {\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_2 \\\\ = & ({\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket c=0 \\rrbracket) (\\sigma_2) \\\\ = & {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_3 \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket \\circ_\\bot{\\mathbb S} \\llbracket s = c + s; c = c + 1; \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket)(\\sigma_4) \\\\ = & {\\mathbb S} \\llbracket return\\ s; \\rrbracket\\sigma_4 \\\\ = & \\sigma_4 \\end{array} \\] where $$ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = { (input,1), (x,1) } \\ \\sigma_2 = \\sigma_1 \\oplus (s,0) = { (input,1), (x,1), (s,0) } \\ \\sigma_3 = \\sigma_2 \\oplus (c,0) = { (input,1), (x,1), (s,0), (c,0) }\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1) = { (input,1), (x,1), (s,0), (c,1) }\\ \\end{array} $$ Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket while\\ input \\{nop;\\}return\\ input; \\rrbracket \\sigma \\\\ = & fix(F) \\sigma \\\\ = & \\bot \\end{array} \\] where \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = true \\\\ \\sigma & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = false \\\\ \\end{array} \\right . \\\\ \\end{array} \\] Since \\({\\mathbb E}\\llbracket input \\rrbracket\\sigma\\) is always \\(true\\) , \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) \\] With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\) . We won't be able to discuss the proof until we look into lattice theory in the upcoming classes. In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function. Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency. Extra readings for denotational semantics https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf https://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf","title":"50.054 - Dynamic Semantics"},{"location":"dynamic_semantics/#50054-dynamic-semantics","text":"","title":"50.054 - Dynamic Semantics"},{"location":"dynamic_semantics/#learning-outcomes","text":"Explain the small step operational semantics of a programming language. Explain the big step operational semantics of a programming language. Formalize the run-time behavior of a programming language using small step operational semantics. Formalize the run-time behavior of a programming language using big step operational semantics. Recall that by formalizing the dynamic semantics of a program we are keen to find out How does the program get executed? What does the program compute / return?","title":"Learning Outcomes"},{"location":"dynamic_semantics/#operational-semantics","text":"Operational Semantics specifies how a program get executed. For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\) -reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\) . The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule. As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed. To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases.","title":"Operational Semantics"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp","text":"Let's try to formalize the Operational Semantics of SIMP language, $$ \\begin{array}{rccl} (\\tt SIMP\\ Environment) & \\Delta & \\subseteq & (X \\times c) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\) , i.e. \\(\\{ X \\mid (X,c) \\in \\Delta \\}\\) . We assume for all \\(X \\in dom(\\Delta)\\) , there exists only one entry of \\((X,c) \\in \\Delta\\) . Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\) , an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\) . We define the operational semantics of SIMP with two sets of rules. The first set of rules deal with expression.","title":"Small-Step Operational Semantics of SIMP"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp-expression","text":"The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\) . \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) $$ The ${\\tt (sVar)}$ rule looks up the value of variable $X$ from the memory environment. If the variable is not found, it gets stuck and an error is returned. $$ \\begin{array}{rc} {\\tt (sOp1)} & \\begin{array}{c} \\Delta \\vdash E_1 \\longrightarrow E_1' \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} & \\begin{array}{c} \\Delta \\vdash E_2 \\longrightarrow E_2' \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} & \\begin{array}{c} C_3 = C_1 \\ OP\\ C_2 \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3 \\end{array} \\end{array} \\] The above three rules handle the binary operation expression. \\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step. \\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step. \\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values. \\[ \\begin{array}{rc} {\\tt (sParen1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline \\Delta \\vdash (E) \\longrightarrow (E') \\end{array} \\\\ \\\\ {\\tt (sParen2)} & \\begin{array}{c} \\Delta \\vdash (c) \\longrightarrow c \\end{array} \\end{array} \\] The rules \\({\\tt (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses.","title":"Small Step Operational Semantics of SIMP Expression"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp-statement","text":"The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . The pair of a environment and a statement is called a program configuration. \\[ \\begin{array}{cc} {\\tt (sAssign1)} & \\begin{array}{c} \\Delta\\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, X = E;) \\longrightarrow (\\Delta, X = E';) \\end{array} \\end{array} \\] $$ \\begin{array}{cc} {\\tt (sAssign2)} & \\begin{array}{c} \\Delta' = \\Delta \\oplus (X, C) \\ \\hline (\\Delta, X = C;) \\longrightarrow (\\Delta', nop) \\end{array} \\end{array} $$ The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements. \\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step. \\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\) . The statement of the resulting configuration a \\(nop\\) . \\[ \\begin{array}{cc} {\\tt (sIf1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} & (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} & (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] The rules \\({\\tt (sIf1)}\\) , \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement. \\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step. \\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\) , it proceeds to evaluate the statements in the then clauses. \\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\) , it proceeds to evaluate the statements in the else clauses. \\[ \\begin{array}{cc} {\\tt (sWhile)} & (\\Delta, while\\ E\\ \\{\\overline{S}\\} ) \\longrightarrow (\\Delta, if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement. In the then branch, we unroll the while loop body once followed by the while loop. In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used. \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} & \\begin{array}{c} S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S') \\\\ \\hline (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S}) \\end{array} \\end{array} \\] The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements. \\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\) . \\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\) . It evalues \\(S\\) by one step. For example, {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # using (sSeq) {(input,1)}, x = input ---> # (sAssign1) {(input,1)}, x = 1 ---> # (sAssign2) {(input, 1), (x,1)}, nop ---> {(input,1), (x,1)}, nop; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sNopSeq) {(input,1), (x,1)}, s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,0)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,0)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,0)}, 0 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,0)}, 0 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,0)}, true ---> {(input,1), (x,1), (s,0), (c,0)}, if true { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf2) {(input,1), (x,1), (s,0), (c,0)}, s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, s = c + s ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + s ---> # (sOp1) 0 + s ---> # (sOp2) 0 + 0 ---> # (sOp3) 0 {(input,1), (x,1), (s,0), (c,0)}, s = 0 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,0)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1 ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + 1 ---> # (sOp1) 0 + 1 ---> # (SOp3) 1 {(input,1), (x,1), (s,0), (c,1)}, c = 1 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,1)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,1)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,1)}, 1 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,1)}, 1 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,1)}, false ---> {(input,1), (x,1), (s,0), (c,1)}, if false { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf3) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)} return s; At last the derivation stop at the return statement. We can return the value 0 as result.","title":"Small Step Operational Semantics of SIMP statement"},{"location":"dynamic_semantics/#big-step-operational-semantics","text":"Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program.","title":"Big Step Operational Semantics"},{"location":"dynamic_semantics/#big-step-operational-semantics-for-simp-expressions","text":"We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\) , which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\) . We consider the following three rules $$ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C $$ In case that the expression is a constant, we return the constant itself. \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] In case that the expression is a variable \\(X\\) , we return the value associated with \\(X\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} {\\tt (bOp)} & \\begin{array}{c} \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~ C_1\\ OP\\ C_2 = C_3 \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3 \\end{array} \\end{array} \\] in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values. \\[ \\begin{array}{rc} {\\tt (bParen)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\\\ \\hline \\Delta \\vdash (E) \\Downarrow C \\end{array} \\end{array} \\] the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses.","title":"Big Step Operational Semantics for SIMP expressions"},{"location":"dynamic_semantics/#big-step-operational-semantics-for-simp-statements","text":"We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\) , which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\) . Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations. We consider the following rules $$ \\begin{array}{rc} {\\tt (bAssign)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\ \\hline (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C) \\end{array} \\end{array} $$ In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment. \\[ \\begin{array}{rc} {\\tt (bIf1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false ~~~~~~ (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2 \\end{array} \\end{array} \\] In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\) , otherwise evaluate \\(\\overline{S_2}\\) . \\[ \\begin{array}{rc} {\\tt (bWhile1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta \\end{array} \\end{array} \\] In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\) , otherwise, we exit the while loop and return the existing memory environment. \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] In case that the statement is a nop statement, there is no change to the memory environment. \\[ \\begin{array}{rc} {\\tt (bSeq)} & \\begin{array}{c} (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta'' \\\\ \\hline (\\Delta, S \\overline{S}) \\Downarrow \\Delta'' \\end{array} \\end{array} \\] In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements. For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively. {(input,1),(x,1)} |- 0 \u21d3 0 (bConst) ---------------------(bAssign) [sub tree 1] {(input,1), (x,1)}, s = 0; {(input,1)} |- input \u21d3 1 (bVar) \u21d3 {(input,1), (x,1), (s,0)} ---------------- (bAssign) -------------------------------------------(bSeq) {(input,1)}, {(input,1), (x,1)}, x = input; s = 0; \u21d3 {(input,1), (x,1)} c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} ---------------------------------------------------------------------------- (bSeq) {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where sub derivation [sub tree 1] is as follows {(input,1), (x,1), (s,0)} [sub tree 2] -------------------- (bReturn) |- 0 \u21d3 0 (bConst) {(input,1), (x,1), (s,0), (c,1)}, return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} --------------------------(bAssign) -------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, {(input,1), (x,1), (s,0), (c,0)}, c = 0; while c < x {s = c + s; c = c + 1;} \u21d3 return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} {(input,1),(x,1),(s,0),(c,0)} ---------------------------------------------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 2] is {(input,1), (x,1), (s,0), (c,0)} |- c \u21d3 0 (bVar) {(input,1), (x,1), (s,0), (c,0)} |- x \u21d3 1 (bVar) 0 < 1 == true [sub tree 3] [sub tree 4] -------------------------------- (bOp) ---------------------------------- (bSeq) {(input,1), (x,1), (s,0), (c,0)} {(input,1), (x,1), (s,0), (c,0)}, |- c < x \u21d3 true s = c + s; c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} ----------------------------------------------------------------------- (bWhile1) {(input,1), (x,1), (s,0), (c,0)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 3] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- s \u21d3 0 (bVar) c + s == 0 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + s \u21d3 0 ------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, s = c + s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 0)} where [sub tree 4] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- 1 \u21d3 1 (bConst) c + 1 == 1 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + 1 \u21d3 1 -------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} [sub tree 5] --------------------------------------------------------------------- (bSeq) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where [sub tree 5] is {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- c \u21d3 1 {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- x \u21d3 1 1 < 1 == false ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 1)} |- c < x \u21d3 false ---------------------------------------------------- (bWhile2) {(input, 1), (x, 1), (s, 0), (c, 1)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}","title":"Big Step Operational Semantics for SIMP statements"},{"location":"dynamic_semantics/#quick-summary-small-step-vs-big-step-operational-semantics","text":"Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating","title":"Quick Summary: Small step vs Big Step operational semantics"},{"location":"dynamic_semantics/#formal-results","text":"We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\) .","title":"Formal Results"},{"location":"dynamic_semantics/#lemma-1-agreement-of-small-step-and-big-step-operational-semantics-of-simp","text":"Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\) . Proof of this lemma requires some knowledge which will be discussed in the upcoming classes.","title":"Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP)"},{"location":"dynamic_semantics/#operational-semantics-of-pseudo-assembly","text":"Next we consider the operational semantics of pseudo assembly. Let's define the environments required for the rules. \\[ \\begin{array}{rccl} (\\tt PA\\ Program) & P & \\subseteq & (l \\times li) \\\\ (\\tt PA\\ Environment) & L & \\subseteq & (t \\times c) \\cup (r \\times c) \\end{array} \\] We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values.","title":"Operational Semantics of Pseudo Assembly"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-pseudo-assembly","text":"The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) , which reads, given a PA program \\(P\\) , the current program context \\((L,li)\\) is evaluated to \\((L', li')\\) . Note that we use a memory environment and program label instruction pair to denote a program context. \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) $$ In the ${\\tt (pConst)}$ rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as $c$ and move on to the next instruction. $$ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1)) \\] $${\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) $$ In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction. \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1)) \\end{array} \\end{array} \\] The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment. \\[ \\begin{array}{rc} {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l')) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1)) \\end{array} \\end{array} \\] The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction. \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction. Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error. For example, let \\(P\\) be 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret and \\(input = 1\\) . We have the following derivation P |- {(input,1)}, 1: x <- input ---> # (pTempVar) P |- {(input,1), (x,1)}, 2: s <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0)}, 3: c <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0), (c,0)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---> # (pIfn0) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s <- c + s ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c <- c + 1 ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---> # (pGoto) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---> # (pIfnNot0) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret <- s ---> # (pTempVar) P |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret","title":"Small Step Operational Semantics of Pseudo Assembly"},{"location":"dynamic_semantics/#formal-results_1","text":"","title":"Formal Results"},{"location":"dynamic_semantics/#definition-consistency-of-the-memory-environments","text":"Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\) ), iff \\(\\forall (x,v) \\in \\Delta\\) , \\((x,conv(v)) \\in L\\) , and \\(\\forall (y,u) \\in L\\) , \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\) .","title":"Definition: Consistency of the memory environments"},{"location":"dynamic_semantics/#lemma-correctness-of-the-maximal-munch-algorithm","text":"Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\) . Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\) . Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\)","title":"Lemma: Correctness of the Maximal Munch Algorithm"},{"location":"dynamic_semantics/#proof","text":"Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time.","title":"Proof"},{"location":"dynamic_semantics/#what-about-big-step-operational-semantics-of-pseudo-assembly","text":"As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se. If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.) https://link.springer.com/article/10.1007/BF00264536","title":"What about big step operational semantics of Pseudo Assembly?"},{"location":"dynamic_semantics/#denotational-semantics-optional-materials","text":"Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit. In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain.","title":"Denotational Semantics (Optional Materials)"},{"location":"dynamic_semantics/#syntactic-domains","text":"In many cases, the syntactic domains are defined by the grammar rules. For SIMP program, we have the following syntactic domains. \\(S\\) denotes the domain of all valid single statement \\(E\\) denotes the domain of all valid expressions \\(\\overline{S}\\) denotes the domain of all valid sequence statements \\(OP\\) denotes the domain of all valid operators. \\(C\\) denotes the domain of all constant values. \\(X\\) denotes the domain of all variables.","title":"Syntactic Domains"},{"location":"dynamic_semantics/#semantic-domains","text":"\\(Int\\) denotes the set of all integers values \\(Bool\\) denotes the set of \\(\\{true, false\\}\\) Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\) . Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\) . Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\) .","title":"Semantic Domains"},{"location":"dynamic_semantics/#denotational-semantics-for-simp-expressions","text":"The denotational semantics for the SIMP expression is defined by the following semantic functions. Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\) \\[ \\begin{array}{lll} {\\mathbb E}\\llbracket \\cdot \\rrbracket\\ :\\ E &\\rightarrow&\\ \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}\\llbracket X \\rrbracket & = & \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}\\llbracket c \\rrbracket & = & \\lambda\\sigma. c \\\\ {\\mathbb E}\\llbracket E_1\\ OP\\ E_2 \\rrbracket & = &\\lambda\\sigma. {\\mathbb E}\\llbracket E_1\\rrbracket\\sigma\\ \\llbracket OP \\rrbracket\\ {\\mathbb E}\\llbracket E_2\\rrbracket\\sigma\\\\\\ \\end{array} \\] The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value. Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\(\\llbracket + \\rrbracket\\) gives us the sum operation among two integers. Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}\\llbracket E\\rrbracket\\sigma\\) is the short hand for \\(({\\mathbb E}\\llbracket E\\rrbracket)(\\sigma)\\) As we observe, \\({\\mathbb E}\\llbracket \\cdot \\rrbracket\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\) , returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains.","title":"Denotational Semantics for SIMP expressions"},{"location":"dynamic_semantics/#denotational-semantics-for-simp-statements","text":"To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs. Let \\(\\bot\\) be a special element, called undefined , that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define $$ \\begin{array}{rcl} f \\circ_\\bot g & = & \\lambda \\sigma. \\left [ \\begin{array}{cc} \\bot & g(\\sigma) = \\bot \\ f(g(\\sigma)) & otherwise \\end{array} \\right . \\end{array} $$ which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements. $$ \\begin{array}{lll} {\\mathbb S}\\llbracket \\cdot \\rrbracket : \\overline{S} & \\rightarrow\\ & \\Sigma \\ \\rightarrow \\ \\Sigma \\cup { \\bot } \\ {\\mathbb S} \\llbracket nop \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket return\\ X \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket X = E \\rrbracket& = & \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}\\llbracket E \\rrbracket\\sigma) \\ {\\mathbb S} \\llbracket S \\overline{S} \\rrbracket& = & {\\mathbb S} \\llbracket \\overline{S} \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket S \\rrbracket\\ {\\mathbb S} \\llbracket if \\ E\\ {\\overline{S_1}} \\ else\\ {\\overline{S_2} } \\rrbracket& = & \\lambda\\sigma. \\left [ \\begin{array}{cc} {\\mathbb S} \\llbracket \\overline{S_1} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ {\\mathbb S} \\llbracket \\overline{S_2} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & fix(F) \\ {\\tt where}\\ & & F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence. In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\) . In case of sequence statements, the semantic function returns a \\(\\bot\\) -function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have $$ \\begin{array}{lll} {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & \\lambda\\sigma. \\left { \\begin{array}{cc} ({\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}}\\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} \\llbracket while \\ E\\ \\{\\overline{S}\\} \\rrbracket\\) For example, let \\(\\sigma = \\{ (input, 1)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket x=input; s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma \\\\ = & ({\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket x=input \\rrbracket) (\\sigma) \\\\ = & {\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_1 \\\\ = & ({\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket s=0 \\rrbracket) (\\sigma_1) \\\\ = & {\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_2 \\\\ = & ({\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket c=0 \\rrbracket) (\\sigma_2) \\\\ = & {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_3 \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket \\circ_\\bot{\\mathbb S} \\llbracket s = c + s; c = c + 1; \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket)(\\sigma_4) \\\\ = & {\\mathbb S} \\llbracket return\\ s; \\rrbracket\\sigma_4 \\\\ = & \\sigma_4 \\end{array} \\] where $$ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = { (input,1), (x,1) } \\ \\sigma_2 = \\sigma_1 \\oplus (s,0) = { (input,1), (x,1), (s,0) } \\ \\sigma_3 = \\sigma_2 \\oplus (c,0) = { (input,1), (x,1), (s,0), (c,0) }\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1) = { (input,1), (x,1), (s,0), (c,1) }\\ \\end{array} $$ Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket while\\ input \\{nop;\\}return\\ input; \\rrbracket \\sigma \\\\ = & fix(F) \\sigma \\\\ = & \\bot \\end{array} \\] where \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = true \\\\ \\sigma & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = false \\\\ \\end{array} \\right . \\\\ \\end{array} \\] Since \\({\\mathbb E}\\llbracket input \\rrbracket\\sigma\\) is always \\(true\\) , \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) \\] With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\) . We won't be able to discuss the proof until we look into lattice theory in the upcoming classes. In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function.","title":"Denotational Semantics for SIMP statements"},{"location":"dynamic_semantics/#denotational-semantics-vs-big-step-operational-semantics-vs-small-step-semantics","text":"support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency.","title":"Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics"},{"location":"dynamic_semantics/#extra-readings-for-denotational-semantics","text":"https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf https://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf","title":"Extra readings for denotational semantics"},{"location":"fp_applicative_monad/","text":"50.054 - Applicative and Monad Learning Outcomes Describe and define derived type class Describe and define Applicative Functors Describe and define Monads Apply Monad to in design and develop highly modular and resusable software. Derived Type Class Recall that in our previous lesson, we talk about the Ordering type class. trait Ordering[A] { def compare(x:A,y:A):Int // less than: -1, equal: 0, greater than 1 } Let's consider a variant called Order (actually it is defined in a popular Scala library named cats ). trait Eq[A] { def eqv(x:A, y:A):Boolean } trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } In the above, the Eq type class is a supertype of the Order type class, because all instances of Order type class should have the method eqv implemented. We also say Order is a derived type class of Eq . Let's consider some instances given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt:Order[Int] = new Order[Int] { def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) An alternative approach trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int // def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt(using eqInt:Eq[Int]):Order[Int] = new Order[Int] { def eqv(x:Int,y:Int):Boolean = eqInt.eqv(x,y) def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) In the above definition, we skip the default implementatoin of eqv in Order and make use of the type class instance context to synthesis the eqv method based on the existing type class instances of Eq . (This approach is closer to the one found in Haskell.) Which one is better? Both have their own pros and cons. In the first approach, we give a default implementation for the eqv overridden method in Order type class, it frees us from re-defining the eqv in every type class instance of Order . In this case, the rule/logic is fixed at the top level. In the second approach, eqv in Order type class is not defined. We are required to define it for every single type class instance of Order , that means more work. The advantage is that we have flexibility to redefine/re-mix definition of eqv coming from other type class instances. Functor (Recap) Recall from the last lesson, we make use of the Functor type class to define generic programming style of data processing. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } enum BTree[+A]{ case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Note that we also swap the first and the second arguments of the map function. Applicative Functor The Applicative Functor is a derived type class of Functor , which is defined as follows trait Applicative[F[_]] extends Functor[F] { def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] def pure[A](a: A): F[A] def map[A, B](fa: F[A])(f: A => B): F[B] = ap(pure(f))(fa) } Note that we \"fix\" the map for Applicative in the type class level in this case. (i.e. we are following the first approach.) given listApplicative:Applicative[List] = new Applicative[List] { def pure[A](a:A):List[A] = List(a) def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.map( f => fa.map(f)).flatten } Recall that flatten flattens a list of lists. Alternatively, we can define the ap method of the Applicative[List] instance flatMap . Given l is a list, l.flatMap(f) is the same as l.map(f).flatten . def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.flatMap( f => fa.map(f)) Recall that Scala compiler desugars expression of shape e1.flatMap( v1 => e2.flatMap( v2 => ... en.map(vn => e ... ))) into for { v1 <- e1 v2 <- e2 ... vn <- en } yield (e) Hence we can rewrite the ap method of the Applicative[List] instance as def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = for { f <- ff a <- fa } yield (f(a)) It is not suprising the following produces the same results as the functor instance. listApplicative.map(l)((x:Int) => x + 1) What about pure and ap ? when can we use them? Let's consider the following contrived example. Suppose we would like to apply two sets of operations to elements from l , each operation will produce its own set of results, and the inputs do not depend on the output of the other set. i.e. If the two set of operations, are (x:Int)=> x+1 and (y:Int)=>y*2 . val intOps= List((x:Int)=>x+1, (y:Int)=>y*2) listApplicative.ap(intOps)(l) we get List(2, 3, 4, 2, 4, 6) as the result. Let's consider another example. Recall that Option[A] algebraic datatype which captures a value of type A could be potentially empty. We define the Applicative[Option] instance as follows given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff match { case None => None case Some(f) => fa match { case None => None case Some(a) => Some(f(a)) } } } In the above Applicative instance, the ap method takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning None . This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value. Recall the builtin Option type is defined as follows, // no need to run this. enum Option[+A] { case None case Some(v) def map[B](f:A=>B):Option[B] = this match { case None => None case Some(v) => Some(f(v)) } def flatMap[B](f:A=>Option[B]):Option[B] = this match { case None => None case Some(v) => f(v) match { case None => None case Some(u) => Some(u) } } } Hence optApplicative can be simplified as given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff.flatMap(f => fa.map(f)) // same as listApplicative } or given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = for { f <- ff a <- fa } yield f(a) // same as listApplicative } Applicative Laws Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable. Identity: ap(pure(x=>x)) \\(\\equiv\\) x=>x Homomorphism: ap(pure(f))(pure(x)) \\(\\equiv\\) pure(f(x)) Interchange: ap(u)(pure(y)) \\(\\equiv\\) ap(pure(f=>f(y)))(u) Composition: ap(ap(ap(pure(f=>f.compose))(u))(v))(w) \\(\\equiv\\) ap(u)(ap(v)(w)) Identity law states that applying a lifted identity function of type A=>A is same as an identity function of type F[A] => F[A] where F is the applicative functor. Homomorphism says that applying a lifted function (which has type A=>A before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result. To understand Interchange law let's consider the following equation $$ u\\ y \\equiv (\\lambda f.(f\\ y))\\ u $$ Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\) . To understand the Composition law, we consider the following equation in lambda calculus \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w & \\longrightarrow_{\\beta} \\\\ (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w & \\longrightarrow_{\\beta} \\\\ (u\\circ v)\\ w & \\longrightarrow_{\\tt composition} \\\\ u\\ (v\\ w) \\end{array} \\] The Composition Law says that the above equation remains valid when \\(u\\) , \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\) . Cohort Exercise show that any applicative functor satisfying the above laws also satisfies the Functor Laws Monad Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor. Let's consider a motivating example. Recall that in the earlier lesson, we came across the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } In which we use Option[A] to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to Either[A,B] if we want to have better error messages. Monad is a good application here. Let's consider the type class definition of Monad[F[_]] . trait Monad[F[_]] extends Applicative[F] { def bind[A,B](fa:F[A])(f:A => F[B]):F[B] def pure[A](v:A):F[A] def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] = bind(ff)((f:A=>B) => bind(fa)((a:A)=> pure(f(a)))) } given optMonad:Monad[Option] = new Monad[Option] { def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } The eval function can be re-expressed using Monad[Option] . def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1+v2)}) }) case MathExp.Minus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1-v2)}) }) case MathExp.Mult(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1*v2)}) }) case MathExp.Div(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => if (v2 == 0) {None} else {m.pure(v1/v2)}}) }) case MathExp.Const(i) => m.pure(i) } It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of for comprehension since Option has the member functions flatMap and map defined. Recall that Scala desugars for {...} yield expression into flatMap and map . Thus the above can be rewritten as def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) if (v2 !=0) } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now the readability is restored. Another advantage of coding with Monad is that its abstraction allows us to switch underlying data structure without major code change. Suppose we would like to use Either[String, A] or some other equivalent as return type of eval function to support better error message. But before that, let's consider some subclasses of the Applicative and the Monad type classes. trait ApplicativeError[F[_], E] extends Applicative[F] { def raiseError[A](e:E):F[A] } trait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] { override def raiseError[A](e:E):F[A] } type ErrMsg = String In the above, we define an extension to the Applicative type class, named ApplicativeError which expects an extra type class parameter E that denotes an error. The raiseError method takes a value of type E and returns the Applicative result. Similarly, we extend Monad type class with MonadError type class. Next we include the following type class instance to include Option as one f the MonadError functor. given optMonadError:MonadError[Option, ErrMsg] = new MonadError[Option, ErrMsg] { def raiseError[A](e:ErrMsg):Option[A] = None def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } Next, we adjust the eval function to takes in a MonadError context instead of a Monad context. In addition, we make the error signal more explicit by calling the raiseError() method from the MonadError type class context. def eval2(e:MathExp)(using m:MonadError[Option, ErrMsg]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now let's try to refactor the code to make use of Either[ErrMsg, A] as the functor instead of Option[A] . enum Either[+A, +B] { case Left(v: A) case Right(v: B) // to support for comprehension def flatMap[C>:A,D](f: B => Either[C,D]):Either[C,D] = this match { case Left(a) => Left(a) case Right(b) => f(b) } def map[D](f:B => D):Either[A,D] = this match { case Right(b) => Right(f(b)) case Left(e) => Left(e) } } In the above, we have to define flatMap and map member functions for Either type so that we could make use of the for comprehension later on. One might argue with the type signature of flatMap should be flatMap[D](f: B => Either[A,D]):Either[A,D] . The issue here is that the type variable A will appear in both co- and contra-variant positions. The top-level annotation +A is no longer true. Hence we \"relax\" the type constraint here by introducing a new type variable C which has a lower bound of A (even though we do not need to upcast the result of the Left alternative.) type EitherErr = [B] =>> Either[ErrMsg,B] In the above we define Either algebraic datatype and the type construcor EitherErr . [B] =>> Either[ErrMsg, B] denotes a type lambda, which means that EitherErr is a type constructor (or type function) that takes a type B and return an Either[ErrMsg, B] type. Next, we define the type class instance for MonadError[EitherErr, ErrMsg] given eitherErrMonad: MonadError[EitherErr, ErrMsg] = new MonadError[EitherErr, ErrMsg] { import Either.* def raiseError[B](e: ErrMsg): EitherErr[B] = Left(e) def bind[A, B]( fa: EitherErr[A] )(f: A => EitherErr[B]): EitherErr[B] = fa match { case Right(b) => f(b) case Left(s) => Left(s) } def pure[B](v: B): EitherErr[B] = Right(v) } And finally, we refactor the eval function by changing its type signature. And its body remains unchanged. def eval3(e:MathExp)(using m:MonadError[EitherErr, ErrMsg]):EitherErr[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Commonly used Monads We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads. List Monad We know that List is a Functor and an Applicative. It is not surprising that List is also a Monad. given listMonad:Monad[List] = new Monad[List] { def pure[A](v:A):List[A] = List(v) def bind[A,B](fa:List[A])(f:A => List[B]):List[B] = fa.flatMap(f) } With the above instance, we can write list processing method in for comprehension which is similar to query languages. import java.util.Date import java.util.Calendar import java.util.GregorianCalendar import java.text.SimpleDateFormat case class Staff(id:Int, dob:Date) def mkStaff(id:Int, dobStr:String):Staff = { val sdf = new SimpleDateFormat(\"yyyy-MM-dd\") val dobDate = sdf.parse(dobStr) Staff(id, dobDate) } val staffData = List( mkStaff(1, \"1076-01-02\"), mkStaff(2, \"1986-07-24\") ) def ageBelow(staff:Staff, age:Int): Boolean = staff match { case Staff(id, dob) => { val today = new Date() val calendar = new GregorianCalendar(); calendar.setTime(today) calendar.add(Calendar.YEAR, -age) val ageYearsAgo = calendar.getTime() dob.after(ageYearsAgo) } } def query(data:List[Staff]):List[Staff] = for { staff <- data // from data if ageBelow(staff, 40) // where staff.age < 40 } yield staff // select * Reader Monad Next we consider the Reader Monad. Reader Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable. For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment. case class Reader[R, A] (run: R=>A) { // we need flatMap and map for for-comprehension def flatMap[B](f:A =>Reader[R,B]):Reader[R,B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) match { case Reader(rb) => rb(r) } ) } def map[B](f:A=>B):Reader[R, B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) ) } } type ReaderM = [R] =>> [A] =>> Reader[R, A] trait ReaderMonad[R] extends Monad[ReaderM[R]] { override def pure[A](v:A):Reader[R, A] = Reader (r => v) override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa match { case Reader(ra) => Reader ( r=> f(ra(r)) match { case Reader(rb) => rb(r) } ) } def ask:Reader[R,R] = Reader( r => r) def local[A](f:R=>R)(r:Reader[R,A]):Reader[R,A] = r match { case Reader(ra) => Reader( r => { val localR = f(r) ra(localR) }) } } In the above Reader[R,A] case class defines the structure of the Reader type, where R denotes the shared information for the computation, (source for reader), A denotes the output of the computation. We would like to define Reader[R,_] as a Monad instance. To do so, we define a type-curry version of Reader , i.e. ReaderM . One crucial observation is that bind method in ReaderMonad is nearly identical to flatMap in Reader , with the arguments swapped. In fact, we can re-express bind for all Monads as the flatMap in their underlying case class. override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa.flatMap(f) The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input https://127.0.0.1/ ). For authentication we need to call the authentication server https://127.0.0.10/ temporarily. case class API(url:String) given APIReader:ReaderMonad[API] = new ReaderMonad[API] {} def get(path:String)(using pr:ReaderMonad[API]):Reader[API,Unit] = for { r <- pr.ask s <- r match { case API(url) => pr.pure(println(s\"${url}${path}\")) } } yield s def authServer(api:API):API = API(\"https://127.0.0.10/\") def test1(using pr:ReaderMonad[API]):Reader[API, Unit] = for { a <- pr.local(authServer)(get(\"auth\")) t <- get(\"time\") j <- get(\"job\") } yield (()) def runtest1():Unit = test1 match { case Reader(run) => run(API(\"https://127.0.0.1/\")) } State Monad We consider the State Monad. A State Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like Scala, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging. The following we define a State case class, which has a member computation run:S => (S,A) . case class State[S,A]( run:S=>(S,A)) { def flatMap[B](f: A => State[S,B]):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1,a) => f(a) match { case State(ssb) => ssb(s1) } } ) } def map[B](f:A => B):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1, a) => (s1, f(a)) } ) } } As suggested by the type, the computationn S=>(S,A) , takes in a state S as input and return a tuple of output, consists a new state and the result of the computation. The State Monad type class is defined as a dervied type class of Monad[StateM[S]] . type StateM = [S] =>> [A] =>> State[S,A] trait StateMonad[S] extends Monad[StateM[S]] { override def pure[A](v:A):State[S,A] = State( s=> (s,v)) override def bind[A,B]( fa:State[S,A] )( ff:A => State[S,B] ):State[S,B] = fa.flatMap(ff) def get:State[S, S] = State(s => (s,s)) def set(v:S):State[S,Unit] = State(s => (v,())) } In the pure method's default implementation, we takes a value v of type A and return a State case class oject by wrapping a lambda which takes a state s and returns back the same state s with the input value v . In the default implementation of the bind method, we take a computation fa of type State[S,A] , i.e. a stateful computation over state type S and return a result of type A . In addition, we take a function that expects input of type A and returns a stateful computation State[S,B] . We apply flatMap of fa to ff ., which can be expanded to fa.flatMap(ff) --> fa match { case State(ssa) => State ( s => { ssa(s) match { case (s1,a) => ff(a) match { case State(ssb) => ssb(s1) } } }) } In essence it \"opens\" the computation in fa to extract the run function ssa which takes a state returns result A with the output state. As the output, we construct stateful computation in which a state s is taken as input, we immediately apply s with ssa (i.e. the computation extracted from fa ) to compute the intermediate state s1 and the output a (of type A ). Next we apply ff to a which returns a Stateful computation State[S,B] . We extract the run function from this stateful copmutation, namley ssb and apply it to s1 to continue with the result of the computation. In otherwords, bind function chains up a stateful computation fa with a lambda expressoin that consumes the result from fa and continue with another stateful copmutation. The get and the set methods give us access to the state environment of type S . For instance, case class Counter(c:Int) given counterStateMonad:StateMonad[Counter] = new StateMonad[Counter] { } def incr(using csm:StateMonad[Counter]):State[Counter,Unit] = for { Counter(c) <- csm.get _ <- csm.set(Counter(c+1)) } yield () def app(using csm:StateMonad[Counter]):State[Counter, Int] = for { _ <- incr _ <- incr Counter(v) <- csm.get } yield v In the above we define the state environment as an integer counter. Monadic function incr increase the counter in the state. Monad Laws Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws. Left Identity: bind(pure(a))(f) \\(\\equiv\\) f(a) Right Identity: bind(m)(pure) \\(\\equiv\\) m Associativity: bind(bind(m)(f))(g) \\(\\equiv\\) bind(m)(x => bind(f(x))(g)) Intutively speaking, a bind operation is to extract results of type A from its first argument with type F[A] and apply f to the extracted results. Left identity law enforces that binding a lifted value to f , is the same as applying f to the unlifted value directly, because the lifting and the extraction of the bind cancel each other. Right identity law enforces that binding a lifted value to pure , is the same as the lifted value, because extracting results from m and pure cancel each other. The Associativity law enforces that binding a lifted value m to f then to g is the same as binding m to a monadic bind composition (x => bind(f(x)(g))) Summary In this lesson we have discussed the following A derived type class is a type class that extends from another one. An Applicative Functor is a sub-class of Functor, with the methods pure and ap . The four laws for Applicative Functor. A Monad Functor is a sub-class of Applicative Functor, with the method bind . The three laws of Monad Functor. A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad. Extra Materials Writer Monad The dual of the Reader Monad is the Writer Monad, which has the following definition. // inspired by https://kseo.github.io/posts/2017-01-21-writer-monad.html trait Monoid[A]{ // We omitted the super class SemiRing[A] def mempty:A def mappend:A => A => A } given listMonoid[A]:Monoid[List[A]] = new Monoid[List[A]] { def mempty:List[A] = Nil def mappend:List[A]=>List[A]=>List[A] = (l1:List[A])=>(l2:List[A]) => l1 ++ l2 } case class Writer[W,A]( run: (W,A))(using mw:Monoid[W]) { def flatMap[B](f:A => Writer[W,B]):Writer[W,B] = this match { case Writer((w,a)) => f(a) match { case Writer((w2,b)) => Writer((mw.mappend(w)(w2), b)) } } def map[B](f:A=>B):Writer[W, B] = this match { case Writer((w,a)) => Writer((w, f(a))) } } Similar to the Reader Monad, in the above we define a case class Writer , which has a member value run that returns a tuple of (W,A) . The subtle difference is that the writer memory W has to be an instance of the Monoid type class, in which mempty and mappend operations are defined. type WriterM = [W] =>> [A] =>> Writer[W,A] trait WriterMonad[W] extends Monad[WriterM[W]] { implicit def W0:Monoid[W] override def pure[A](v: A): Writer[W, A] = Writer((W0.mempty, v)) override def bind[A, B]( fa: Writer[W, A] )(f: A => Writer[W, B]): Writer[W, B] = fa match { case Writer((w, a)) => f(a) match { case Writer((w2, b)) => { Writer((W0.mappend(w)(w2), b)) } } } def tell(w: W): Writer[W, Unit] = Writer((w, ())) def pass[A](ma: Writer[W, (A, W => W)]): Writer[W, A] = ma match { case Writer((w, (a, f))) => Writer((f(w), a)) } } In the above we define WriterMonad to be a derived type class of Monad[WriterM[W]] . For a similar reason, we need to include the type class Monoid[W] to ensure that mempty and mappend are defined on W . Besides the pure and bind members, we introduce tell and pass . tell writes the given argument into the writer's memory. pass execute a given computation which returns a value of type A and a memory update function W=>W , and return a Writer whose memory is updated by applied the update function to the memory. In the following we define a simple application with logging mechanism using the Writer Monad. case class LogEntry(msg:String) given logWriterMonad:WriterMonad[List[LogEntry]] = new WriterMonad[List[LogEntry]] { override def W0:Monoid[List[LogEntry]] = new Monoid[List[LogEntry]] { override def mempty = Nil override def mappend = (x:List[LogEntry]) => (y:List[LogEntry]) => x ++ y } } def logger(m: String)(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Unit] = wm.tell(List(LogEntry(m))) def app(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Int] = for { _ <- logger(\"start\") x <- wm.pure(1 + 1) _ <- logger(s\"result is ${x}\") _ <- logger(\"done\") } yield x def runApp(): Int = app match { case Writer((w, i)) => { println(w) i } } Monad Transformer Is the following class a Monad? case class MyState[S,A]( run:S=>Option[(S,A)]) The difference between this class and the State class we've seen earlier is that the execution method run yields result of type Option[(S,A)] instead of (S,A) which means that it can potentially fail. It is ascertained that MyState is also a Monad, and it is a kind of special State Monad. case class MyState[S, A](run: S => Option[(S, A)]) { def flatMap[B](f: A => MyState[S, B]): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => f(a) match { case MyState(ssb) => ssb(s1) } } ) } def map[B](f: A => B): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => Some((s1, f(a))) } ) } } type MyStateM = [S] =>> [A] =>> MyState[S,A] trait MyStateMonad[S] extends Monad[MyStateM[S]] { override def pure[A](v:A):MyState[S,A] = MyState( s=> Some((s,v))) override def bind[A,B]( fa:MyState[S,A] )( ff:A => MyState[S,B] ):MyState[S,B] = fa.flatMap(ff) def get:MyState[S, S] = MyState(s => Some((s,s))) def set(v:S):MyState[S,Unit] = MyState(s => Some((v,()))) } Besides \"stuffing-in\" an Option type, one could use an Either type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer . We begin by parameterizing the Option functor in MyState case class StateT[S, M[_], A](run: S => M[(S, A)])(using m:Monad[M]) { def flatMap[B](f: A => StateT[S, M, B]): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1,a) => f(a) match { case StateT(ssb) => ssb(s1) } } ) ) } def map[B](f: A => B): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1, a) => m.pure((s1, f(a))) }) ) } } In the above it is largely similar to MyState class, except that we parameterize Option by a type parameter M . M[_] indicates that it is of kind *=>* . (using m:Monad[M]) further contraints M must be an instance of Monad, so that we could make use of the bind and pure from M 's Monad instance. Naturally, we can define a derived type class called StateTMonad . type StateTM = [S] =>> [M[_]] =>> [A] =>> StateT[S, M, A] trait StateTMonad[S,M[_]] extends Monad[StateTM[S][M]] { implicit def M0:Monad[M] override def pure[A](v: A): StateT[S, M, A] = StateT(s => M0.pure((s, v))) override def bind[A, B]( fa: StateT[S, M, A] )( ff: A => StateT[S, M, B] ): StateT[S, M, B] = fa.flatMap(ff) def get: StateT[S, M, S] = StateT(s => M0.pure((s, s))) def set(v: S): StateT[S, M, Unit] = StateT(s => M0.pure(v, ())) } Given that Option is a Monad, we can redefine MyStateMonad in terms of StateTMonad and optMonad . trait StateOptMonad[S] extends StateTMonad[S, Option] { override def M0 = optMonad } What about the original vanilla State Monad? We could introduce an Identity Monad. case class Identity[A](run:A) { def flatMap[B](f:A=>Identity[B]):Identity[B] = this match { case Identity(a) => f(a) } def map[B](f:A=>B):Identity[B] = this match { case Identity(a) => Identity(f(a)) } } given identityMonad:Monad[Identity] = new Monad[Identity] { override def pure[A](v:A):Identity[A] = Identity(v) override def bind[A,B](fa:Identity[A])(f: A => Identity[B]):Identity[B] = fa.flatMap(f) } Then we can re-define the vanilla State Monad as follows, (in fact like many existing Monad libraries out there.) trait StateIdentMonad[S] extends StateTMonad[S, Identity] { // same as StateMonad override def M0 = identityMonad } One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes. Similarly we could generalize the Reader Monad to its transformer variant. case class ReaderT[R, M[_], A](run: R => M[A])(using m:Monad[M]) { def flatMap[B](f: A => ReaderT[R, M, B]):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => f(a) match { case ReaderT(rb) => rb(r) })) } def map[B](f: A => B):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => m.pure(f(a)))) } } type ReaderTM = [R] =>>[M[_]] =>> [A] =>> ReaderT[R, M, A] trait ReaderTMonad[R,M[_]] extends Monad[ReaderTM[R][M]] { implicit def M0:Monad[M] override def pure[A](v: A): ReaderT[R, M, A] = ReaderT(r => M0.pure(v)) override def bind[A, B]( fa: ReaderT[R, M, A] )(f: A => ReaderT[R, M, B]): ReaderT[R, M, B] = fa.flatMap(f) def ask: ReaderT[R, M, R] = ReaderT(r => M0.pure(r)) def local[A](f: R => R)(r: ReaderT[R, M, A]): ReaderT[R, M, A] = r match { case ReaderT(ra) => ReaderT(r => { val localR = f(r) ra(localR) }) } } trait ReaderIdentMonad[R] extends ReaderTMonad[R, Identity] { // same as ReaderMonad override def M0 = identityMonad } Note that the order of how Monad Transfomers being stacked up makes a difference, For instance, can you explain what is the difference between the following two? trait ReaderStateIdentMonad[R, S] extends ReaderTMonad[R, StateTM[S][Identity]] { override def M0:StateIdentMonad[S] = new StateIdentMonad[S]{} } trait StateReaderIdentMonad[S, R] extends StateTMonad[S, ReaderTM[R][Identity]] { override def M0:ReaderIdentMonad[R] = new ReaderIdentMonad[R]{} }","title":"50.054 - Applicative and Monad"},{"location":"fp_applicative_monad/#50054-applicative-and-monad","text":"","title":"50.054 - Applicative and Monad"},{"location":"fp_applicative_monad/#learning-outcomes","text":"Describe and define derived type class Describe and define Applicative Functors Describe and define Monads Apply Monad to in design and develop highly modular and resusable software.","title":"Learning Outcomes"},{"location":"fp_applicative_monad/#derived-type-class","text":"Recall that in our previous lesson, we talk about the Ordering type class. trait Ordering[A] { def compare(x:A,y:A):Int // less than: -1, equal: 0, greater than 1 } Let's consider a variant called Order (actually it is defined in a popular Scala library named cats ). trait Eq[A] { def eqv(x:A, y:A):Boolean } trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } In the above, the Eq type class is a supertype of the Order type class, because all instances of Order type class should have the method eqv implemented. We also say Order is a derived type class of Eq . Let's consider some instances given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt:Order[Int] = new Order[Int] { def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1)","title":"Derived Type Class"},{"location":"fp_applicative_monad/#an-alternative-approach","text":"trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int // def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt(using eqInt:Eq[Int]):Order[Int] = new Order[Int] { def eqv(x:Int,y:Int):Boolean = eqInt.eqv(x,y) def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) In the above definition, we skip the default implementatoin of eqv in Order and make use of the type class instance context to synthesis the eqv method based on the existing type class instances of Eq . (This approach is closer to the one found in Haskell.)","title":"An alternative approach"},{"location":"fp_applicative_monad/#which-one-is-better","text":"Both have their own pros and cons. In the first approach, we give a default implementation for the eqv overridden method in Order type class, it frees us from re-defining the eqv in every type class instance of Order . In this case, the rule/logic is fixed at the top level. In the second approach, eqv in Order type class is not defined. We are required to define it for every single type class instance of Order , that means more work. The advantage is that we have flexibility to redefine/re-mix definition of eqv coming from other type class instances.","title":"Which one is better?"},{"location":"fp_applicative_monad/#functor-recap","text":"Recall from the last lesson, we make use of the Functor type class to define generic programming style of data processing. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } enum BTree[+A]{ case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Note that we also swap the first and the second arguments of the map function.","title":"Functor (Recap)"},{"location":"fp_applicative_monad/#applicative-functor","text":"The Applicative Functor is a derived type class of Functor , which is defined as follows trait Applicative[F[_]] extends Functor[F] { def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] def pure[A](a: A): F[A] def map[A, B](fa: F[A])(f: A => B): F[B] = ap(pure(f))(fa) } Note that we \"fix\" the map for Applicative in the type class level in this case. (i.e. we are following the first approach.) given listApplicative:Applicative[List] = new Applicative[List] { def pure[A](a:A):List[A] = List(a) def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.map( f => fa.map(f)).flatten } Recall that flatten flattens a list of lists. Alternatively, we can define the ap method of the Applicative[List] instance flatMap . Given l is a list, l.flatMap(f) is the same as l.map(f).flatten . def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.flatMap( f => fa.map(f)) Recall that Scala compiler desugars expression of shape e1.flatMap( v1 => e2.flatMap( v2 => ... en.map(vn => e ... ))) into for { v1 <- e1 v2 <- e2 ... vn <- en } yield (e) Hence we can rewrite the ap method of the Applicative[List] instance as def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = for { f <- ff a <- fa } yield (f(a)) It is not suprising the following produces the same results as the functor instance. listApplicative.map(l)((x:Int) => x + 1) What about pure and ap ? when can we use them? Let's consider the following contrived example. Suppose we would like to apply two sets of operations to elements from l , each operation will produce its own set of results, and the inputs do not depend on the output of the other set. i.e. If the two set of operations, are (x:Int)=> x+1 and (y:Int)=>y*2 . val intOps= List((x:Int)=>x+1, (y:Int)=>y*2) listApplicative.ap(intOps)(l) we get List(2, 3, 4, 2, 4, 6) as the result. Let's consider another example. Recall that Option[A] algebraic datatype which captures a value of type A could be potentially empty. We define the Applicative[Option] instance as follows given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff match { case None => None case Some(f) => fa match { case None => None case Some(a) => Some(f(a)) } } } In the above Applicative instance, the ap method takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning None . This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value. Recall the builtin Option type is defined as follows, // no need to run this. enum Option[+A] { case None case Some(v) def map[B](f:A=>B):Option[B] = this match { case None => None case Some(v) => Some(f(v)) } def flatMap[B](f:A=>Option[B]):Option[B] = this match { case None => None case Some(v) => f(v) match { case None => None case Some(u) => Some(u) } } } Hence optApplicative can be simplified as given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff.flatMap(f => fa.map(f)) // same as listApplicative } or given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = for { f <- ff a <- fa } yield f(a) // same as listApplicative }","title":"Applicative Functor"},{"location":"fp_applicative_monad/#applicative-laws","text":"Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable. Identity: ap(pure(x=>x)) \\(\\equiv\\) x=>x Homomorphism: ap(pure(f))(pure(x)) \\(\\equiv\\) pure(f(x)) Interchange: ap(u)(pure(y)) \\(\\equiv\\) ap(pure(f=>f(y)))(u) Composition: ap(ap(ap(pure(f=>f.compose))(u))(v))(w) \\(\\equiv\\) ap(u)(ap(v)(w)) Identity law states that applying a lifted identity function of type A=>A is same as an identity function of type F[A] => F[A] where F is the applicative functor. Homomorphism says that applying a lifted function (which has type A=>A before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result. To understand Interchange law let's consider the following equation $$ u\\ y \\equiv (\\lambda f.(f\\ y))\\ u $$ Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\) . To understand the Composition law, we consider the following equation in lambda calculus \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w & \\longrightarrow_{\\beta} \\\\ (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w & \\longrightarrow_{\\beta} \\\\ (u\\circ v)\\ w & \\longrightarrow_{\\tt composition} \\\\ u\\ (v\\ w) \\end{array} \\] The Composition Law says that the above equation remains valid when \\(u\\) , \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\) .","title":"Applicative Laws"},{"location":"fp_applicative_monad/#cohort-exercise","text":"show that any applicative functor satisfying the above laws also satisfies the Functor Laws","title":"Cohort Exercise"},{"location":"fp_applicative_monad/#monad","text":"Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor. Let's consider a motivating example. Recall that in the earlier lesson, we came across the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } In which we use Option[A] to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to Either[A,B] if we want to have better error messages. Monad is a good application here. Let's consider the type class definition of Monad[F[_]] . trait Monad[F[_]] extends Applicative[F] { def bind[A,B](fa:F[A])(f:A => F[B]):F[B] def pure[A](v:A):F[A] def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] = bind(ff)((f:A=>B) => bind(fa)((a:A)=> pure(f(a)))) } given optMonad:Monad[Option] = new Monad[Option] { def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } The eval function can be re-expressed using Monad[Option] . def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1+v2)}) }) case MathExp.Minus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1-v2)}) }) case MathExp.Mult(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1*v2)}) }) case MathExp.Div(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => if (v2 == 0) {None} else {m.pure(v1/v2)}}) }) case MathExp.Const(i) => m.pure(i) } It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of for comprehension since Option has the member functions flatMap and map defined. Recall that Scala desugars for {...} yield expression into flatMap and map . Thus the above can be rewritten as def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) if (v2 !=0) } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now the readability is restored. Another advantage of coding with Monad is that its abstraction allows us to switch underlying data structure without major code change. Suppose we would like to use Either[String, A] or some other equivalent as return type of eval function to support better error message. But before that, let's consider some subclasses of the Applicative and the Monad type classes. trait ApplicativeError[F[_], E] extends Applicative[F] { def raiseError[A](e:E):F[A] } trait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] { override def raiseError[A](e:E):F[A] } type ErrMsg = String In the above, we define an extension to the Applicative type class, named ApplicativeError which expects an extra type class parameter E that denotes an error. The raiseError method takes a value of type E and returns the Applicative result. Similarly, we extend Monad type class with MonadError type class. Next we include the following type class instance to include Option as one f the MonadError functor. given optMonadError:MonadError[Option, ErrMsg] = new MonadError[Option, ErrMsg] { def raiseError[A](e:ErrMsg):Option[A] = None def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } Next, we adjust the eval function to takes in a MonadError context instead of a Monad context. In addition, we make the error signal more explicit by calling the raiseError() method from the MonadError type class context. def eval2(e:MathExp)(using m:MonadError[Option, ErrMsg]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now let's try to refactor the code to make use of Either[ErrMsg, A] as the functor instead of Option[A] . enum Either[+A, +B] { case Left(v: A) case Right(v: B) // to support for comprehension def flatMap[C>:A,D](f: B => Either[C,D]):Either[C,D] = this match { case Left(a) => Left(a) case Right(b) => f(b) } def map[D](f:B => D):Either[A,D] = this match { case Right(b) => Right(f(b)) case Left(e) => Left(e) } } In the above, we have to define flatMap and map member functions for Either type so that we could make use of the for comprehension later on. One might argue with the type signature of flatMap should be flatMap[D](f: B => Either[A,D]):Either[A,D] . The issue here is that the type variable A will appear in both co- and contra-variant positions. The top-level annotation +A is no longer true. Hence we \"relax\" the type constraint here by introducing a new type variable C which has a lower bound of A (even though we do not need to upcast the result of the Left alternative.) type EitherErr = [B] =>> Either[ErrMsg,B] In the above we define Either algebraic datatype and the type construcor EitherErr . [B] =>> Either[ErrMsg, B] denotes a type lambda, which means that EitherErr is a type constructor (or type function) that takes a type B and return an Either[ErrMsg, B] type. Next, we define the type class instance for MonadError[EitherErr, ErrMsg] given eitherErrMonad: MonadError[EitherErr, ErrMsg] = new MonadError[EitherErr, ErrMsg] { import Either.* def raiseError[B](e: ErrMsg): EitherErr[B] = Left(e) def bind[A, B]( fa: EitherErr[A] )(f: A => EitherErr[B]): EitherErr[B] = fa match { case Right(b) => f(b) case Left(s) => Left(s) } def pure[B](v: B): EitherErr[B] = Right(v) } And finally, we refactor the eval function by changing its type signature. And its body remains unchanged. def eval3(e:MathExp)(using m:MonadError[EitherErr, ErrMsg]):EitherErr[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) }","title":"Monad"},{"location":"fp_applicative_monad/#commonly-used-monads","text":"We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads.","title":"Commonly used Monads"},{"location":"fp_applicative_monad/#list-monad","text":"We know that List is a Functor and an Applicative. It is not surprising that List is also a Monad. given listMonad:Monad[List] = new Monad[List] { def pure[A](v:A):List[A] = List(v) def bind[A,B](fa:List[A])(f:A => List[B]):List[B] = fa.flatMap(f) } With the above instance, we can write list processing method in for comprehension which is similar to query languages. import java.util.Date import java.util.Calendar import java.util.GregorianCalendar import java.text.SimpleDateFormat case class Staff(id:Int, dob:Date) def mkStaff(id:Int, dobStr:String):Staff = { val sdf = new SimpleDateFormat(\"yyyy-MM-dd\") val dobDate = sdf.parse(dobStr) Staff(id, dobDate) } val staffData = List( mkStaff(1, \"1076-01-02\"), mkStaff(2, \"1986-07-24\") ) def ageBelow(staff:Staff, age:Int): Boolean = staff match { case Staff(id, dob) => { val today = new Date() val calendar = new GregorianCalendar(); calendar.setTime(today) calendar.add(Calendar.YEAR, -age) val ageYearsAgo = calendar.getTime() dob.after(ageYearsAgo) } } def query(data:List[Staff]):List[Staff] = for { staff <- data // from data if ageBelow(staff, 40) // where staff.age < 40 } yield staff // select *","title":"List Monad"},{"location":"fp_applicative_monad/#reader-monad","text":"Next we consider the Reader Monad. Reader Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable. For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment. case class Reader[R, A] (run: R=>A) { // we need flatMap and map for for-comprehension def flatMap[B](f:A =>Reader[R,B]):Reader[R,B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) match { case Reader(rb) => rb(r) } ) } def map[B](f:A=>B):Reader[R, B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) ) } } type ReaderM = [R] =>> [A] =>> Reader[R, A] trait ReaderMonad[R] extends Monad[ReaderM[R]] { override def pure[A](v:A):Reader[R, A] = Reader (r => v) override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa match { case Reader(ra) => Reader ( r=> f(ra(r)) match { case Reader(rb) => rb(r) } ) } def ask:Reader[R,R] = Reader( r => r) def local[A](f:R=>R)(r:Reader[R,A]):Reader[R,A] = r match { case Reader(ra) => Reader( r => { val localR = f(r) ra(localR) }) } } In the above Reader[R,A] case class defines the structure of the Reader type, where R denotes the shared information for the computation, (source for reader), A denotes the output of the computation. We would like to define Reader[R,_] as a Monad instance. To do so, we define a type-curry version of Reader , i.e. ReaderM . One crucial observation is that bind method in ReaderMonad is nearly identical to flatMap in Reader , with the arguments swapped. In fact, we can re-express bind for all Monads as the flatMap in their underlying case class. override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa.flatMap(f) The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input https://127.0.0.1/ ). For authentication we need to call the authentication server https://127.0.0.10/ temporarily. case class API(url:String) given APIReader:ReaderMonad[API] = new ReaderMonad[API] {} def get(path:String)(using pr:ReaderMonad[API]):Reader[API,Unit] = for { r <- pr.ask s <- r match { case API(url) => pr.pure(println(s\"${url}${path}\")) } } yield s def authServer(api:API):API = API(\"https://127.0.0.10/\") def test1(using pr:ReaderMonad[API]):Reader[API, Unit] = for { a <- pr.local(authServer)(get(\"auth\")) t <- get(\"time\") j <- get(\"job\") } yield (()) def runtest1():Unit = test1 match { case Reader(run) => run(API(\"https://127.0.0.1/\")) }","title":"Reader Monad"},{"location":"fp_applicative_monad/#state-monad","text":"We consider the State Monad. A State Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like Scala, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging. The following we define a State case class, which has a member computation run:S => (S,A) . case class State[S,A]( run:S=>(S,A)) { def flatMap[B](f: A => State[S,B]):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1,a) => f(a) match { case State(ssb) => ssb(s1) } } ) } def map[B](f:A => B):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1, a) => (s1, f(a)) } ) } } As suggested by the type, the computationn S=>(S,A) , takes in a state S as input and return a tuple of output, consists a new state and the result of the computation. The State Monad type class is defined as a dervied type class of Monad[StateM[S]] . type StateM = [S] =>> [A] =>> State[S,A] trait StateMonad[S] extends Monad[StateM[S]] { override def pure[A](v:A):State[S,A] = State( s=> (s,v)) override def bind[A,B]( fa:State[S,A] )( ff:A => State[S,B] ):State[S,B] = fa.flatMap(ff) def get:State[S, S] = State(s => (s,s)) def set(v:S):State[S,Unit] = State(s => (v,())) } In the pure method's default implementation, we takes a value v of type A and return a State case class oject by wrapping a lambda which takes a state s and returns back the same state s with the input value v . In the default implementation of the bind method, we take a computation fa of type State[S,A] , i.e. a stateful computation over state type S and return a result of type A . In addition, we take a function that expects input of type A and returns a stateful computation State[S,B] . We apply flatMap of fa to ff ., which can be expanded to fa.flatMap(ff) --> fa match { case State(ssa) => State ( s => { ssa(s) match { case (s1,a) => ff(a) match { case State(ssb) => ssb(s1) } } }) } In essence it \"opens\" the computation in fa to extract the run function ssa which takes a state returns result A with the output state. As the output, we construct stateful computation in which a state s is taken as input, we immediately apply s with ssa (i.e. the computation extracted from fa ) to compute the intermediate state s1 and the output a (of type A ). Next we apply ff to a which returns a Stateful computation State[S,B] . We extract the run function from this stateful copmutation, namley ssb and apply it to s1 to continue with the result of the computation. In otherwords, bind function chains up a stateful computation fa with a lambda expressoin that consumes the result from fa and continue with another stateful copmutation. The get and the set methods give us access to the state environment of type S . For instance, case class Counter(c:Int) given counterStateMonad:StateMonad[Counter] = new StateMonad[Counter] { } def incr(using csm:StateMonad[Counter]):State[Counter,Unit] = for { Counter(c) <- csm.get _ <- csm.set(Counter(c+1)) } yield () def app(using csm:StateMonad[Counter]):State[Counter, Int] = for { _ <- incr _ <- incr Counter(v) <- csm.get } yield v In the above we define the state environment as an integer counter. Monadic function incr increase the counter in the state.","title":"State Monad"},{"location":"fp_applicative_monad/#monad-laws","text":"Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws. Left Identity: bind(pure(a))(f) \\(\\equiv\\) f(a) Right Identity: bind(m)(pure) \\(\\equiv\\) m Associativity: bind(bind(m)(f))(g) \\(\\equiv\\) bind(m)(x => bind(f(x))(g)) Intutively speaking, a bind operation is to extract results of type A from its first argument with type F[A] and apply f to the extracted results. Left identity law enforces that binding a lifted value to f , is the same as applying f to the unlifted value directly, because the lifting and the extraction of the bind cancel each other. Right identity law enforces that binding a lifted value to pure , is the same as the lifted value, because extracting results from m and pure cancel each other. The Associativity law enforces that binding a lifted value m to f then to g is the same as binding m to a monadic bind composition (x => bind(f(x)(g)))","title":"Monad Laws"},{"location":"fp_applicative_monad/#summary","text":"In this lesson we have discussed the following A derived type class is a type class that extends from another one. An Applicative Functor is a sub-class of Functor, with the methods pure and ap . The four laws for Applicative Functor. A Monad Functor is a sub-class of Applicative Functor, with the method bind . The three laws of Monad Functor. A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad.","title":"Summary"},{"location":"fp_applicative_monad/#extra-materials","text":"","title":"Extra Materials"},{"location":"fp_applicative_monad/#writer-monad","text":"The dual of the Reader Monad is the Writer Monad, which has the following definition. // inspired by https://kseo.github.io/posts/2017-01-21-writer-monad.html trait Monoid[A]{ // We omitted the super class SemiRing[A] def mempty:A def mappend:A => A => A } given listMonoid[A]:Monoid[List[A]] = new Monoid[List[A]] { def mempty:List[A] = Nil def mappend:List[A]=>List[A]=>List[A] = (l1:List[A])=>(l2:List[A]) => l1 ++ l2 } case class Writer[W,A]( run: (W,A))(using mw:Monoid[W]) { def flatMap[B](f:A => Writer[W,B]):Writer[W,B] = this match { case Writer((w,a)) => f(a) match { case Writer((w2,b)) => Writer((mw.mappend(w)(w2), b)) } } def map[B](f:A=>B):Writer[W, B] = this match { case Writer((w,a)) => Writer((w, f(a))) } } Similar to the Reader Monad, in the above we define a case class Writer , which has a member value run that returns a tuple of (W,A) . The subtle difference is that the writer memory W has to be an instance of the Monoid type class, in which mempty and mappend operations are defined. type WriterM = [W] =>> [A] =>> Writer[W,A] trait WriterMonad[W] extends Monad[WriterM[W]] { implicit def W0:Monoid[W] override def pure[A](v: A): Writer[W, A] = Writer((W0.mempty, v)) override def bind[A, B]( fa: Writer[W, A] )(f: A => Writer[W, B]): Writer[W, B] = fa match { case Writer((w, a)) => f(a) match { case Writer((w2, b)) => { Writer((W0.mappend(w)(w2), b)) } } } def tell(w: W): Writer[W, Unit] = Writer((w, ())) def pass[A](ma: Writer[W, (A, W => W)]): Writer[W, A] = ma match { case Writer((w, (a, f))) => Writer((f(w), a)) } } In the above we define WriterMonad to be a derived type class of Monad[WriterM[W]] . For a similar reason, we need to include the type class Monoid[W] to ensure that mempty and mappend are defined on W . Besides the pure and bind members, we introduce tell and pass . tell writes the given argument into the writer's memory. pass execute a given computation which returns a value of type A and a memory update function W=>W , and return a Writer whose memory is updated by applied the update function to the memory. In the following we define a simple application with logging mechanism using the Writer Monad. case class LogEntry(msg:String) given logWriterMonad:WriterMonad[List[LogEntry]] = new WriterMonad[List[LogEntry]] { override def W0:Monoid[List[LogEntry]] = new Monoid[List[LogEntry]] { override def mempty = Nil override def mappend = (x:List[LogEntry]) => (y:List[LogEntry]) => x ++ y } } def logger(m: String)(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Unit] = wm.tell(List(LogEntry(m))) def app(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Int] = for { _ <- logger(\"start\") x <- wm.pure(1 + 1) _ <- logger(s\"result is ${x}\") _ <- logger(\"done\") } yield x def runApp(): Int = app match { case Writer((w, i)) => { println(w) i } }","title":"Writer Monad"},{"location":"fp_applicative_monad/#monad-transformer","text":"Is the following class a Monad? case class MyState[S,A]( run:S=>Option[(S,A)]) The difference between this class and the State class we've seen earlier is that the execution method run yields result of type Option[(S,A)] instead of (S,A) which means that it can potentially fail. It is ascertained that MyState is also a Monad, and it is a kind of special State Monad. case class MyState[S, A](run: S => Option[(S, A)]) { def flatMap[B](f: A => MyState[S, B]): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => f(a) match { case MyState(ssb) => ssb(s1) } } ) } def map[B](f: A => B): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => Some((s1, f(a))) } ) } } type MyStateM = [S] =>> [A] =>> MyState[S,A] trait MyStateMonad[S] extends Monad[MyStateM[S]] { override def pure[A](v:A):MyState[S,A] = MyState( s=> Some((s,v))) override def bind[A,B]( fa:MyState[S,A] )( ff:A => MyState[S,B] ):MyState[S,B] = fa.flatMap(ff) def get:MyState[S, S] = MyState(s => Some((s,s))) def set(v:S):MyState[S,Unit] = MyState(s => Some((v,()))) } Besides \"stuffing-in\" an Option type, one could use an Either type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer . We begin by parameterizing the Option functor in MyState case class StateT[S, M[_], A](run: S => M[(S, A)])(using m:Monad[M]) { def flatMap[B](f: A => StateT[S, M, B]): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1,a) => f(a) match { case StateT(ssb) => ssb(s1) } } ) ) } def map[B](f: A => B): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1, a) => m.pure((s1, f(a))) }) ) } } In the above it is largely similar to MyState class, except that we parameterize Option by a type parameter M . M[_] indicates that it is of kind *=>* . (using m:Monad[M]) further contraints M must be an instance of Monad, so that we could make use of the bind and pure from M 's Monad instance. Naturally, we can define a derived type class called StateTMonad . type StateTM = [S] =>> [M[_]] =>> [A] =>> StateT[S, M, A] trait StateTMonad[S,M[_]] extends Monad[StateTM[S][M]] { implicit def M0:Monad[M] override def pure[A](v: A): StateT[S, M, A] = StateT(s => M0.pure((s, v))) override def bind[A, B]( fa: StateT[S, M, A] )( ff: A => StateT[S, M, B] ): StateT[S, M, B] = fa.flatMap(ff) def get: StateT[S, M, S] = StateT(s => M0.pure((s, s))) def set(v: S): StateT[S, M, Unit] = StateT(s => M0.pure(v, ())) } Given that Option is a Monad, we can redefine MyStateMonad in terms of StateTMonad and optMonad . trait StateOptMonad[S] extends StateTMonad[S, Option] { override def M0 = optMonad } What about the original vanilla State Monad? We could introduce an Identity Monad. case class Identity[A](run:A) { def flatMap[B](f:A=>Identity[B]):Identity[B] = this match { case Identity(a) => f(a) } def map[B](f:A=>B):Identity[B] = this match { case Identity(a) => Identity(f(a)) } } given identityMonad:Monad[Identity] = new Monad[Identity] { override def pure[A](v:A):Identity[A] = Identity(v) override def bind[A,B](fa:Identity[A])(f: A => Identity[B]):Identity[B] = fa.flatMap(f) } Then we can re-define the vanilla State Monad as follows, (in fact like many existing Monad libraries out there.) trait StateIdentMonad[S] extends StateTMonad[S, Identity] { // same as StateMonad override def M0 = identityMonad } One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes. Similarly we could generalize the Reader Monad to its transformer variant. case class ReaderT[R, M[_], A](run: R => M[A])(using m:Monad[M]) { def flatMap[B](f: A => ReaderT[R, M, B]):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => f(a) match { case ReaderT(rb) => rb(r) })) } def map[B](f: A => B):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => m.pure(f(a)))) } } type ReaderTM = [R] =>>[M[_]] =>> [A] =>> ReaderT[R, M, A] trait ReaderTMonad[R,M[_]] extends Monad[ReaderTM[R][M]] { implicit def M0:Monad[M] override def pure[A](v: A): ReaderT[R, M, A] = ReaderT(r => M0.pure(v)) override def bind[A, B]( fa: ReaderT[R, M, A] )(f: A => ReaderT[R, M, B]): ReaderT[R, M, B] = fa.flatMap(f) def ask: ReaderT[R, M, R] = ReaderT(r => M0.pure(r)) def local[A](f: R => R)(r: ReaderT[R, M, A]): ReaderT[R, M, A] = r match { case ReaderT(ra) => ReaderT(r => { val localR = f(r) ra(localR) }) } } trait ReaderIdentMonad[R] extends ReaderTMonad[R, Identity] { // same as ReaderMonad override def M0 = identityMonad } Note that the order of how Monad Transfomers being stacked up makes a difference, For instance, can you explain what is the difference between the following two? trait ReaderStateIdentMonad[R, S] extends ReaderTMonad[R, StateTM[S][Identity]] { override def M0:StateIdentMonad[S] = new StateIdentMonad[S]{} } trait StateReaderIdentMonad[S, R] extends StateTMonad[S, ReaderTM[R][Identity]] { override def M0:ReaderIdentMonad[R] = new ReaderIdentMonad[R]{} }","title":"Monad Transformer"},{"location":"fp_intro/","text":"50.054 - Introduction to functional programming Learning Outcomes By the end of this lesson, you should be able to: Characterize functional programming Comprehend, evaluate lambda terms Differentiate different evaluation strategies Implement simple algorithms using Lambda Calculus What is Functional programming? Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules. How FP differs from other programming languages? The main differences were listed in the earlier section. However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects. Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm): def isort(vals): for i in range(1, len(vals)): curr = i for j in range(i, 0, -1): # scan backward to insert vals[curr] into the right pos if vals[curr] > vals[j-1]: vals[curr], vals[j-1] = vals[j-1], vals[curr] curr = j-1 return vals def isort2(vals): def insert(x, xs): # invarant: xs is already sorted in descending order if len(xs) > 0: if x > xs[0]: return [x] + xs else: return [xs[0]] + insert(x, xs[1:]) else: return [x] def isort_sub(sorted, to_be_sorted): # invariant sorted is already sorted in descending order if len(to_be_sorted) > 0: val = to_be_sorted[0] to_be_sorted_next = to_be_sorted[1:] sorted_next = insert(val, sorted) return isort_sub(sorted_next, to_be_sorted_next) else: return sorted return isort_sub([], vals) isort is implemented in the imperative style; the way we are familiar with. isort2 is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in isort2 in Python, because: it is lengthy it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations. But why are people are interested in FP? The reason is that the invariant of isort is much harder to derive compared to isort2 in which the nested functions' parameters are the subject of the invariants, and the variables in isort2 are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming. Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in isort2 can be expressed as type constraints, which can be verified by the compiler. What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction. In fact many modern FP languagues are quite fast. For example: https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/ Why FP in Compiler Design? Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles. To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in function programs compared to other programming paradigms. One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions. This implies that loop invariances are not constraints among the input and output of these recurisve function. In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages. Lambda Calculus Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s) . Lambda Expression This comic gives a very easy way to understand Lambda Expressions The valid syntax of lambda expression is described as the following EBNF ( Extended Backus Naur Form ) grammar: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] Where: Each line denotes a grammar rule The left hand side (LHS) of the ::= is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol. The RHS of the ::= is a set of alternatives, separated by | . Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\) , \\(\\lambda x.t\\) or \\(t\\ t\\) . \\(x\\) denotes a variable, \\(\\lambda x.t\\) denotes a lambda abstraction. Within a lambda abstraction, \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body. \\(t\\ t\\) denotes a function application. For example, the following are three instances of \\(t\\) . \\(x\\) \\(\\lambda x.x\\) \\((\\lambda x.x)\\ y\\) Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\) , we could interpret it as either \\((\\lambda x.x)\\ (\\lambda y.y)\\) , or \\(\\lambda x.(x\\ \\lambda y.y)\\) As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can. Evaluation Rules Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term. There are only two rules to consider. Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\) , which reads as \\(t\\) is reduced to \\(t'\\) by a step. Beta Reduction \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] What's new here is the term \\([t_2/x]\\) , which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\) , Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\) . For instance, recall our earlier example: \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) & \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x & \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example: \\[ (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction. To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound. \\[ \\begin{array}{rcl} fv(x) & = & \\{x\\}\\\\ fv(\\lambda x.t) & = & fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\end{array} \\] For instance. \\[ \\begin{array}{rcl} fv(\\lambda x.x) & = & fv(x) - \\{x\\} \\\\ & = & \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) & = & fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\ & = & \\{ y \\} \\end{array} \\] One common error we often encounter is capturing the free variables . Consider: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] Note: \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) = \\{ {\\tt y}, w \\} \\] Thus: \\[ \\begin{array}{rl} (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) & \\longrightarrow \\\\ \\lbrack({\\tt y}\\ w)/x\\rbrack \\lambda y.x\\ y & \\longrightarrow \\\\ \\lambda y. ({\\tt y}\\ w)\\ y \\end{array} \\] Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake. Substitution and Alpha Renaming In the following we consider all the possible cases for subsititution \\[ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack x & = & t_1 \\\\ \\lbrack t_1 / x \\rbrack y & = & y & {\\tt if}\\ x \\neq y \\\\ \\lbrack t_1 / x \\rbrack (t_2\\ t_3) & = & \\lbrack t_1 / x \\rbrack t_2\\ \\lbrack t_1 / x \\rbrack t_3 & \\\\ \\lbrack t_1 / x \\rbrack \\lambda y.t_2 & = & \\lambda y. \\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\end{array} \\] In case \\[ y\\neq x\\ {\\tt and} \\ y \\not \\in fv(t_1) \\] is not satified, we need to rename the lambda bound variables that are clashing. Recall: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] We rename the inner lambda bound variable \\(y\\) to \\(z\\) : \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] to avoid clashing, prior applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming. Evaluation strategies So far we have three rules (roughly) $\\beta $ reduction, substitution, and $\\alpha $ renaming. Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules. We call a (sub-)expression of shape \\(\\lambda x.t_1\\ t_2\\) a redex . The task is to look for redexes in a lambda term and rewrite them by applying $\\beta $ reduction and substitution, and sometimes $\\alpha $ renaming to avoid capturing free variables. But in what order shall we apply these rules? There are two mostly known strategies Inner-most, leftmost - Applicative Order Reduction (AOR) Outer-most, leftmost - Normal Order Reduction (NOR) Consider $(\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y) $, AOR: \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y) & \\longrightarrow_{\\tt (\\beta\\ reduction)} &\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] NOR: \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda x. x)\\ z))\\ (\\lambda y.y)} & \\longrightarrow_{\\tt(\\beta)} \\\\ \\underline{(\\lambda x. x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta)} \\\\ \\lambda y.y \\end{array} \\] Interesting Notes Some connection with real world languages: Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions. Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions. AOR or NOR, which one is better? By Church-Rosser Theorem, if a lambda term can be evaluated in two different ways and both ways terminate, both will yield the same result. Recall our earlier example. So how can it be non-terminating? Consider: \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) & \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ ... \\end{array} \\] NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how \\(((\\lambda x.\\lambda y.x)\\ x)\\ ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\) , but diverges in AOR. NOR can be used to evaluate terms that deals with infinite data. Let Binding Let-binding allows us to introduce local (immutable) variables. Approach 1 - extending the syntax and evaluation rules We extend the syntax with let-binding: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] and the evaluation rule: \\[ \\begin{array}{rl} {\\tt (Let)} & let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] and the substitution rule and the free variable function \\(fv()\\) : \\[ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 & = & let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\\\ \\end{array} $$ $$ \\begin{array}{rcl} fv(let\\ x=t_1\\ in\\ t_2) & = & (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\ \\end{array} \\] Note that the alpha renaming should be applied when name clash arises. Approach 2 - desugaring In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language. We can rewrite: \\[ let\\ x=t_1\\ in\\ t_2 \\] into: \\[ (\\lambda x.t_2)\\ t_1 \\] where \\(x \\not\\in fv(t_1)\\) . What happen if \\(x \\in fv(t_1)\\) ? It forms a recursive definition. We will look into recursion in a later section. Conditional Expression A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . There are at least two different ways of incorporating conditional expression in our lambda term language. Approach 1 - Extending the syntax and the evaluation rules We could extend the grammar \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] and the evaluation rules \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3 \\end{array} \\\\ \\\\ {\\tt (ifT)} & if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} & if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ op\\ t_2\\ \\longrightarrow t_1'\\ op\\ t_2 \\end{array} \\\\ \\\\ {\\tt (OpI2)} & \\begin{array}{c} t_2 \\longrightarrow t_2' \\\\ \\hline c_1\\ op\\ t_2\\ \\longrightarrow c_1\\ op\\ t_2' \\end{array} \\\\ \\\\ {\\tt (OpC)} & \\begin{array}{c} invoke\\ low\\ level\\ call\\ op(c_1, c_2) = c_3 \\\\ \\hline c_1\\ op\\ c_2\\ \\longrightarrow c_3 \\end{array} \\\\ \\\\ ... \\end{array} \\] In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises , and the relation the written below is called the conclusion . The conclusion holds if the premises are valid. The rule ${\\tt (ifI)} $ states that if we can evaluate \\(t_1\\) to $t_1' $, then $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ can be evaluated to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $. In otherwords, for us to reduce $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $, a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\) . The rule ${\\tt (ifT)} $ states that if the conditional expression is \\(true\\) , the entire term is evaluated to the then-branch. The rule ${\\tt (ifF)} $ is similar. Rules \\({\\tt (OpI1)}\\) and ${\\tt (OpI2)} $ are similar to rule \\({\\tt (IfI)}\\) . The rule ${\\tt (OpC)} $ invokes the built-in low level call to apply the binary operation to the two operands $c_1 $ and $c_2 $. The substitution rules and free variable function \\(fv()\\) also extended too $$ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack c & = & c \\ \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 & = & (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\ \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 & = & if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\ \\end{array} $$ $$ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\ fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) & = & fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\ fv(c) & = & {} \\ \\end{array} $$ Let's consider an example: \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\ 0\\ else\\ 10/x)\\ 2 & \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\ 0\\ else\\ 10/x & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\] Approach 2 - Church Encoding Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms. Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus. Let's define: \\(true\\) as \\(\\lambda x.\\lambda y.x\\) \\(false\\) as \\(\\lambda x.\\lambda y.y\\) \\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\) We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv (e_1\\ e_2)\\ e_3\\) . For example, \\[ \\begin{array}{rl} ite\\ true\\ w\\ z & = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z & \\longrightarrow \\\\ true\\ w\\ z & = \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z & \\longrightarrow \\\\ w \\end{array} \\] Recursion To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion. Similar to the conditional expression, there are at least two ways of introducing recursion to our language. Approach 1 - Extending the syntax and the evaluation rules We extend the syntax with a mu-abstraction ( \\(\\mu\\) ): \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & ... \\mid \\mu f.t \\end{array} \\] and the evaluation rules: \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ t_2 \\longrightarrow t_1'\\ t_2 \\end{array} \\\\ {\\tt (unfold)} & \\mu f.t \\longrightarrow [(\\mu f.t)/f] t \\\\ \\end{array} \\] Note that we include the ${\\tt (NOR)} $ rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate. We include the following cases for the free variable function \\(fv()\\) and the substitution $$ \\begin{array}{rcl} fv(\\mu f.t) & = & fv(t) - {f} \\end{array} $$ and $$ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack \\mu f.t_2 & = & \\mu f.\\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ f\\neq x\\ {\\tt and}\\ f \\not \\in fv(t_1) \\end{array} $$ For instance: \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)) & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\) , \\({\\tt (NOR)}\\) , \\({\\tt (unfold)}\\) , \\({\\tt (IfT)}\\) , \\({\\tt (IfF)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (OpC)}\\) , \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try to rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course. Approach 2 - Church Encoding Alternatively, recursion can be encoded using the fix-pointer combinator (AKA $Y $-combinator). Let $Y $ be \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] We find that for any function \\(g\\) , we have \\(Y\\ g = g\\ (Y\\ g)\\) . We will work on the derivation during exercise. Let's try to implement the factorial function over natural numbers: \\[ \\begin{array}{cc} fac(n) = \\left [ \\begin{array}{ll} 1 & {if}~ n = 0 \\\\ n*fac(n-1) & {otherwise} \\end{array} \\right . \\end{array} \\] Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition. Let \\(Fac\\) be \\[ \\begin{array}{c} \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above. Discussion 1 How to define the following? \\(one\\) \\(iszero\\) \\(mul\\) \\(pred\\) Discussion 2 The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)? We will work on the two topics discussed above during the cohort class. Summary We have covered Syntax (lambda terms) and Semantics ( \\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming). Evaluation strategies, their properties and connection to real world programming Extending lambda calculus to support conditional and loop Via language extension (we will use) Via Church encoding (fun but not very pragmatic in our context)","title":"50.054 - Introduction to functional programming"},{"location":"fp_intro/#50054-introduction-to-functional-programming","text":"","title":"50.054 - Introduction to functional programming"},{"location":"fp_intro/#learning-outcomes","text":"By the end of this lesson, you should be able to: Characterize functional programming Comprehend, evaluate lambda terms Differentiate different evaluation strategies Implement simple algorithms using Lambda Calculus","title":"Learning Outcomes"},{"location":"fp_intro/#what-is-functional-programming","text":"Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules.","title":"What is Functional programming?"},{"location":"fp_intro/#how-fp-differs-from-other-programming-languages","text":"The main differences were listed in the earlier section. However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects. Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm): def isort(vals): for i in range(1, len(vals)): curr = i for j in range(i, 0, -1): # scan backward to insert vals[curr] into the right pos if vals[curr] > vals[j-1]: vals[curr], vals[j-1] = vals[j-1], vals[curr] curr = j-1 return vals def isort2(vals): def insert(x, xs): # invarant: xs is already sorted in descending order if len(xs) > 0: if x > xs[0]: return [x] + xs else: return [xs[0]] + insert(x, xs[1:]) else: return [x] def isort_sub(sorted, to_be_sorted): # invariant sorted is already sorted in descending order if len(to_be_sorted) > 0: val = to_be_sorted[0] to_be_sorted_next = to_be_sorted[1:] sorted_next = insert(val, sorted) return isort_sub(sorted_next, to_be_sorted_next) else: return sorted return isort_sub([], vals) isort is implemented in the imperative style; the way we are familiar with. isort2 is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in isort2 in Python, because: it is lengthy it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations. But why are people are interested in FP? The reason is that the invariant of isort is much harder to derive compared to isort2 in which the nested functions' parameters are the subject of the invariants, and the variables in isort2 are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming. Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in isort2 can be expressed as type constraints, which can be verified by the compiler. What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction. In fact many modern FP languagues are quite fast. For example: https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/","title":"How FP differs from other programming languages?"},{"location":"fp_intro/#why-fp-in-compiler-design","text":"Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles. To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in function programs compared to other programming paradigms. One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions. This implies that loop invariances are not constraints among the input and output of these recurisve function. In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages.","title":"Why FP in Compiler Design?"},{"location":"fp_intro/#lambda-calculus","text":"Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s) .","title":"Lambda Calculus"},{"location":"fp_intro/#lambda-expression","text":"This comic gives a very easy way to understand Lambda Expressions The valid syntax of lambda expression is described as the following EBNF ( Extended Backus Naur Form ) grammar: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] Where: Each line denotes a grammar rule The left hand side (LHS) of the ::= is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol. The RHS of the ::= is a set of alternatives, separated by | . Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\) , \\(\\lambda x.t\\) or \\(t\\ t\\) . \\(x\\) denotes a variable, \\(\\lambda x.t\\) denotes a lambda abstraction. Within a lambda abstraction, \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body. \\(t\\ t\\) denotes a function application. For example, the following are three instances of \\(t\\) . \\(x\\) \\(\\lambda x.x\\) \\((\\lambda x.x)\\ y\\) Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\) , we could interpret it as either \\((\\lambda x.x)\\ (\\lambda y.y)\\) , or \\(\\lambda x.(x\\ \\lambda y.y)\\) As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can.","title":"Lambda Expression"},{"location":"fp_intro/#evaluation-rules","text":"Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term. There are only two rules to consider. Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\) , which reads as \\(t\\) is reduced to \\(t'\\) by a step.","title":"Evaluation Rules"},{"location":"fp_intro/#beta-reduction","text":"\\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] What's new here is the term \\([t_2/x]\\) , which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\) , Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\) . For instance, recall our earlier example: \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) & \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x & \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example: \\[ (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction. To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound. \\[ \\begin{array}{rcl} fv(x) & = & \\{x\\}\\\\ fv(\\lambda x.t) & = & fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\end{array} \\] For instance. \\[ \\begin{array}{rcl} fv(\\lambda x.x) & = & fv(x) - \\{x\\} \\\\ & = & \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) & = & fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\ & = & \\{ y \\} \\end{array} \\] One common error we often encounter is capturing the free variables . Consider: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] Note: \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) = \\{ {\\tt y}, w \\} \\] Thus: \\[ \\begin{array}{rl} (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) & \\longrightarrow \\\\ \\lbrack({\\tt y}\\ w)/x\\rbrack \\lambda y.x\\ y & \\longrightarrow \\\\ \\lambda y. ({\\tt y}\\ w)\\ y \\end{array} \\] Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake.","title":"Beta Reduction"},{"location":"fp_intro/#substitution-and-alpha-renaming","text":"In the following we consider all the possible cases for subsititution \\[ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack x & = & t_1 \\\\ \\lbrack t_1 / x \\rbrack y & = & y & {\\tt if}\\ x \\neq y \\\\ \\lbrack t_1 / x \\rbrack (t_2\\ t_3) & = & \\lbrack t_1 / x \\rbrack t_2\\ \\lbrack t_1 / x \\rbrack t_3 & \\\\ \\lbrack t_1 / x \\rbrack \\lambda y.t_2 & = & \\lambda y. \\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\end{array} \\] In case \\[ y\\neq x\\ {\\tt and} \\ y \\not \\in fv(t_1) \\] is not satified, we need to rename the lambda bound variables that are clashing. Recall: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] We rename the inner lambda bound variable \\(y\\) to \\(z\\) : \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] to avoid clashing, prior applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming.","title":"Substitution and Alpha Renaming"},{"location":"fp_intro/#evaluation-strategies","text":"So far we have three rules (roughly) $\\beta $ reduction, substitution, and $\\alpha $ renaming. Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules. We call a (sub-)expression of shape \\(\\lambda x.t_1\\ t_2\\) a redex . The task is to look for redexes in a lambda term and rewrite them by applying $\\beta $ reduction and substitution, and sometimes $\\alpha $ renaming to avoid capturing free variables. But in what order shall we apply these rules? There are two mostly known strategies Inner-most, leftmost - Applicative Order Reduction (AOR) Outer-most, leftmost - Normal Order Reduction (NOR) Consider $(\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y) $, AOR: \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y) & \\longrightarrow_{\\tt (\\beta\\ reduction)} &\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] NOR: \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda x. x)\\ z))\\ (\\lambda y.y)} & \\longrightarrow_{\\tt(\\beta)} \\\\ \\underline{(\\lambda x. x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta)} \\\\ \\lambda y.y \\end{array} \\]","title":"Evaluation strategies"},{"location":"fp_intro/#interesting-notes","text":"Some connection with real world languages: Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions. Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions. AOR or NOR, which one is better? By Church-Rosser Theorem, if a lambda term can be evaluated in two different ways and both ways terminate, both will yield the same result. Recall our earlier example. So how can it be non-terminating? Consider: \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) & \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ ... \\end{array} \\] NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how \\(((\\lambda x.\\lambda y.x)\\ x)\\ ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\) , but diverges in AOR. NOR can be used to evaluate terms that deals with infinite data.","title":"Interesting Notes"},{"location":"fp_intro/#let-binding","text":"Let-binding allows us to introduce local (immutable) variables.","title":"Let Binding"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-evaluation-rules","text":"We extend the syntax with let-binding: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] and the evaluation rule: \\[ \\begin{array}{rl} {\\tt (Let)} & let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] and the substitution rule and the free variable function \\(fv()\\) : \\[ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 & = & let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\\\ \\end{array} $$ $$ \\begin{array}{rcl} fv(let\\ x=t_1\\ in\\ t_2) & = & (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\ \\end{array} \\] Note that the alpha renaming should be applied when name clash arises.","title":"Approach 1 - extending the syntax and evaluation rules"},{"location":"fp_intro/#approach-2-desugaring","text":"In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language. We can rewrite: \\[ let\\ x=t_1\\ in\\ t_2 \\] into: \\[ (\\lambda x.t_2)\\ t_1 \\] where \\(x \\not\\in fv(t_1)\\) . What happen if \\(x \\in fv(t_1)\\) ? It forms a recursive definition. We will look into recursion in a later section.","title":"Approach 2 - desugaring"},{"location":"fp_intro/#conditional-expression","text":"A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . There are at least two different ways of incorporating conditional expression in our lambda term language.","title":"Conditional Expression"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules","text":"We could extend the grammar \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] and the evaluation rules \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3 \\end{array} \\\\ \\\\ {\\tt (ifT)} & if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} & if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ op\\ t_2\\ \\longrightarrow t_1'\\ op\\ t_2 \\end{array} \\\\ \\\\ {\\tt (OpI2)} & \\begin{array}{c} t_2 \\longrightarrow t_2' \\\\ \\hline c_1\\ op\\ t_2\\ \\longrightarrow c_1\\ op\\ t_2' \\end{array} \\\\ \\\\ {\\tt (OpC)} & \\begin{array}{c} invoke\\ low\\ level\\ call\\ op(c_1, c_2) = c_3 \\\\ \\hline c_1\\ op\\ c_2\\ \\longrightarrow c_3 \\end{array} \\\\ \\\\ ... \\end{array} \\] In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises , and the relation the written below is called the conclusion . The conclusion holds if the premises are valid. The rule ${\\tt (ifI)} $ states that if we can evaluate \\(t_1\\) to $t_1' $, then $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ can be evaluated to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $. In otherwords, for us to reduce $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $, a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\) . The rule ${\\tt (ifT)} $ states that if the conditional expression is \\(true\\) , the entire term is evaluated to the then-branch. The rule ${\\tt (ifF)} $ is similar. Rules \\({\\tt (OpI1)}\\) and ${\\tt (OpI2)} $ are similar to rule \\({\\tt (IfI)}\\) . The rule ${\\tt (OpC)} $ invokes the built-in low level call to apply the binary operation to the two operands $c_1 $ and $c_2 $. The substitution rules and free variable function \\(fv()\\) also extended too $$ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack c & = & c \\ \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 & = & (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\ \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 & = & if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\ \\end{array} $$ $$ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\ fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) & = & fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\ fv(c) & = & {} \\ \\end{array} $$ Let's consider an example: \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\ 0\\ else\\ 10/x)\\ 2 & \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\ 0\\ else\\ 10/x & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\]","title":"Approach 1 - Extending the syntax and the evaluation rules"},{"location":"fp_intro/#approach-2-church-encoding","text":"Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms. Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus. Let's define: \\(true\\) as \\(\\lambda x.\\lambda y.x\\) \\(false\\) as \\(\\lambda x.\\lambda y.y\\) \\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\) We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv (e_1\\ e_2)\\ e_3\\) . For example, \\[ \\begin{array}{rl} ite\\ true\\ w\\ z & = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z & \\longrightarrow \\\\ true\\ w\\ z & = \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z & \\longrightarrow \\\\ w \\end{array} \\]","title":"Approach 2 - Church Encoding"},{"location":"fp_intro/#recursion","text":"To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion. Similar to the conditional expression, there are at least two ways of introducing recursion to our language.","title":"Recursion"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules_1","text":"We extend the syntax with a mu-abstraction ( \\(\\mu\\) ): \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & ... \\mid \\mu f.t \\end{array} \\] and the evaluation rules: \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ t_2 \\longrightarrow t_1'\\ t_2 \\end{array} \\\\ {\\tt (unfold)} & \\mu f.t \\longrightarrow [(\\mu f.t)/f] t \\\\ \\end{array} \\] Note that we include the ${\\tt (NOR)} $ rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate. We include the following cases for the free variable function \\(fv()\\) and the substitution $$ \\begin{array}{rcl} fv(\\mu f.t) & = & fv(t) - {f} \\end{array} $$ and $$ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack \\mu f.t_2 & = & \\mu f.\\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ f\\neq x\\ {\\tt and}\\ f \\not \\in fv(t_1) \\end{array} $$ For instance: \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)) & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\) , \\({\\tt (NOR)}\\) , \\({\\tt (unfold)}\\) , \\({\\tt (IfT)}\\) , \\({\\tt (IfF)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (OpC)}\\) , \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try to rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course.","title":"Approach 1 - Extending the syntax and the evaluation rules"},{"location":"fp_intro/#approach-2-church-encoding_1","text":"Alternatively, recursion can be encoded using the fix-pointer combinator (AKA $Y $-combinator). Let $Y $ be \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] We find that for any function \\(g\\) , we have \\(Y\\ g = g\\ (Y\\ g)\\) . We will work on the derivation during exercise. Let's try to implement the factorial function over natural numbers: \\[ \\begin{array}{cc} fac(n) = \\left [ \\begin{array}{ll} 1 & {if}~ n = 0 \\\\ n*fac(n-1) & {otherwise} \\end{array} \\right . \\end{array} \\] Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition. Let \\(Fac\\) be \\[ \\begin{array}{c} \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above.","title":"Approach 2 - Church Encoding"},{"location":"fp_intro/#discussion-1","text":"How to define the following? \\(one\\) \\(iszero\\) \\(mul\\) \\(pred\\)","title":"Discussion 1"},{"location":"fp_intro/#discussion-2","text":"The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)? We will work on the two topics discussed above during the cohort class.","title":"Discussion 2"},{"location":"fp_intro/#summary","text":"We have covered Syntax (lambda terms) and Semantics ( \\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming). Evaluation strategies, their properties and connection to real world programming Extending lambda calculus to support conditional and loop Via language extension (we will use) Via Church encoding (fun but not very pragmatic in our context)","title":"Summary"},{"location":"fp_scala/","text":"50.054 - Instroduction to Scala Learning Outcomes By the end of this class, you should be able to Develop simple implementation in Scala using List, Conditional, and Recursion Model problems and design solutions using Algebraic Datatype and Pattern Matching Compile and execute simple Scala programs What is Scala? Scala is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Scala has many backends, including JVM, node.js and native. Scala is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Scala, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Scala is used in the real-world business, you may refer to the following for further readings. Scala at Scale at Databricks Why Scala is seeing a renewed interest for developing enterprise software Who is using Scala, Akka and Play framework Type-safe Tensor Scala Hello World Let's say we have a Scala file named HelloWorld.scala println(\"hello world\") We can execute it via either scala HelloWorld.scala or to compile it then run scalac HelloWorld.scala && scala HelloWorld Although in the cohort problems, we are going to rely on a Scala project manager called sbt to build, execute and test our codes. Scala OOP vs Java OOP If you know Object Oriented Programming, you already know 70% of Scala. Consider the following Java code snippet interface FlyBehavior { void fly(); } abstract class Bird { private String species; private FlyBehavior fb; public Bird(String species, FlyBehavior fb) { this.species = species; this.fb = fb; } public String getSpecies() { return this.species; } public void fly() { return this.fb.fly(); } } class Duck extends Bird { public Duck() { super(\"Duck\", new FlyBehavior() { @override void fly() { System.out.println(\"I can't fly\"); } }) } } class BlueJay extends Bird { public BlueJay() { super(\"BlueJay\", new FlyBehavior() { @override void fly() { System.out.println(\"Swwooshh!\"); } }) } } We define an abstract class Bird which has two member attributes, species and fb . We adopt the Strategy design pattern to delegate the fly behavior of the bird through an interface FlyBehavior . Scala has the equivalence of language features as Java. The language has much concise syntax. In the following we implement the same logic in Scala. trait FlyBehavior { def fly() } abstract class Bird(species:String, fb:FlyBehavior) { def getSpecies():String = this.species def fly():Unit = this.fb.fly() } class Duck extends Bird(\"Duck\", new FlyBehavior() { override def fly() = println(\"I can't fly\") }) class BlueJay extends Bird(\"BlueJay\", new FlyBehavior() { override def fly() = println(\"Swwooshh!\") }) In Scala, we prefer inline constructors. A trait is the Scala equivalent of Java's interface. Similar to Python, methods start with def . A method's return type comes after the method name declaration. Type annotations follow their arguments instead of preceding them. Method bodies are defined after an equality sign. The return keyword is optional; the last expression will be returned as the result. The Java style of method body definition is also supported, i.e. the getSpecies() method can be defined as follows: def getSpecies():String { return this.species } Being a JVM language, Scala allows us to import and invoke Java libraries in Scala code. import java.util.LinkedList val l = new java.util.LinkedList[String]() Keyword val defines an immutable variable, and var defines a mutable variable. Functional Programming in Scala at a glance In this module, we focus and utilise mostly the functional programming feature of Scala. Lambda Calculus Scala Variable \\(x\\) x Constant \\(c\\) 1 , 2 , true , false Lambda abstraction \\(\\lambda x.t\\) (x:T) => e Function application \\(t_1\\ t_2\\) e1(e2) Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) if (e1) { e2 } else { e3 } Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) val x = e1 ; e2 Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) def f(x:Int):Int = f(x); f(1); where T denotes a type and :T denotes a type annotation. e , e1 , e2 and e3 denote expressions. Similar to other mainstream languages, defining recursion in Scala is straight-forward, we just make reference to the recursive function name in its body. def fac(x:Int):Int = { if (x == 0) { 1 } else { x*fac(x-1) } } val result = fac(10) Scala Strict and Lazy Evaluation Let f be a non-terminating function def f(x:Int):Int = f(x) The following shows that the function application in Scala is using strict evaluation. def g(x:Int):Int = 1 g(f(1)) // it does not terminate On the other hand, the following code is terminating. def h(x: => Int):Int = 1 h(f(1)) // it terminates! The type annotation : => Int after x states that the argument x is passed in by name (lazy evaluation), not by value (strict evaluation). List Data type We consider a commonly used builtin data type in Scala, the list data type. In Scala, the following define some list values. Nil - an empty list. List() - an empty list. List(1,2) - an integer list contains two values. List(\"a\") - an string list contains one value. 1::List(2,3) - prepends a value 1 to a list containing 2 and 3 . List(\"hello\") ++ List(\"world\") - concatenating two string lists. To iterate through the items in a list, we can use a for-loop: def sum(l:List[Int]):Int = { var s = 0 for (i <- l) { s = s+i } s } which is very similar to what we could implement in Java or Python. However, we are more interested in using the functional programming features in Scala: def sum(l:List[Int]):Int = { l match { case Nil => 0 case (hd::tl) => hd + sum(tl) } } in which l match {case Nil => 0; case (hd::tl) => hd+sum(tl) } denotes a pattern-matching expression in Scala. It is similar to the switch statement found in other main stream languages, except that it has more perks . In this expression, we pattern match the input list l against two list patterns, namely: Nil the empty list, and (hd::tl) the non-empty list Note that here Nil and hd::tl are not list values, because they are appearing after a case keyword and on the left of a thick arrow => . Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list l is an empty list. If it is empty, the sum of an empty list must be 0 . If the input list l is not an empty list, it must have at least one element. The pattern (hd::tl) extracts the first element of the list and binds it to a local variable hd and the remainder (which is the sub list formed by taking away the first element from l ) is bound to hd . We often call hd as the head of the list and tl as the tail. We would like to remind that hd is storing a single integer in this case, and tl is capturing a list of integers. One advantage of implementing the sum function in FP style is that it is much closer to its math specification. \\[ \\begin{array}{rl} sum(l) = & \\left [ \\begin{array}{ll} 0 & {l\\ is\\ empty} \\\\ head(l)+sum(tail(l)) & {otherwise} \\end{array} \\right . \\end{array} \\] Let's consider another example. def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The function reverse takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the match expression. When the input list l is an empty list, we return an empty list. The reverse of an empty list is an empty list When the input l is not empty, we make use of the pattern (hd::tl) to extract the head and the tail of the list We apply reverse recursively to the tail and then concatenate it with a list containing the head. You may notice that the same reverse function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the reverse function into a generic version as follows: def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Note that the first [A] denotes a type argument, with which we specify that the element type of the list is A (any possible type). The type argument is resolved when we apply reverse to a actual argument. For instance in reverse(List(1,2,3)) the Scala compiler will resolve A=Int and in reverse(List(\"a\",\"b\")) it will resolve A=String . A Note on Recursion Note that recursive calls to reverse will incur additional memory space in the machine in form of additional function call frames on the call stack. A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error. While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion. A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. For instance, the reverse() function presented earlier is not. The following variant is a tail recursion def reverse[A](l:List[A]):List[A] = { def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } In the above definition, we rely on a inner function go which is a recursive function. In go , the recursion take places at the last instruction in the (x::xs) case. The trick is to pass around an accumulated output o in each recursive call. Some compilers such as GHC can detect a tail recursive function, but it will not rewrite into a form which no stack is required. As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. However Scala does not automatically re-write a non-tail recursion into a tail recursion. Instead it offers a check: import scala.annotation.tailrec def reverse[A](l:List[A]):List[A] = { @tailrec def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } The annotation tailrec is to hint to the Scala compiler that go should be compiled in a way that no stack frame should be created. If the compiler fails to do that, it will complain. In the absence of the tailrec annotation, the compiler will still try to optimize the tail recursion. If we apply the tailrec annotation to a non-tail recursive function, Scala will complain. @tailrec def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The following error is reported: -- Error: ---------------------------------------------------------------------- 4 | case (hd::tl) => reverse(tl) ++ List(hd) | ^^^^^^^^^^^ | Cannot rewrite recursive call: it is not in tail position 1 error found Map, Fold and Filter Consider the following function def addToEach(x:Int, l:List[Int]):List[Int] = l match { case Nil => Nil case (y::ys) => { val yx = y+x yx::addToEach(x,ys) } } It takes two inputs, an integer x and an integer list l , and adds x to every element in l and put the results in the output list. For instance addToEach(1, List(1,2,3)) yields List(2,3,4) . The above can rewritten by using a generic library method shipped with Scala. def addToEach(x:Int, l:List[Int]):List[Int] = l.map(y=>y+x) The method map is a method of the list class that takes an function as input argument and applies it to all elements in the list object. Note that the above is same as def addToEach(x:Int, l:List[Int]):List[Int] = { def addX(y:Int):Int = y+x l.map(addX) } We can observe that the input list and the output list of the map method must be of the same type and have the same length. Recall in the sum function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function. def sum(l:List[Int]):Int = l.foldLeft(0)((acc,x)=> acc+x) The foldLeft method takes a base accumulator, and a binary function as inputs, and aggregates the elements from the list using the binary function. In particular, the binary aggreation function assumes the first argument is the accumulator. Besides foldLeft , there exists a foldRight method, in which the binary aggregation function expects the second argument is the accumulator. def sum(l:List[Int]):Int = l.foldRight(0)((x,acc)=> x+acc) So what is the difference between foldLeft and foldRight ? What happen if you run the following? Can you explain the difference? val l = List(\"a\",\"better\",\"world\", \"by\", \"design\") l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) Note that + is an overloaded operator. In the above it concatenates two string values. Intuitively, l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) aggregates the list of words using the aggregation function by nesting the recursive calls to the left. ((((\"\"+\" \"+\"a\")+\" \"+\"better\")+\" \"+\"world\")+\" \"+\"by\")+\" \"+\"design\" where l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) aggregates the list of words by nesting the recursive calls to the right. \"a\"+\" \"+(\"better\"+\" \"+(\"world\"+\" \"+(\"by\"+\" \"+(\"design\"+\" \"+\"\")))) The method filter takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false. val l = List(1,2,3,4) def even(x:Int):Boolean = x%2==0 l.filter(even) returns List(2,4) . val l = List('a','1','0','d') l.filter((c:Char) => c.isDigit) returns List('1','0') . With map , foldLeft and filter , we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm: def qsort(l:List[Int]):List[Int] = l match { case Nil => Nil case List(x) => List(x) case (p::rest) => { val ltp = rest.filter( x => x < p) val gep = rest.filter( x => !(x < p)) qsort(ltp) ++ List(p) ++ qsort(gep) } } which resembles the math specification \\[ \\begin{array}{cc} qsort(l) = & \\left[ \\begin{array}{ll} l & |l| < 2 \\\\ qsort(\\{x|x \\in l \\wedge x < head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x < head(l)) \\}) & otherwise \\end{array} \\right . \\end{array} \\] where \\(\\uplus\\) unions two bags and maintains the order. flatMap and for-comprehension There is a variant of map method, consider val l = (1 to 5).toList l.map( i => if (i%2 ==0) { List(i) } else { Nil }) would yield List(List(), List(2), List(), List(4), List()) We would like to get rid of the nested lists and flatten the outer list. One possibility is to: l.flatMap( i => if (i%2 ==0) { List(i) } else { Nil }) Like map , flatMap applies its parameter function to every element in the list. Unlike map , flatMap expects the parameter function produces a list, thus it will join all the sub-lists into one list. With map and flatMap , we can define complex list transformation operations like the following: def listProd[A,B](la:List[A], lb:List[B]):List[(A,B)] = la.flatMap( a => lb.map(b => (a,b))) val l2 = List('a', 'b', 'c') listProd(l, l2) which produces: List((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (4,a), (4,b), (4,c), (5,a), (5,b), (5,c)) Note that Scala supports list comprehension via the for ... yield construct. We could re-express listProd as follows: def listProd2[A,B](la:List[A], lb:List[B]):List[(A,B)] = for { a <- la b <- lb } yield (a,b) The Scala compiler desugars: for { x1 <- e1; x2 <- e2; ...; xn <- en } yield e ```` into: ```scala e1.flatMap( x1 => e2.flatMap(x2 => .... en.map( xn => e) ...)) The above syntactic sugar not only works for the list data type but any data type with flatMap and map defined (as we will see in the upcoming lessons). In its general form, we refer to it as for-comprehension . One extra note to take is that the for-comprehension should not be confused with the for-loop statement exists in the imperative style programming in Scala. var sum = 0 for (i <- 1 to 10) {sum = sum + i} println(sum) The Algebraic Datatype Like many other languages, Scala supports user defined data type. From an earlier section, we have discussed how to use classes and traits in Scala to define data types, making using of the OOP concepts that we have learned. This style of defining data types using abstraction and encapsulation is also known as the abstract datatype. In this section, we consider an alternative, the Algebraic Datatype. Consider the following EBNF of a math expression. \\[ \\begin{array}{rccl} {\\tt (Math Exp)} & e & ::= & e + e \\mid e - e \\mid e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} & c & ::= & ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] And we would like to implement a function eval() which evaluates a \\({\\tt (Math Exp)}\\) to a value. If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\) , and use if-else statements with instanceof to check for a specific subclass instance. Alternative, we can also rely on visitor pattern or delegation. It turns out that using Abstract Datatypes to model the above result in some engineering overhead. Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms) Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values) For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\) The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums. In Scala 3, it is recommended to use enum for Algebraic datatypes. enum MathExp: case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) end MathExp In the above the MathExp ( enum ) datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance Plus(e1:MathExp, e2:MathExp) , which states that a plus expression has two operands, both of which are of type MathExp . Note that the end MathExp is optional, as long as there is an extra line. Alternatively, we can use { } . enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } We can represent the math expression (1+2) * 3 as MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3)) . Note that we call Plus(_,_) , Minus(_,_) , Mult(_,_) , Div(_,_) and Const(_) \"data constructors\", as we use them to construct values of the enum algebraic datatype MathExp . Next let's implement an evaluation function based the specification: \\[ eval(e) = \\left [ \\begin{array}{cl} eval(e_1) + eval(e_2) & if\\ e = e_1+e_2 \\\\ eval(e_1) - eval(e_2) & if\\ e = e_1-e_2 \\\\ eval(e_1) * eval(e_2) & if\\ e = e_1*e_2 \\\\ eval(e_1) / eval(e_2) & if\\ e = e_1/e_2 \\\\ c & if\\ e = c \\end{array} \\right. \\] def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } In Scala, the `enum`` Algebraic datatype can be accessed (destructured) via pattern matching. If we run: eval(MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3))) we get 9 as result. Let's consider another example where we can implement some real-world data structures using the algebraic datatype. Suppose for experimental purposes, we would like to re-implement the list datatype in Scala (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. We will look into the generic version in the next lesson In the following we consider the specification of the MyList data type in EBNF: \\[ \\begin{array}{rccl} {\\tt (MyList)} & l & ::= & Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} & i & ::= & 1 \\mid 2 \\mid ... \\end{array} \\] And we implement using enum in Scala: enum MyList { case Nil case Cons(x:Int, xs:MyList) } Next we implement the map function based on the following specification \\[ map(f, l) = \\left [ \\begin{array}{ll} Nil & if\\ l = Nil\\\\ Cons(f(hd), map(f, tl)) & if\\ l = Cons(hd, tl) \\end{array} \\right . \\] Then we could implement the map function def mapML(f:Int=>Int, l:MyList):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(f,tl)) } Running mapML(x => x+1, MyList.Cons(1,MyList.Nil)) yields MyList.Cons(2,MyList.Nil) . But hang on a second! The map method from the Scala built-in list is a method of a list object, not a stand-alone function. In Scala 3, enum allows us to package the method inside enum values. enum MyList { case Nil case Cons(x:Int, xs:MyList) def mapML(f:Int=>Int):MyList = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Running: val l = MyList.Cons(1, MyList.Nil) l.mapML(x=> x+1) yields the same output as above. Summary In this lesson, we have discussed Scala's OOP vs Java's OOP Scala's FP vs Lambda Calculus How to use the List datatype to model and manipulate collections of multiple values. How to use the Algebraic data type to define user customized data type to solve complex problems.","title":"50.054 - Instroduction to Scala"},{"location":"fp_scala/#50054-instroduction-to-scala","text":"","title":"50.054 - Instroduction to Scala"},{"location":"fp_scala/#learning-outcomes","text":"By the end of this class, you should be able to Develop simple implementation in Scala using List, Conditional, and Recursion Model problems and design solutions using Algebraic Datatype and Pattern Matching Compile and execute simple Scala programs","title":"Learning Outcomes"},{"location":"fp_scala/#what-is-scala","text":"Scala is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Scala has many backends, including JVM, node.js and native. Scala is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Scala, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Scala is used in the real-world business, you may refer to the following for further readings. Scala at Scale at Databricks Why Scala is seeing a renewed interest for developing enterprise software Who is using Scala, Akka and Play framework Type-safe Tensor","title":"What is Scala?"},{"location":"fp_scala/#scala-hello-world","text":"Let's say we have a Scala file named HelloWorld.scala println(\"hello world\") We can execute it via either scala HelloWorld.scala or to compile it then run scalac HelloWorld.scala && scala HelloWorld Although in the cohort problems, we are going to rely on a Scala project manager called sbt to build, execute and test our codes.","title":"Scala Hello World"},{"location":"fp_scala/#scala-oop-vs-java-oop","text":"If you know Object Oriented Programming, you already know 70% of Scala. Consider the following Java code snippet interface FlyBehavior { void fly(); } abstract class Bird { private String species; private FlyBehavior fb; public Bird(String species, FlyBehavior fb) { this.species = species; this.fb = fb; } public String getSpecies() { return this.species; } public void fly() { return this.fb.fly(); } } class Duck extends Bird { public Duck() { super(\"Duck\", new FlyBehavior() { @override void fly() { System.out.println(\"I can't fly\"); } }) } } class BlueJay extends Bird { public BlueJay() { super(\"BlueJay\", new FlyBehavior() { @override void fly() { System.out.println(\"Swwooshh!\"); } }) } } We define an abstract class Bird which has two member attributes, species and fb . We adopt the Strategy design pattern to delegate the fly behavior of the bird through an interface FlyBehavior . Scala has the equivalence of language features as Java. The language has much concise syntax. In the following we implement the same logic in Scala. trait FlyBehavior { def fly() } abstract class Bird(species:String, fb:FlyBehavior) { def getSpecies():String = this.species def fly():Unit = this.fb.fly() } class Duck extends Bird(\"Duck\", new FlyBehavior() { override def fly() = println(\"I can't fly\") }) class BlueJay extends Bird(\"BlueJay\", new FlyBehavior() { override def fly() = println(\"Swwooshh!\") }) In Scala, we prefer inline constructors. A trait is the Scala equivalent of Java's interface. Similar to Python, methods start with def . A method's return type comes after the method name declaration. Type annotations follow their arguments instead of preceding them. Method bodies are defined after an equality sign. The return keyword is optional; the last expression will be returned as the result. The Java style of method body definition is also supported, i.e. the getSpecies() method can be defined as follows: def getSpecies():String { return this.species } Being a JVM language, Scala allows us to import and invoke Java libraries in Scala code. import java.util.LinkedList val l = new java.util.LinkedList[String]() Keyword val defines an immutable variable, and var defines a mutable variable.","title":"Scala OOP vs Java OOP"},{"location":"fp_scala/#functional-programming-in-scala-at-a-glance","text":"In this module, we focus and utilise mostly the functional programming feature of Scala. Lambda Calculus Scala Variable \\(x\\) x Constant \\(c\\) 1 , 2 , true , false Lambda abstraction \\(\\lambda x.t\\) (x:T) => e Function application \\(t_1\\ t_2\\) e1(e2) Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) if (e1) { e2 } else { e3 } Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) val x = e1 ; e2 Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) def f(x:Int):Int = f(x); f(1); where T denotes a type and :T denotes a type annotation. e , e1 , e2 and e3 denote expressions. Similar to other mainstream languages, defining recursion in Scala is straight-forward, we just make reference to the recursive function name in its body. def fac(x:Int):Int = { if (x == 0) { 1 } else { x*fac(x-1) } } val result = fac(10)","title":"Functional Programming in Scala at a glance"},{"location":"fp_scala/#scala-strict-and-lazy-evaluation","text":"Let f be a non-terminating function def f(x:Int):Int = f(x) The following shows that the function application in Scala is using strict evaluation. def g(x:Int):Int = 1 g(f(1)) // it does not terminate On the other hand, the following code is terminating. def h(x: => Int):Int = 1 h(f(1)) // it terminates! The type annotation : => Int after x states that the argument x is passed in by name (lazy evaluation), not by value (strict evaluation).","title":"Scala Strict and Lazy Evaluation"},{"location":"fp_scala/#list-data-type","text":"We consider a commonly used builtin data type in Scala, the list data type. In Scala, the following define some list values. Nil - an empty list. List() - an empty list. List(1,2) - an integer list contains two values. List(\"a\") - an string list contains one value. 1::List(2,3) - prepends a value 1 to a list containing 2 and 3 . List(\"hello\") ++ List(\"world\") - concatenating two string lists. To iterate through the items in a list, we can use a for-loop: def sum(l:List[Int]):Int = { var s = 0 for (i <- l) { s = s+i } s } which is very similar to what we could implement in Java or Python. However, we are more interested in using the functional programming features in Scala: def sum(l:List[Int]):Int = { l match { case Nil => 0 case (hd::tl) => hd + sum(tl) } } in which l match {case Nil => 0; case (hd::tl) => hd+sum(tl) } denotes a pattern-matching expression in Scala. It is similar to the switch statement found in other main stream languages, except that it has more perks . In this expression, we pattern match the input list l against two list patterns, namely: Nil the empty list, and (hd::tl) the non-empty list Note that here Nil and hd::tl are not list values, because they are appearing after a case keyword and on the left of a thick arrow => . Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list l is an empty list. If it is empty, the sum of an empty list must be 0 . If the input list l is not an empty list, it must have at least one element. The pattern (hd::tl) extracts the first element of the list and binds it to a local variable hd and the remainder (which is the sub list formed by taking away the first element from l ) is bound to hd . We often call hd as the head of the list and tl as the tail. We would like to remind that hd is storing a single integer in this case, and tl is capturing a list of integers. One advantage of implementing the sum function in FP style is that it is much closer to its math specification. \\[ \\begin{array}{rl} sum(l) = & \\left [ \\begin{array}{ll} 0 & {l\\ is\\ empty} \\\\ head(l)+sum(tail(l)) & {otherwise} \\end{array} \\right . \\end{array} \\] Let's consider another example. def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The function reverse takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the match expression. When the input list l is an empty list, we return an empty list. The reverse of an empty list is an empty list When the input l is not empty, we make use of the pattern (hd::tl) to extract the head and the tail of the list We apply reverse recursively to the tail and then concatenate it with a list containing the head. You may notice that the same reverse function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the reverse function into a generic version as follows: def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Note that the first [A] denotes a type argument, with which we specify that the element type of the list is A (any possible type). The type argument is resolved when we apply reverse to a actual argument. For instance in reverse(List(1,2,3)) the Scala compiler will resolve A=Int and in reverse(List(\"a\",\"b\")) it will resolve A=String .","title":"List Data type"},{"location":"fp_scala/#a-note-on-recursion","text":"Note that recursive calls to reverse will incur additional memory space in the machine in form of additional function call frames on the call stack. A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error. While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion. A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. For instance, the reverse() function presented earlier is not. The following variant is a tail recursion def reverse[A](l:List[A]):List[A] = { def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } In the above definition, we rely on a inner function go which is a recursive function. In go , the recursion take places at the last instruction in the (x::xs) case. The trick is to pass around an accumulated output o in each recursive call. Some compilers such as GHC can detect a tail recursive function, but it will not rewrite into a form which no stack is required. As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. However Scala does not automatically re-write a non-tail recursion into a tail recursion. Instead it offers a check: import scala.annotation.tailrec def reverse[A](l:List[A]):List[A] = { @tailrec def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } The annotation tailrec is to hint to the Scala compiler that go should be compiled in a way that no stack frame should be created. If the compiler fails to do that, it will complain. In the absence of the tailrec annotation, the compiler will still try to optimize the tail recursion. If we apply the tailrec annotation to a non-tail recursive function, Scala will complain. @tailrec def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The following error is reported: -- Error: ---------------------------------------------------------------------- 4 | case (hd::tl) => reverse(tl) ++ List(hd) | ^^^^^^^^^^^ | Cannot rewrite recursive call: it is not in tail position 1 error found","title":"A Note on Recursion"},{"location":"fp_scala/#map-fold-and-filter","text":"Consider the following function def addToEach(x:Int, l:List[Int]):List[Int] = l match { case Nil => Nil case (y::ys) => { val yx = y+x yx::addToEach(x,ys) } } It takes two inputs, an integer x and an integer list l , and adds x to every element in l and put the results in the output list. For instance addToEach(1, List(1,2,3)) yields List(2,3,4) . The above can rewritten by using a generic library method shipped with Scala. def addToEach(x:Int, l:List[Int]):List[Int] = l.map(y=>y+x) The method map is a method of the list class that takes an function as input argument and applies it to all elements in the list object. Note that the above is same as def addToEach(x:Int, l:List[Int]):List[Int] = { def addX(y:Int):Int = y+x l.map(addX) } We can observe that the input list and the output list of the map method must be of the same type and have the same length. Recall in the sum function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function. def sum(l:List[Int]):Int = l.foldLeft(0)((acc,x)=> acc+x) The foldLeft method takes a base accumulator, and a binary function as inputs, and aggregates the elements from the list using the binary function. In particular, the binary aggreation function assumes the first argument is the accumulator. Besides foldLeft , there exists a foldRight method, in which the binary aggregation function expects the second argument is the accumulator. def sum(l:List[Int]):Int = l.foldRight(0)((x,acc)=> x+acc) So what is the difference between foldLeft and foldRight ? What happen if you run the following? Can you explain the difference? val l = List(\"a\",\"better\",\"world\", \"by\", \"design\") l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) Note that + is an overloaded operator. In the above it concatenates two string values. Intuitively, l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) aggregates the list of words using the aggregation function by nesting the recursive calls to the left. ((((\"\"+\" \"+\"a\")+\" \"+\"better\")+\" \"+\"world\")+\" \"+\"by\")+\" \"+\"design\" where l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) aggregates the list of words by nesting the recursive calls to the right. \"a\"+\" \"+(\"better\"+\" \"+(\"world\"+\" \"+(\"by\"+\" \"+(\"design\"+\" \"+\"\")))) The method filter takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false. val l = List(1,2,3,4) def even(x:Int):Boolean = x%2==0 l.filter(even) returns List(2,4) . val l = List('a','1','0','d') l.filter((c:Char) => c.isDigit) returns List('1','0') . With map , foldLeft and filter , we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm: def qsort(l:List[Int]):List[Int] = l match { case Nil => Nil case List(x) => List(x) case (p::rest) => { val ltp = rest.filter( x => x < p) val gep = rest.filter( x => !(x < p)) qsort(ltp) ++ List(p) ++ qsort(gep) } } which resembles the math specification \\[ \\begin{array}{cc} qsort(l) = & \\left[ \\begin{array}{ll} l & |l| < 2 \\\\ qsort(\\{x|x \\in l \\wedge x < head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x < head(l)) \\}) & otherwise \\end{array} \\right . \\end{array} \\] where \\(\\uplus\\) unions two bags and maintains the order.","title":"Map, Fold and Filter"},{"location":"fp_scala/#flatmap-and-for-comprehension","text":"There is a variant of map method, consider val l = (1 to 5).toList l.map( i => if (i%2 ==0) { List(i) } else { Nil }) would yield List(List(), List(2), List(), List(4), List()) We would like to get rid of the nested lists and flatten the outer list. One possibility is to: l.flatMap( i => if (i%2 ==0) { List(i) } else { Nil }) Like map , flatMap applies its parameter function to every element in the list. Unlike map , flatMap expects the parameter function produces a list, thus it will join all the sub-lists into one list. With map and flatMap , we can define complex list transformation operations like the following: def listProd[A,B](la:List[A], lb:List[B]):List[(A,B)] = la.flatMap( a => lb.map(b => (a,b))) val l2 = List('a', 'b', 'c') listProd(l, l2) which produces: List((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (4,a), (4,b), (4,c), (5,a), (5,b), (5,c)) Note that Scala supports list comprehension via the for ... yield construct. We could re-express listProd as follows: def listProd2[A,B](la:List[A], lb:List[B]):List[(A,B)] = for { a <- la b <- lb } yield (a,b) The Scala compiler desugars: for { x1 <- e1; x2 <- e2; ...; xn <- en } yield e ```` into: ```scala e1.flatMap( x1 => e2.flatMap(x2 => .... en.map( xn => e) ...)) The above syntactic sugar not only works for the list data type but any data type with flatMap and map defined (as we will see in the upcoming lessons). In its general form, we refer to it as for-comprehension . One extra note to take is that the for-comprehension should not be confused with the for-loop statement exists in the imperative style programming in Scala. var sum = 0 for (i <- 1 to 10) {sum = sum + i} println(sum)","title":"flatMap and for-comprehension"},{"location":"fp_scala/#the-algebraic-datatype","text":"Like many other languages, Scala supports user defined data type. From an earlier section, we have discussed how to use classes and traits in Scala to define data types, making using of the OOP concepts that we have learned. This style of defining data types using abstraction and encapsulation is also known as the abstract datatype. In this section, we consider an alternative, the Algebraic Datatype. Consider the following EBNF of a math expression. \\[ \\begin{array}{rccl} {\\tt (Math Exp)} & e & ::= & e + e \\mid e - e \\mid e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} & c & ::= & ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] And we would like to implement a function eval() which evaluates a \\({\\tt (Math Exp)}\\) to a value. If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\) , and use if-else statements with instanceof to check for a specific subclass instance. Alternative, we can also rely on visitor pattern or delegation. It turns out that using Abstract Datatypes to model the above result in some engineering overhead. Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms) Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values) For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\) The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums. In Scala 3, it is recommended to use enum for Algebraic datatypes. enum MathExp: case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) end MathExp In the above the MathExp ( enum ) datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance Plus(e1:MathExp, e2:MathExp) , which states that a plus expression has two operands, both of which are of type MathExp . Note that the end MathExp is optional, as long as there is an extra line. Alternatively, we can use { } . enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } We can represent the math expression (1+2) * 3 as MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3)) . Note that we call Plus(_,_) , Minus(_,_) , Mult(_,_) , Div(_,_) and Const(_) \"data constructors\", as we use them to construct values of the enum algebraic datatype MathExp . Next let's implement an evaluation function based the specification: \\[ eval(e) = \\left [ \\begin{array}{cl} eval(e_1) + eval(e_2) & if\\ e = e_1+e_2 \\\\ eval(e_1) - eval(e_2) & if\\ e = e_1-e_2 \\\\ eval(e_1) * eval(e_2) & if\\ e = e_1*e_2 \\\\ eval(e_1) / eval(e_2) & if\\ e = e_1/e_2 \\\\ c & if\\ e = c \\end{array} \\right. \\] def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } In Scala, the `enum`` Algebraic datatype can be accessed (destructured) via pattern matching. If we run: eval(MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3))) we get 9 as result. Let's consider another example where we can implement some real-world data structures using the algebraic datatype. Suppose for experimental purposes, we would like to re-implement the list datatype in Scala (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. We will look into the generic version in the next lesson In the following we consider the specification of the MyList data type in EBNF: \\[ \\begin{array}{rccl} {\\tt (MyList)} & l & ::= & Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} & i & ::= & 1 \\mid 2 \\mid ... \\end{array} \\] And we implement using enum in Scala: enum MyList { case Nil case Cons(x:Int, xs:MyList) } Next we implement the map function based on the following specification \\[ map(f, l) = \\left [ \\begin{array}{ll} Nil & if\\ l = Nil\\\\ Cons(f(hd), map(f, tl)) & if\\ l = Cons(hd, tl) \\end{array} \\right . \\] Then we could implement the map function def mapML(f:Int=>Int, l:MyList):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(f,tl)) } Running mapML(x => x+1, MyList.Cons(1,MyList.Nil)) yields MyList.Cons(2,MyList.Nil) . But hang on a second! The map method from the Scala built-in list is a method of a list object, not a stand-alone function. In Scala 3, enum allows us to package the method inside enum values. enum MyList { case Nil case Cons(x:Int, xs:MyList) def mapML(f:Int=>Int):MyList = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Running: val l = MyList.Cons(1, MyList.Nil) l.mapML(x=> x+1) yields the same output as above.","title":"The Algebraic Datatype"},{"location":"fp_scala/#summary","text":"In this lesson, we have discussed Scala's OOP vs Java's OOP Scala's FP vs Lambda Calculus How to use the List datatype to model and manipulate collections of multiple values. How to use the Algebraic data type to define user customized data type to solve complex problems.","title":"Summary"},{"location":"fp_scala_poly/","text":"50.054 - Parametric Polymorphism and Adhoc Polymorphism Learning Outcomes By this end of this lesson, you should be able to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes develop generic programming style code using Functor type class. make use of Option and Either to handle and manipulate errors and exceptions. Currying In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments. For example, def sum(x:Int, y:Int):Int = x + y can be rewritten into def sum_curry(x:Int)(y:Int):Int = x + y These two functions are equivalent except that Their invocations are different, e.g. sum(1,2) sum_curry(1)(2) It is easier to reuse the curried version to define other function, e.g. def plus1(x:Int):Int = sum_curry(1)(x) Function Composition Every function and method in Scala is an object with a .compose() method. It works like the mathmethical composition. In math, let \\(g\\) and \\(f\\) be functions, then \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] Let g and f be Scala functions (or methods), then g.compose(f) is equivalent to x => g(f(x)) For example def f(x:Int):Int = 2 * x + 3 def g(x:Int):Int = x * x assert((g.compose(f))(2) == g(f(2))) Generics Generics is also known as type variables. It enables a language to support parametric polymoprhism. Polymorphic functions Recall that the reverse function introduced in the last lesson def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace Int by a type variable A . def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Polymorphic Algebraic Datatype Recall that the following Algebraic Datatype from the last lesson. enum MyList { case Nil case Cons(x:Int, xs:MyList) } def mapML(l:MyList, f:Int => Int):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl, f)) } Same observation applies. MyList could have a generic element type A instead of Int and mapML should remains unchanged. enum MyList[A] { case Nil // type error case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A], f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML( tl, f)) } The caveat here is that the Scala compiler would complain about the Nil case above -- Error: ---------------------------------------------------------------------- 2 | case Nil | ^^^^^^^^ | cannot determine type argument for enum parent class MyList, | type parameter type A is invariant 1 error found To understand that error, we need to understand how Scala desugar the enum datatype. The above MyList datatype is desugared as enum MyList[A] { case Nil extends MyList[Nothing] // type error case Cons(x:A, xs:MyList[A]) extends MyList[A] } In which all sub cases within the enum type must be sub-class of the enum type. However it is not trivial for Nil . It can't be declared as a subtype of MyList[A] since type variable A is not mentioned in its definition, unlike Cons(x:A, xs:MyList[A]) . The best we can get is MyList[Nothing] where Nothing is the subtype of all other types in Scala. (As the dual, Any is the supertype of all other types in Scala). We are getting very close. Now we know that Nil extends MyList[Nothing] . If we can argue that MyList[Nothing] extends MyList[A] then we are all good. For MyList[Nothing] extends MyList[A] to hold, A must be covariant type parameter. In type system with subtyping, a type is covariant if it preserves the subtyping order when it is applied a type constructor. In the above situation, MyList[_] is a type constructor. The type parameter A is covarient because we note Nothing <: A for all A , thus MyList[Nothing] <: MyList[A] . a type is contravariant if it reverses the subtyping order when it is applied to a type constructor. For instance, given function type A => Boolean , the type parameter A is contravariant, because for A <: B , we have B => Boolean <: A => Boolean . (We can use functions of type B => Boolean in the context where a function A => Boolean is expected, but not the other way round.) a type is invariant if it does not preserve nor reverse the subtyping order when it is applied to a type constructor. Hence to fix the above type error with the MyList[A] datatype, we declared that A is covariant, +A . enum MyList[+A] { case Nil // type error is fixed. case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A])(f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl)(f)) } For easy of reasoning, we also rewrite mapML into currying style. Recall that we could make mapML function as a method of MyList enum MyList[+A] { case Nil case Cons(x:A, xs:MyList[A]) def mapML[B](f:A=>B):MyList[B] = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Scala Variances Type class Suppose we would like to convert some of the Scala value to JSON string. We could rely on overloading. def toJS(v:Int):String = v.toString def toJS(v:String):String = s\"'${v}'\" def toJS(v:Boolean):String = v.toString Given v is a Scala string value, s\"some_prefix ${v} some_suffix\" denotes a Scala string interpolation, which inserts v 's content into the \"place holder\" in the string \"some_prefix ${v} some_suffix\" where the ${v} is the place holder. However this becomes hard to manage as we consider complex datatype. enum Contact { case Email(e:String) case Phone(ph:String) } import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${toJS(e)}\" // compilation error case Phone(ph) => s\"'phone': ${toJS(ph)}\" // compilation error } When we try to define the toJS function for Contact datatype, we can't make use of the toJS function for string value because the compiler is confused that we are trying to make recursive calls. This is the first issue we faced. Let's pretend that the first issue has been addressed. There's still another issue. Consider case class Person(name:String, contacts:List[Contact]) case class Team(members:List[Person]) A case class is like a normal class we have seen earlier except that we can apply pattern matching to its values. Let's continue to overload toJS to handle Person and Team . def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${toJS(name)}, 'contacts':${toJS(contacts)} }\" } def toJS(cs:List[Contact]):String = { val j = cs.map(c=>toJS(c)).mkString(\",\") s\"[${j}]\" } def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${toJS(members)} }\" } def toJS(ps:List[Person]):String = { val j = ps.map(p=>toJS(p)).mkString(\",\") s\"[${j}]\" } The second issue is that the toJS(cs:List[Contact]) and toJS(ps:List[Person]) are the identical modulo the variable names. Can we combine two into one? def toJS[A](vs:List[A]):String = { val j = vs.map(v=>toJS(v)).mkString(\",\") // compiler error s\"[${j}]\" } However a compilation error occurs because the compiler is unable to resolve the toJS[A](v:A) used in the .map() . It seems that we need to give some extra information to the compiler so that it knows that when we use the above generic toJS we are referring to either Person or Contact , or whatever type that has a toJS defined. One solution to address the two above issues is to use type class . In Scala 3, a type class is defined by a polymoprhic trait and a set of type class instances. trait JS[A] { def toJS(v:A):String } given toJSInt:JS[Int] = new JS[Int]{ def toJS(v:Int):String = v.toString } given toJSString:JS[String] = new JS[String] { def toJS(v:String):String = s\"'${v}'\" } given toJSBoolean:JS[Boolean] = new JS[Boolean] { def toJS(v:Boolean):String = v.toString } given toJSContact(using jsstr:JS[String]):JS[Contact] = new JS[Contact] { import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${jsstr.toJS(e)}\" // compilation error is fixed case Phone(ph) => s\"'phone': ${jsstr.toJS(ph)}\" // compilation error is fixed } } given toJSPerson(using jsstr:JS[String], jsl:JS[List[Contact]]):JS[Person] = new JS[Person] { def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${jsstr.toJS(name)}, 'contacts':${jsl.toJS(contacts)} }\" } } given toJSTeam(using jsl:JS[List[Person]]):JS[Team] = new JS[Team] { def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${jsl.toJS(members)} }\" } } given toJSList[A](using jsa:JS[A]):JS[List[A]] = new JS[List[A]] { def toJS(as:List[A]):String = { val j = as.map(a=>jsa.toJS(a)).mkString(\",\") s\"[${j}]\" } } given defines a type class instance. An instance consists of a name and the context parameters (those with using ) and instance type. In the body of the type class instance, we instantiate an anonymous object that extends type class with the specific type and provide the defintion. We can refer to the particular type class instance by the instance's name. For instance import Contact.* val myTeam = Team( List( Person(\"kenny\", List(Email(\"kenny_lu@sutd.edu.sg\"))), Person(\"simon\", List(Email(\"simon_perrault@sutd.edu.sg\"))) )) toJSTeam.toJS(myTeam) yields 'team':{ 'members':['person':{ 'name':'kenny', 'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon', 'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] } We can also refer to the type class instance by the instace's type. For example, recall the last two instances. In the context of the toJSTeam , we refer to another instance of type JS[List[Person]] . Note that none of the defined instances has the required type. Scala is smart enought to synthesize it from the instances of toJSList and toJSPerson . Given the required type class instance is JS[List[Person]] , the type class resolver finds the instance toJSList having type JS[List[A]] , and it unifies both and find that A=Person . In the context of the instance toJSList , JS[A] is demanded. We can refine the required instance's type as JS[Person] , which is toJSPerson . Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. def printAsJSON[A](v:A)(using jsa:JS[A]):Unit = { println(jsa.toJS(v)) } printAsJSON(myTeam) Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming . In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects. In the next few section, we consider some common patterns in FP that are promoting generic programming. Functor Recall that we have a map method for list datatype. val l = List(1,2,3) l.map(x => x + 1) Can we make map to work for other data type? For example enum BTree[+A] { case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } It turns out that extending map to different datatypes is similar to toJS function that we implemented earlier. We consider introducing a type class for this purpose. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } In the above type class definition, T[_] denotes a polymorphic type that of kind * => * . A kind is a type of types. In the above, it means Functor takes any type constructors T . When T is instantiated, it could be List[_] or BTree[_] and etc. (C.f. In the type class JS[A] , the type argument has kind * .) given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } Some example val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Functor Laws All instances of functor must obey a set of mathematic laws for their computation to be predictable. Let i be a functor instance 1. Identity: i => map(i)(x => x) \\(\\equiv\\) x => x . When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: i=> map(i)(f.compose(g)) \\(\\equiv\\) (i => map(i)(f)).compose(j => map(j)(g)) . If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second. Foldable Similarly we can define a Foldable type class for generic and overloaded foldLeft ( and foldRight ). trait Foldable[T[_]]{ def foldLeft[A,B](t:T[B])(acc:A)(f:(A,B)=>A):A } given listFoldable:Foldable[List] = new Foldable[List] { def foldLeft[A,B](t:List[B])(acc:A)(f:(A,B)=>A):A = t.foldLeft(acc)(f) } given btreeFoldable:Foldable[BTree] = new Foldable[BTree] { import BTree.* def foldLeft[A,B](t:BTree[B])(acc:A)(f:(A,B)=>A):A = t match { case Empty => acc case Node(v, lft, rgt) => { val acc1 = f(acc,v) val acc2 = foldLeft(lft)(acc1)(f) foldLeft(rgt)(acc2)(f) } } } listFoldable.foldLeft(l)(0)((x:Int,y:Int) => x + y) btreeFoldable.foldLeft(t)(0)((x:Int,y:Int) => x + y) Option and Either Recall in the earlier lesson, we encountered the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } An error occurs when we try to evalue a MathExp which contains a division by zero sub-expression. Executing import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) yields java.lang.ArithmeticException: / by zero at rs$line$2$.eval(rs$line$2:5) ... 41 elided Like other main stream languages, we could use try-catch statement to handle the exception. try { import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) } catch { case e:java.lang.ArithmeticException => println(\"handinging div by zero\") } One downside of this approach is that at compile type it is hard to track the unhandled exceptions, (in particular with the presence of Java unchecked exceptions.) A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes). Consider the following builtin Scala datatype Option // no need to run this. enum Option[+A] { case None case Some(v:A) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } When we execute eval(Div(Const(1), Minus(Const(2), Const(2)))) , we get None as the result instead of the exception. One advantage of this is that whoever is using eval function has to respect that its return type is Option[Int] instead of just Int therefore, a match must be applied before using the result to look out for potential None value. There are still two drawbacks. Firstly, the updated version of the eval function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue. We could instead of using Option , use the Either datatype // no need to run this, it's builtin enum Either[+A, +B] { case Left(v:A) case Right(v:B) } type ErrMsg = String def eval(e: MathExp): Either[ErrMsg,Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(0) => Left(s\"div by zero caused by ${e.toString}\") case Right(v2) => Right(v1 / v2) } } case MathExp.Const(i) => Right(i) } Executing eval(Div(Const(1), Minus(Const(2), Const(2)))) will yield Left(div by zero caused by Div(Const(1),Minus(Const(2),Const(2)))) Summary In this lesson, we have discussed how to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes how to develop generic programming style code using Functor type class. how to make use of Option and Either to handle and manipulate errors and exceptions. Appendix Generalized Algebraic Data Type Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example. Firstly, we need some type acrobatics to encode nature numbers on the level of type. enum Zero { case Zero } enum Succ[A] { case Succ(v:A) } Next we define our GADT SList[S,A] which is a generic list of elements A and with size S . enum SList[S,+A] { case Nil extends SList[Zero,Nothing] // additional type constraint S = Zero case Cons[N,A](hd:A, tl:SList[N,A]) extends SList[Succ[N],A] // add'n type constraint S = Succ[N] } In the first subcase Nil , it is declared with the type of SList[Zero, Nothing] which indicates on type level that the list is empty. In the second case Cons , we define it to have the type SList[Succ[N],A] for some natural number N . This indicates on the type level that the list is non-empty. Having these information lifted to the type level allows us to define a type safe head function. import SList.* def head[A,N](sl:SList[Succ[N],A]):A = sl match { case Cons(hd, tl) => hd } Compiling head(Nil) yields a type error. Similarly we can define a size-aware function snoc which add an element at the tail of a list. def snoc[A,N](v:A, sl:SList[N,A]):SList[Succ[N],A] = sl match { case Nil => Cons(v,Nil) // case Cons(hd, tl) => snoc(v, tl) will result in compilation error. case Cons(hd, tl) => Cons(hd, snoc(v, tl)) }","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism"},{"location":"fp_scala_poly/#50054-parametric-polymorphism-and-adhoc-polymorphism","text":"","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism"},{"location":"fp_scala_poly/#learning-outcomes","text":"By this end of this lesson, you should be able to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes develop generic programming style code using Functor type class. make use of Option and Either to handle and manipulate errors and exceptions.","title":"Learning Outcomes"},{"location":"fp_scala_poly/#currying","text":"In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments. For example, def sum(x:Int, y:Int):Int = x + y can be rewritten into def sum_curry(x:Int)(y:Int):Int = x + y These two functions are equivalent except that Their invocations are different, e.g. sum(1,2) sum_curry(1)(2) It is easier to reuse the curried version to define other function, e.g. def plus1(x:Int):Int = sum_curry(1)(x)","title":"Currying"},{"location":"fp_scala_poly/#function-composition","text":"Every function and method in Scala is an object with a .compose() method. It works like the mathmethical composition. In math, let \\(g\\) and \\(f\\) be functions, then \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] Let g and f be Scala functions (or methods), then g.compose(f) is equivalent to x => g(f(x)) For example def f(x:Int):Int = 2 * x + 3 def g(x:Int):Int = x * x assert((g.compose(f))(2) == g(f(2)))","title":"Function Composition"},{"location":"fp_scala_poly/#generics","text":"Generics is also known as type variables. It enables a language to support parametric polymoprhism.","title":"Generics"},{"location":"fp_scala_poly/#polymorphic-functions","text":"Recall that the reverse function introduced in the last lesson def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace Int by a type variable A . def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) }","title":"Polymorphic functions"},{"location":"fp_scala_poly/#polymorphic-algebraic-datatype","text":"Recall that the following Algebraic Datatype from the last lesson. enum MyList { case Nil case Cons(x:Int, xs:MyList) } def mapML(l:MyList, f:Int => Int):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl, f)) } Same observation applies. MyList could have a generic element type A instead of Int and mapML should remains unchanged. enum MyList[A] { case Nil // type error case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A], f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML( tl, f)) } The caveat here is that the Scala compiler would complain about the Nil case above -- Error: ---------------------------------------------------------------------- 2 | case Nil | ^^^^^^^^ | cannot determine type argument for enum parent class MyList, | type parameter type A is invariant 1 error found To understand that error, we need to understand how Scala desugar the enum datatype. The above MyList datatype is desugared as enum MyList[A] { case Nil extends MyList[Nothing] // type error case Cons(x:A, xs:MyList[A]) extends MyList[A] } In which all sub cases within the enum type must be sub-class of the enum type. However it is not trivial for Nil . It can't be declared as a subtype of MyList[A] since type variable A is not mentioned in its definition, unlike Cons(x:A, xs:MyList[A]) . The best we can get is MyList[Nothing] where Nothing is the subtype of all other types in Scala. (As the dual, Any is the supertype of all other types in Scala). We are getting very close. Now we know that Nil extends MyList[Nothing] . If we can argue that MyList[Nothing] extends MyList[A] then we are all good. For MyList[Nothing] extends MyList[A] to hold, A must be covariant type parameter. In type system with subtyping, a type is covariant if it preserves the subtyping order when it is applied a type constructor. In the above situation, MyList[_] is a type constructor. The type parameter A is covarient because we note Nothing <: A for all A , thus MyList[Nothing] <: MyList[A] . a type is contravariant if it reverses the subtyping order when it is applied to a type constructor. For instance, given function type A => Boolean , the type parameter A is contravariant, because for A <: B , we have B => Boolean <: A => Boolean . (We can use functions of type B => Boolean in the context where a function A => Boolean is expected, but not the other way round.) a type is invariant if it does not preserve nor reverse the subtyping order when it is applied to a type constructor. Hence to fix the above type error with the MyList[A] datatype, we declared that A is covariant, +A . enum MyList[+A] { case Nil // type error is fixed. case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A])(f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl)(f)) } For easy of reasoning, we also rewrite mapML into currying style. Recall that we could make mapML function as a method of MyList enum MyList[+A] { case Nil case Cons(x:A, xs:MyList[A]) def mapML[B](f:A=>B):MyList[B] = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Scala Variances","title":"Polymorphic Algebraic Datatype"},{"location":"fp_scala_poly/#type-class","text":"Suppose we would like to convert some of the Scala value to JSON string. We could rely on overloading. def toJS(v:Int):String = v.toString def toJS(v:String):String = s\"'${v}'\" def toJS(v:Boolean):String = v.toString Given v is a Scala string value, s\"some_prefix ${v} some_suffix\" denotes a Scala string interpolation, which inserts v 's content into the \"place holder\" in the string \"some_prefix ${v} some_suffix\" where the ${v} is the place holder. However this becomes hard to manage as we consider complex datatype. enum Contact { case Email(e:String) case Phone(ph:String) } import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${toJS(e)}\" // compilation error case Phone(ph) => s\"'phone': ${toJS(ph)}\" // compilation error } When we try to define the toJS function for Contact datatype, we can't make use of the toJS function for string value because the compiler is confused that we are trying to make recursive calls. This is the first issue we faced. Let's pretend that the first issue has been addressed. There's still another issue. Consider case class Person(name:String, contacts:List[Contact]) case class Team(members:List[Person]) A case class is like a normal class we have seen earlier except that we can apply pattern matching to its values. Let's continue to overload toJS to handle Person and Team . def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${toJS(name)}, 'contacts':${toJS(contacts)} }\" } def toJS(cs:List[Contact]):String = { val j = cs.map(c=>toJS(c)).mkString(\",\") s\"[${j}]\" } def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${toJS(members)} }\" } def toJS(ps:List[Person]):String = { val j = ps.map(p=>toJS(p)).mkString(\",\") s\"[${j}]\" } The second issue is that the toJS(cs:List[Contact]) and toJS(ps:List[Person]) are the identical modulo the variable names. Can we combine two into one? def toJS[A](vs:List[A]):String = { val j = vs.map(v=>toJS(v)).mkString(\",\") // compiler error s\"[${j}]\" } However a compilation error occurs because the compiler is unable to resolve the toJS[A](v:A) used in the .map() . It seems that we need to give some extra information to the compiler so that it knows that when we use the above generic toJS we are referring to either Person or Contact , or whatever type that has a toJS defined. One solution to address the two above issues is to use type class . In Scala 3, a type class is defined by a polymoprhic trait and a set of type class instances. trait JS[A] { def toJS(v:A):String } given toJSInt:JS[Int] = new JS[Int]{ def toJS(v:Int):String = v.toString } given toJSString:JS[String] = new JS[String] { def toJS(v:String):String = s\"'${v}'\" } given toJSBoolean:JS[Boolean] = new JS[Boolean] { def toJS(v:Boolean):String = v.toString } given toJSContact(using jsstr:JS[String]):JS[Contact] = new JS[Contact] { import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${jsstr.toJS(e)}\" // compilation error is fixed case Phone(ph) => s\"'phone': ${jsstr.toJS(ph)}\" // compilation error is fixed } } given toJSPerson(using jsstr:JS[String], jsl:JS[List[Contact]]):JS[Person] = new JS[Person] { def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${jsstr.toJS(name)}, 'contacts':${jsl.toJS(contacts)} }\" } } given toJSTeam(using jsl:JS[List[Person]]):JS[Team] = new JS[Team] { def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${jsl.toJS(members)} }\" } } given toJSList[A](using jsa:JS[A]):JS[List[A]] = new JS[List[A]] { def toJS(as:List[A]):String = { val j = as.map(a=>jsa.toJS(a)).mkString(\",\") s\"[${j}]\" } } given defines a type class instance. An instance consists of a name and the context parameters (those with using ) and instance type. In the body of the type class instance, we instantiate an anonymous object that extends type class with the specific type and provide the defintion. We can refer to the particular type class instance by the instance's name. For instance import Contact.* val myTeam = Team( List( Person(\"kenny\", List(Email(\"kenny_lu@sutd.edu.sg\"))), Person(\"simon\", List(Email(\"simon_perrault@sutd.edu.sg\"))) )) toJSTeam.toJS(myTeam) yields 'team':{ 'members':['person':{ 'name':'kenny', 'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon', 'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] } We can also refer to the type class instance by the instace's type. For example, recall the last two instances. In the context of the toJSTeam , we refer to another instance of type JS[List[Person]] . Note that none of the defined instances has the required type. Scala is smart enought to synthesize it from the instances of toJSList and toJSPerson . Given the required type class instance is JS[List[Person]] , the type class resolver finds the instance toJSList having type JS[List[A]] , and it unifies both and find that A=Person . In the context of the instance toJSList , JS[A] is demanded. We can refine the required instance's type as JS[Person] , which is toJSPerson . Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. def printAsJSON[A](v:A)(using jsa:JS[A]):Unit = { println(jsa.toJS(v)) } printAsJSON(myTeam) Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming . In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects. In the next few section, we consider some common patterns in FP that are promoting generic programming.","title":"Type class"},{"location":"fp_scala_poly/#functor","text":"Recall that we have a map method for list datatype. val l = List(1,2,3) l.map(x => x + 1) Can we make map to work for other data type? For example enum BTree[+A] { case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } It turns out that extending map to different datatypes is similar to toJS function that we implemented earlier. We consider introducing a type class for this purpose. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } In the above type class definition, T[_] denotes a polymorphic type that of kind * => * . A kind is a type of types. In the above, it means Functor takes any type constructors T . When T is instantiated, it could be List[_] or BTree[_] and etc. (C.f. In the type class JS[A] , the type argument has kind * .) given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } Some example val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1)","title":"Functor"},{"location":"fp_scala_poly/#functor-laws","text":"All instances of functor must obey a set of mathematic laws for their computation to be predictable. Let i be a functor instance 1. Identity: i => map(i)(x => x) \\(\\equiv\\) x => x . When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: i=> map(i)(f.compose(g)) \\(\\equiv\\) (i => map(i)(f)).compose(j => map(j)(g)) . If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second.","title":"Functor Laws"},{"location":"fp_scala_poly/#foldable","text":"Similarly we can define a Foldable type class for generic and overloaded foldLeft ( and foldRight ). trait Foldable[T[_]]{ def foldLeft[A,B](t:T[B])(acc:A)(f:(A,B)=>A):A } given listFoldable:Foldable[List] = new Foldable[List] { def foldLeft[A,B](t:List[B])(acc:A)(f:(A,B)=>A):A = t.foldLeft(acc)(f) } given btreeFoldable:Foldable[BTree] = new Foldable[BTree] { import BTree.* def foldLeft[A,B](t:BTree[B])(acc:A)(f:(A,B)=>A):A = t match { case Empty => acc case Node(v, lft, rgt) => { val acc1 = f(acc,v) val acc2 = foldLeft(lft)(acc1)(f) foldLeft(rgt)(acc2)(f) } } } listFoldable.foldLeft(l)(0)((x:Int,y:Int) => x + y) btreeFoldable.foldLeft(t)(0)((x:Int,y:Int) => x + y)","title":"Foldable"},{"location":"fp_scala_poly/#option-and-either","text":"Recall in the earlier lesson, we encountered the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } An error occurs when we try to evalue a MathExp which contains a division by zero sub-expression. Executing import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) yields java.lang.ArithmeticException: / by zero at rs$line$2$.eval(rs$line$2:5) ... 41 elided Like other main stream languages, we could use try-catch statement to handle the exception. try { import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) } catch { case e:java.lang.ArithmeticException => println(\"handinging div by zero\") } One downside of this approach is that at compile type it is hard to track the unhandled exceptions, (in particular with the presence of Java unchecked exceptions.) A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes). Consider the following builtin Scala datatype Option // no need to run this. enum Option[+A] { case None case Some(v:A) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } When we execute eval(Div(Const(1), Minus(Const(2), Const(2)))) , we get None as the result instead of the exception. One advantage of this is that whoever is using eval function has to respect that its return type is Option[Int] instead of just Int therefore, a match must be applied before using the result to look out for potential None value. There are still two drawbacks. Firstly, the updated version of the eval function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue. We could instead of using Option , use the Either datatype // no need to run this, it's builtin enum Either[+A, +B] { case Left(v:A) case Right(v:B) } type ErrMsg = String def eval(e: MathExp): Either[ErrMsg,Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(0) => Left(s\"div by zero caused by ${e.toString}\") case Right(v2) => Right(v1 / v2) } } case MathExp.Const(i) => Right(i) } Executing eval(Div(Const(1), Minus(Const(2), Const(2)))) will yield Left(div by zero caused by Div(Const(1),Minus(Const(2),Const(2))))","title":"Option and Either"},{"location":"fp_scala_poly/#summary","text":"In this lesson, we have discussed how to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes how to develop generic programming style code using Functor type class. how to make use of Option and Either to handle and manipulate errors and exceptions.","title":"Summary"},{"location":"fp_scala_poly/#appendix","text":"","title":"Appendix"},{"location":"fp_scala_poly/#generalized-algebraic-data-type","text":"Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example. Firstly, we need some type acrobatics to encode nature numbers on the level of type. enum Zero { case Zero } enum Succ[A] { case Succ(v:A) } Next we define our GADT SList[S,A] which is a generic list of elements A and with size S . enum SList[S,+A] { case Nil extends SList[Zero,Nothing] // additional type constraint S = Zero case Cons[N,A](hd:A, tl:SList[N,A]) extends SList[Succ[N],A] // add'n type constraint S = Succ[N] } In the first subcase Nil , it is declared with the type of SList[Zero, Nothing] which indicates on type level that the list is empty. In the second case Cons , we define it to have the type SList[Succ[N],A] for some natural number N . This indicates on the type level that the list is non-empty. Having these information lifted to the type level allows us to define a type safe head function. import SList.* def head[A,N](sl:SList[Succ[N],A]):A = sl match { case Cons(hd, tl) => hd } Compiling head(Nil) yields a type error. Similarly we can define a size-aware function snoc which add an element at the tail of a list. def snoc[A,N](v:A, sl:SList[N,A]):SList[Succ[N],A] = sl match { case Nil => Cons(v,Nil) // case Cons(hd, tl) => snoc(v, tl) will result in compilation error. case Cons(hd, tl) => Cons(hd, snoc(v, tl)) }","title":"Generalized Algebraic Data Type"},{"location":"handout/","text":"50.054 Compiler Design and Program Analysis Course Handout This page will be updated regularly. Sync up often. Course Description This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis. Module Learning Outcomes By the end of this module, students are able to 1. Implement software solutions using functional programming language and applying design patterns 1. Define the essential components in a program compilation pipeline 1. Design a compiler for an imperative programming language 1. Optimise the generated machine codes by applying program analysis techniques 1. Detect bugs and security flaws in software by applying program analysis techniques Measurable Outcomes Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming Implement a type checker for the source language Develop a static analyser to eliminate dead codes Implement the register allocation algorithm in the target code generation module Develop a static analyser to identify potential security flaws in the source language Topics Functional Programming : Expression, Function, Conditional Functional Programming : List, Algebraic data type and Pattern Matching Functional Programming : Type class Functional Programming : Generic and Functor Functional Programming : Applicative and Monad Syntax analysis: Lexing Syntax analysis: Parsing (LL, LR, SLR) Syntax analysis: Parser Combinator Intermediate Representation: Pseudo-Assembly Intermediate Representation: SSA Semantic analysis: Dynamic Semantics Semantic analysis: Type checking Semantic analysis: Type Inference Semantic analysis: Sign analysis Semantic analysis: Liveness analysis Code Gen: Instruction selection Code Gen: Register allocation Memory Management Resource The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman Modern Compiler Implementation in ML by Andrew W. Appel Types and Programming Languages by Benjamin C. Pierce Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach Instructors Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Wednesday 3:00-4:30pm (please send email to arrange) Communication If you have course/assignment/project related questions, please post it on the dedicated MS teams channel. Assessment Mid-term 10% Project 35% Homework 20% Final 30% Class Participation 5% Things you need to prepare If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew. Make sure JVM >=11 is installed and ant is installed. Install Scala >= 3 https://www.scala-lang.org/download/ IDE: It's your choice, but VSCode works fine. Project The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. You may work as a team (up to max 3 members). Please register your team here . Lab 1 (10%, Deadline - 12 Nov 2023 23:59) Lab 2 (10%, Deadline - 26 Nov 2023 23:59) Lab 3 (15%, Deadline - 10 Dec 2023 23:59) Submission Policy and Plagiarism You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work. Schedule Week Session 1 Session 2 Session 3 Assessment 1 Intro FP: Expression, Function, Conditional, Recursion Cohort Problem 1 , Homework 1 Homework 1 no submission required 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2 , Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3 , Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4 , Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5 , Homework 3 (Cont'd) Homework 3 5% 6 Bottom-up Parsing IR: Pseudo-Assembly Cohort Problem 6 , Homework 4 7 Homework 4 5% 8 Mid-term , Semantic Analysis Dynamic Semantics Cohort Problem 7 Mid-term 10% 9 Static Semantics for SIMP Static Semantics for Lambda Calculus Cohort Problem 8 , Homework 5 Project Lab 1 10% 10 Public Holiday. No Class Scheduled Name Analysis, SSA Cohort Problem 9 11 Lattice, Sign Analysis Liveness Analysis Cohort Problem 10 Project Lab 2 10%, Homework 5 5% 12 Code Generation Information Flow Analysis Cohort Problem 11 13 Guest Lecture Memory Management Revision Project Lab 3 15% 14 Final Exam (13 Dec Wed 9:00AM-11:00AM) 30% Make Up and Alternative Assessment Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"50.054 Compiler Design and Program Analysis Course Handout"},{"location":"handout/#50054-compiler-design-and-program-analysis-course-handout","text":"","title":"50.054 Compiler Design and Program Analysis Course Handout"},{"location":"handout/#this-page-will-be-updated-regularly-sync-up-often","text":"","title":"This page will be updated regularly. Sync up often."},{"location":"handout/#course-description","text":"This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis.","title":"Course Description"},{"location":"handout/#module-learning-outcomes","text":"By the end of this module, students are able to 1. Implement software solutions using functional programming language and applying design patterns 1. Define the essential components in a program compilation pipeline 1. Design a compiler for an imperative programming language 1. Optimise the generated machine codes by applying program analysis techniques 1. Detect bugs and security flaws in software by applying program analysis techniques","title":"Module Learning Outcomes"},{"location":"handout/#measurable-outcomes","text":"Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming Implement a type checker for the source language Develop a static analyser to eliminate dead codes Implement the register allocation algorithm in the target code generation module Develop a static analyser to identify potential security flaws in the source language","title":"Measurable Outcomes"},{"location":"handout/#topics","text":"Functional Programming : Expression, Function, Conditional Functional Programming : List, Algebraic data type and Pattern Matching Functional Programming : Type class Functional Programming : Generic and Functor Functional Programming : Applicative and Monad Syntax analysis: Lexing Syntax analysis: Parsing (LL, LR, SLR) Syntax analysis: Parser Combinator Intermediate Representation: Pseudo-Assembly Intermediate Representation: SSA Semantic analysis: Dynamic Semantics Semantic analysis: Type checking Semantic analysis: Type Inference Semantic analysis: Sign analysis Semantic analysis: Liveness analysis Code Gen: Instruction selection Code Gen: Register allocation Memory Management","title":"Topics"},{"location":"handout/#resource","text":"The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman Modern Compiler Implementation in ML by Andrew W. Appel Types and Programming Languages by Benjamin C. Pierce Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach","title":"Resource"},{"location":"handout/#instructors","text":"Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Wednesday 3:00-4:30pm (please send email to arrange)","title":"Instructors"},{"location":"handout/#communication","text":"If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.","title":"Communication"},{"location":"handout/#assessment","text":"Mid-term 10% Project 35% Homework 20% Final 30% Class Participation 5%","title":"Assessment"},{"location":"handout/#things-you-need-to-prepare","text":"If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew. Make sure JVM >=11 is installed and ant is installed. Install Scala >= 3 https://www.scala-lang.org/download/ IDE: It's your choice, but VSCode works fine.","title":"Things you need to prepare"},{"location":"handout/#project","text":"The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. You may work as a team (up to max 3 members). Please register your team here . Lab 1 (10%, Deadline - 12 Nov 2023 23:59) Lab 2 (10%, Deadline - 26 Nov 2023 23:59) Lab 3 (15%, Deadline - 10 Dec 2023 23:59)","title":"Project"},{"location":"handout/#submission-policy-and-plagiarism","text":"You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.","title":"Submission Policy and Plagiarism"},{"location":"handout/#schedule","text":"Week Session 1 Session 2 Session 3 Assessment 1 Intro FP: Expression, Function, Conditional, Recursion Cohort Problem 1 , Homework 1 Homework 1 no submission required 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2 , Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3 , Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4 , Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5 , Homework 3 (Cont'd) Homework 3 5% 6 Bottom-up Parsing IR: Pseudo-Assembly Cohort Problem 6 , Homework 4 7 Homework 4 5% 8 Mid-term , Semantic Analysis Dynamic Semantics Cohort Problem 7 Mid-term 10% 9 Static Semantics for SIMP Static Semantics for Lambda Calculus Cohort Problem 8 , Homework 5 Project Lab 1 10% 10 Public Holiday. No Class Scheduled Name Analysis, SSA Cohort Problem 9 11 Lattice, Sign Analysis Liveness Analysis Cohort Problem 10 Project Lab 2 10%, Homework 5 5% 12 Code Generation Information Flow Analysis Cohort Problem 11 13 Guest Lecture Memory Management Revision Project Lab 3 15% 14 Final Exam (13 Dec Wed 9:00AM-11:00AM) 30%","title":"Schedule"},{"location":"handout/#make-up-and-alternative-assessment","text":"Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"Make Up and Alternative Assessment"},{"location":"introduction/","text":"50.054 - Introduction Module Description This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis. Module Learning Objective By the end of this module, you should be able to Comprehend and reason functional programming Develop functional program to solve real world problem Identify the major components in compiler development Explain and implement different techniques and algorithms used in compiler development What is compilation? Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc. A compiler is a software system that manages the program compliation process. What properties a good compiler should posess? An ideal compiler should be: Correct. The produced target program should behave the same as the the source program. Reliable. Any errors that could arise in the program should be detected and reported before execution. Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform. Some optional properties, User friendly. The error report should be comprehensive. Intelligent. Helps to automate some of the repeatitive tasks. ...","title":"50.054 - Introduction"},{"location":"introduction/#50054-introduction","text":"","title":"50.054 - Introduction"},{"location":"introduction/#module-description","text":"This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis.","title":"Module Description"},{"location":"introduction/#module-learning-objective","text":"By the end of this module, you should be able to Comprehend and reason functional programming Develop functional program to solve real world problem Identify the major components in compiler development Explain and implement different techniques and algorithms used in compiler development","title":"Module Learning Objective"},{"location":"introduction/#what-is-compilation","text":"Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc. A compiler is a software system that manages the program compliation process.","title":"What is compilation?"},{"location":"introduction/#what-properties-a-good-compiler-should-posess","text":"An ideal compiler should be: Correct. The produced target program should behave the same as the the source program. Reliable. Any errors that could arise in the program should be detected and reported before execution. Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform. Some optional properties, User friendly. The error report should be comprehensive. Intelligent. Helps to automate some of the repeatitive tasks. ...","title":"What properties a good compiler should posess?"},{"location":"ir_pseudo_assembly/","text":"50.054 - Pseudo Assembly Learning Outcomes By the end of this lesson, you should be able to Describe the syntax of the source language SIMP. Describe the syntax of the intermediate representation language pseudo-assembly. Describe how pseudo-assembly program is executed. Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code. Recap the compiler pipeline Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation. For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly. The SIMP Language We consider the syntax of SIMP as follows \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. For example (Example SIMP1) x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Pseudo Assembly We consider the Pseudo Assembly language as follows. \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] where \\(li\\) , a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\) . For simplicity, we use positive integers as labels. An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\) ), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use 0 to denote false and 1 to denote true . \\(r_{ret}\\) is a special register for the return statement. Example (PA1) 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Informal Specification of Pseudo Assembly We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. For example when we execute the above program with input = 2 |Program Counter| Local Memory | Next Instr | |---|---|---| | 1 | {input: 2, x : 2} | 2 | | 2 | {input: 2, x : 2, s : 0} | 3 | | 3 | {input: 2, x : 2, s : 0, c : 0} | 4 | | 4 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 5 | | 5 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 6 | | 6 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 7 | | 7 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 8 | | 8 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 4 | | 4 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 5 | | 5 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 6 | | 6 | {input: 2, x : 2, s : 1, c : 1, t : 1} | 7 | | 7 | {input: 2, x : 2, s : 1, c : 2, t : 1} | 8 | | 8 | {input: 2, x : 2, s : 1, c : 2, t : 1} | 4 | | 4 | {input: 2, x : 2, s : 1, c : 2, t : 0} | 5 | | 5 | {input: 2, x : 2, s : 1, c : 2, t : 0} | 9 | | 9 | {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} | 10 | | 10 | {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} | - | For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons. Maximal Munch Algorithm To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. \\[ \\begin{array}{rc} {\\tt (mAssign)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_s(X = E) \\vdash lis \\end{array} \\\\ \\end{array} \\] In case we have an assignment statement \\(X = E\\) , we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions. \\[ \\begin{array}{rc} {\\tt (mReturn)} & \\begin{array}{c} G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_s(return\\ X) \\vdash lis + [ l: ret ] \\end{array} \\end{array} \\] In case we have a return statement \\(return\\ E\\) , we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\) . We then generate a new label \\(l\\) , and append \\(l:ret\\) to the instructions. \\[ \\begin{array}{rc} {\\tt (mSequence)} & \\begin{array}{c} {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\ \\hline G_s(S_1;...;S_n) \\vdash lis_1 + ... + lis_n \\end{array} \\end{array} \\] In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation. \\[ \\begin{array}{rl} {\\tt (mIf)} & \\begin{array}{c} t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3' \\end{array} \\\\ \\end{array} \\] In case we have a if-else statement, we 1. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\) . 6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\) , which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) \\[ \\begin{array}{rl} {\\tt (mWhile)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2' \\end{array} \\\\ \\end{array} \\] In case we have a while statement, we 1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\) , which can be used later as the reference for the backward jump. 2. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\) . (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) The above summarizes all cases of \\(G_s(S)\\) . We now consider the sub algorithm, \\(G_a(d)(E)\\) , it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash [] \\] The case of \\(nop\\) statement is stratight-forward. \\[ \\begin{array}{rc} {\\tt (mConst)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\ \\hline G_a(X)(C) \\vdash [l : X \\leftarrow c] \\end{array} \\\\ \\end{array} \\] In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\) . where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. \\[ \\begin{array}{rcl} conv(true) & = & 1\\\\ conv(false) & = & 0\\\\ conv(C) & =& C \\end{array} \\] For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. \\[ \\begin{array}{rc} {\\tt (mVar)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(Y) \\vdash [l : X \\leftarrow Y] \\end{array} \\\\ \\end{array} \\] In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\) . The treat is similar to the case of \\({\\tt (Const)}\\) . \\[ \\begin{array}{rc} {\\tt (mParen)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_a(X)((E)) \\vdash lis \\end{array} \\end{array} \\] In the rule \\({\\tt (mParen)}\\) , we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression. \\[ \\begin{array}{rc} {\\tt (mOp)} & \\begin{array}{c} t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_1)(E_1) \\vdash lis_1 \\\\ t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_2)(E_2) \\vdash lis_2 \\\\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2] \\end{array} \\\\ \\end{array} \\] The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\) , we generate two temp variables \\(t_1\\) and \\(t_2\\) , and apply the generation function recursively to \\(E_1\\) and \\(E_2\\) . Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\) . For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation. Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement, Gs(x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;) ---> Gs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c < x { s = c + s; c = c + 1;}) ; Gs(return s); The derivation for Gs(x = input) is trivial, we apply \\({\\tt (mAssign)}\\) rule. Gs(x = input) ---> # using (mAssign) rule Ga(x)(input) ---> # using (mVar) rule ---> [ 1: x <- input ] Similarly we generate Gs( s = 0) ---> # using (mAssign) rule Ga(s)(0) ---> # using (mConst) rule ---> [ 2: s <- 0 ] and Gs(c = 0) ---> # using (mAssign) rule Ga(c)(0) ---> # using (mConst) rule ---> [ 3: c <- 0 ] Next we consider the while statement Gs( while c < x { s = c + s; c = c + 1; } ) ---> # using (mWhile) rule # the condition exp t is a fresh var Ga(t)(c<x) ---> # using (mOp) rule t1 is a fresh var Ga(t1)(x) ---> [4: t1 <- x] t2 is a fresh var Ga(t2)(c) ---> [5: t2 <- c] ---> [4: t1 <- x, 5: t2 <-c, 6: t <- t1 < t2 ] # the conditional jump, we generate a new label 7 reserved for whilecondjump # the while loop body Gs[ s = c + s; c = c + 1] ---> # (mSequence), (mOp) and (mOp) rules [ 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7 ] # end of the while loop [ 14: goto 4 ] # the conditional jump ---> [7: ifn t goto 15 ] ---> # putting altogther [4: t1 <- x, 5: t2 <- c, 6: t <- t1 < t2, 7: ifn t goto 15, 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7, 14: goto 4] Finally we convert the return statement Gs(return s) ---> # (mReturn) rule [15: r_ret <- s, 16: ret] Putting 1,2,3 together 1: x <- input 2: s <- 0 3: c <- 0 4: t1 <- x 5: t2 <- c 6: t <- t1 < t2 7: ifn t goto 15 8: t3 <- c 9: t4 <- s 10: t5 <- t3 + t4 11: t6 <- c 12: t7 <- 1 13: t8 <- t6 + t7 14: goto 4 15: rret <- s 16: ret As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction). Maximal Munch Algorithm V2 Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR. We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\^{e}, \\v{e}\\) . where \\(\\v{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\^{e}\\) is the \"result\" operand storing the final result of \\(\\v{e}\\) . The adjusted \\({\\tt (mConst)}\\) , \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows, \\[ \\begin{array}{rc} {\\tt (m2Const)} & \\begin{array}{c} G_e(C) \\vdash (conv(C), []) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} & \\begin{array}{c} G_e(Y) \\vdash (Y, []) \\end{array} \\end{array} \\] The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\^{e}\\) with an empty set of label instructions. \\[ \\begin{array}{rc} {\\tt (m2Paren)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ \\hline G_e((E)) \\vdash (\\^{e}, \\v{e}) \\end{array} \\end{array} \\] In the rule \\({\\tt (m2Paren)}\\) , we generate the results by recursivelly applying the algorithm to the inner expression. \\[ \\begin{array}{rc} {\\tt (m2Op)} & \\begin{array}{c} G_e(E_1) \\vdash (\\^{e}_1, \\v{e}_1) \\\\ G_e(E_2) \\vdash (\\^{e}_2, \\v{e}_2) \\\\ t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\ l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_e(E_1 OP E_2) \\vdash (t, \\v{e}_1 + \\v{e}_2 + [l : t \\leftarrow \\^{e}_1 OP \\^{e}_2]) \\end{array} \\\\ \\end{array} \\] In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the results for \\(E_1\\) and \\(E_2\\) , namely \\((\\^{e}_1, \\v{e}_1)\\) and \\((\\^{e}_2, \\v{e}_2)\\) . We then use them to synthesis the final output. The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\) . $$ \\begin{array}{rc} {\\tt (m2Assign)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label.} \\ \\hline G_s(X = E) \\vdash \\v{e} + [ l : X \\leftarrow \\^{e}] \\end{array} \\ \\end{array} $$ \\[ \\begin{array}{rc} {\\tt (m2Return)} & \\begin{array}{c} G_s(return\\ X) \\vdash \\v{e} + [ l_1 : r_{ret} \\leftarrow X, l_2: ret ] \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ \\^{e}\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\v{e} + lis_1 + lis_2' + lis_3' \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ \\^{e}\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash \\v{e} + lis_1 + lis_2' \\end{array} \\end{array} \\] By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.","title":"50.054 - Pseudo Assembly"},{"location":"ir_pseudo_assembly/#50054-pseudo-assembly","text":"","title":"50.054 - Pseudo Assembly"},{"location":"ir_pseudo_assembly/#learning-outcomes","text":"By the end of this lesson, you should be able to Describe the syntax of the source language SIMP. Describe the syntax of the intermediate representation language pseudo-assembly. Describe how pseudo-assembly program is executed. Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code.","title":"Learning Outcomes"},{"location":"ir_pseudo_assembly/#recap-the-compiler-pipeline","text":"Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation. For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly.","title":"Recap the compiler pipeline"},{"location":"ir_pseudo_assembly/#the-simp-language","text":"We consider the syntax of SIMP as follows \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. For example (Example SIMP1) x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;","title":"The SIMP Language"},{"location":"ir_pseudo_assembly/#pseudo-assembly","text":"We consider the Pseudo Assembly language as follows. \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] where \\(li\\) , a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\) . For simplicity, we use positive integers as labels. An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\) ), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use 0 to denote false and 1 to denote true . \\(r_{ret}\\) is a special register for the return statement. Example (PA1) 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret","title":"Pseudo Assembly"},{"location":"ir_pseudo_assembly/#informal-specification-of-pseudo-assembly","text":"We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. For example when we execute the above program with input = 2 |Program Counter| Local Memory | Next Instr | |---|---|---| | 1 | {input: 2, x : 2} | 2 | | 2 | {input: 2, x : 2, s : 0} | 3 | | 3 | {input: 2, x : 2, s : 0, c : 0} | 4 | | 4 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 5 | | 5 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 6 | | 6 | {input: 2, x : 2, s : 0, c : 0, t : 1} | 7 | | 7 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 8 | | 8 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 4 | | 4 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 5 | | 5 | {input: 2, x : 2, s : 0, c : 1, t : 1} | 6 | | 6 | {input: 2, x : 2, s : 1, c : 1, t : 1} | 7 | | 7 | {input: 2, x : 2, s : 1, c : 2, t : 1} | 8 | | 8 | {input: 2, x : 2, s : 1, c : 2, t : 1} | 4 | | 4 | {input: 2, x : 2, s : 1, c : 2, t : 0} | 5 | | 5 | {input: 2, x : 2, s : 1, c : 2, t : 0} | 9 | | 9 | {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} | 10 | | 10 | {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} | - | For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons.","title":"Informal Specification of Pseudo Assembly"},{"location":"ir_pseudo_assembly/#maximal-munch-algorithm","text":"To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. \\[ \\begin{array}{rc} {\\tt (mAssign)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_s(X = E) \\vdash lis \\end{array} \\\\ \\end{array} \\] In case we have an assignment statement \\(X = E\\) , we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions. \\[ \\begin{array}{rc} {\\tt (mReturn)} & \\begin{array}{c} G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_s(return\\ X) \\vdash lis + [ l: ret ] \\end{array} \\end{array} \\] In case we have a return statement \\(return\\ E\\) , we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\) . We then generate a new label \\(l\\) , and append \\(l:ret\\) to the instructions. \\[ \\begin{array}{rc} {\\tt (mSequence)} & \\begin{array}{c} {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\ \\hline G_s(S_1;...;S_n) \\vdash lis_1 + ... + lis_n \\end{array} \\end{array} \\] In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation. \\[ \\begin{array}{rl} {\\tt (mIf)} & \\begin{array}{c} t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3' \\end{array} \\\\ \\end{array} \\] In case we have a if-else statement, we 1. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\) . 6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\) , which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) \\[ \\begin{array}{rl} {\\tt (mWhile)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2' \\end{array} \\\\ \\end{array} \\] In case we have a while statement, we 1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\) , which can be used later as the reference for the backward jump. 2. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\) . (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) The above summarizes all cases of \\(G_s(S)\\) . We now consider the sub algorithm, \\(G_a(d)(E)\\) , it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash [] \\] The case of \\(nop\\) statement is stratight-forward. \\[ \\begin{array}{rc} {\\tt (mConst)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\ \\hline G_a(X)(C) \\vdash [l : X \\leftarrow c] \\end{array} \\\\ \\end{array} \\] In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\) . where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. \\[ \\begin{array}{rcl} conv(true) & = & 1\\\\ conv(false) & = & 0\\\\ conv(C) & =& C \\end{array} \\] For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. \\[ \\begin{array}{rc} {\\tt (mVar)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(Y) \\vdash [l : X \\leftarrow Y] \\end{array} \\\\ \\end{array} \\] In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\) . The treat is similar to the case of \\({\\tt (Const)}\\) . \\[ \\begin{array}{rc} {\\tt (mParen)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_a(X)((E)) \\vdash lis \\end{array} \\end{array} \\] In the rule \\({\\tt (mParen)}\\) , we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression. \\[ \\begin{array}{rc} {\\tt (mOp)} & \\begin{array}{c} t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_1)(E_1) \\vdash lis_1 \\\\ t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_2)(E_2) \\vdash lis_2 \\\\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2] \\end{array} \\\\ \\end{array} \\] The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\) , we generate two temp variables \\(t_1\\) and \\(t_2\\) , and apply the generation function recursively to \\(E_1\\) and \\(E_2\\) . Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\) . For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation. Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement, Gs(x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;) ---> Gs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c < x { s = c + s; c = c + 1;}) ; Gs(return s); The derivation for Gs(x = input) is trivial, we apply \\({\\tt (mAssign)}\\) rule. Gs(x = input) ---> # using (mAssign) rule Ga(x)(input) ---> # using (mVar) rule ---> [ 1: x <- input ] Similarly we generate Gs( s = 0) ---> # using (mAssign) rule Ga(s)(0) ---> # using (mConst) rule ---> [ 2: s <- 0 ] and Gs(c = 0) ---> # using (mAssign) rule Ga(c)(0) ---> # using (mConst) rule ---> [ 3: c <- 0 ] Next we consider the while statement Gs( while c < x { s = c + s; c = c + 1; } ) ---> # using (mWhile) rule # the condition exp t is a fresh var Ga(t)(c<x) ---> # using (mOp) rule t1 is a fresh var Ga(t1)(x) ---> [4: t1 <- x] t2 is a fresh var Ga(t2)(c) ---> [5: t2 <- c] ---> [4: t1 <- x, 5: t2 <-c, 6: t <- t1 < t2 ] # the conditional jump, we generate a new label 7 reserved for whilecondjump # the while loop body Gs[ s = c + s; c = c + 1] ---> # (mSequence), (mOp) and (mOp) rules [ 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7 ] # end of the while loop [ 14: goto 4 ] # the conditional jump ---> [7: ifn t goto 15 ] ---> # putting altogther [4: t1 <- x, 5: t2 <- c, 6: t <- t1 < t2, 7: ifn t goto 15, 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7, 14: goto 4] Finally we convert the return statement Gs(return s) ---> # (mReturn) rule [15: r_ret <- s, 16: ret] Putting 1,2,3 together 1: x <- input 2: s <- 0 3: c <- 0 4: t1 <- x 5: t2 <- c 6: t <- t1 < t2 7: ifn t goto 15 8: t3 <- c 9: t4 <- s 10: t5 <- t3 + t4 11: t6 <- c 12: t7 <- 1 13: t8 <- t6 + t7 14: goto 4 15: rret <- s 16: ret As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction).","title":"Maximal Munch Algorithm"},{"location":"ir_pseudo_assembly/#maximal-munch-algorithm-v2","text":"Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR. We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\^{e}, \\v{e}\\) . where \\(\\v{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\^{e}\\) is the \"result\" operand storing the final result of \\(\\v{e}\\) . The adjusted \\({\\tt (mConst)}\\) , \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows, \\[ \\begin{array}{rc} {\\tt (m2Const)} & \\begin{array}{c} G_e(C) \\vdash (conv(C), []) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} & \\begin{array}{c} G_e(Y) \\vdash (Y, []) \\end{array} \\end{array} \\] The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\^{e}\\) with an empty set of label instructions. \\[ \\begin{array}{rc} {\\tt (m2Paren)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ \\hline G_e((E)) \\vdash (\\^{e}, \\v{e}) \\end{array} \\end{array} \\] In the rule \\({\\tt (m2Paren)}\\) , we generate the results by recursivelly applying the algorithm to the inner expression. \\[ \\begin{array}{rc} {\\tt (m2Op)} & \\begin{array}{c} G_e(E_1) \\vdash (\\^{e}_1, \\v{e}_1) \\\\ G_e(E_2) \\vdash (\\^{e}_2, \\v{e}_2) \\\\ t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\ l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_e(E_1 OP E_2) \\vdash (t, \\v{e}_1 + \\v{e}_2 + [l : t \\leftarrow \\^{e}_1 OP \\^{e}_2]) \\end{array} \\\\ \\end{array} \\] In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the results for \\(E_1\\) and \\(E_2\\) , namely \\((\\^{e}_1, \\v{e}_1)\\) and \\((\\^{e}_2, \\v{e}_2)\\) . We then use them to synthesis the final output. The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\) . $$ \\begin{array}{rc} {\\tt (m2Assign)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label.} \\ \\hline G_s(X = E) \\vdash \\v{e} + [ l : X \\leftarrow \\^{e}] \\end{array} \\ \\end{array} $$ \\[ \\begin{array}{rc} {\\tt (m2Return)} & \\begin{array}{c} G_s(return\\ X) \\vdash \\v{e} + [ l_1 : r_{ret} \\leftarrow X, l_2: ret ] \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} & \\begin{array}{c} G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ \\^{e}\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\v{e} + lis_1 + lis_2' + lis_3' \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ G_e(E) \\vdash (\\^{e}, \\v{e}) \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ \\^{e}\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash \\v{e} + lis_1 + lis_2' \\end{array} \\end{array} \\] By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.","title":"Maximal Munch Algorithm V2"},{"location":"liveness_analysis/","text":"50.054 - Liveness Analysis Learning Outcomes Define the liveness analysis problem Apply lattice and fixed point algorithm to solve the liveness analysis problem Recall // SIMP1 x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; In the above program the statement t = s is redundant as t is not used. It can be statically detected by a liveness analysis. Liveness Analysis A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\) . Otherwise, the variable is considered not live or dead . Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite. By applying this analysis to the above program, we can find out at the program locations where variables must be dead. Defining the Lattice for Livenesss Analysis Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\) . Applying this approach the liveness analysis, we consider the powerset the set of all variables in the program. Let's recast the SIMP1 program into pseudo assembly, let's label it as PA1 1: x <- input 2: y <- 0 3: s <- 0 4: b <- y < x 5: ifn b goto 10 6: y <- y + 1 7: t <- s 8: s <- s + y 9: goto 4 10: rret <- s 11: ret In PA1 we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\) , if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\) , we see the following hasse diagram graph TD; N58[\"{b}\"] --- N64[\"{}\"] N59[\"{t}\"] --- N64[\"{}\"] N60[\"{s}\"] --- N64[\"{}\"] N61[\"{y}\"] --- N64[\"{}\"] N62[\"{x}\"] --- N64[\"{}\"] N63[\"{input}\"] --- N64[\"{}\"] N43[\"{t,b}\"] --- N58[\"{b}\"] N44[\"{s,b}\"] --- N58[\"{b}\"] N46[\"{y,b}\"] --- N58[\"{b}\"] N49[\"{x,b}\"] --- N58[\"{b}\"] N53[\"{input,b}\"] --- N58[\"{b}\"] N43[\"{t,b}\"] --- N59[\"{t}\"] N45[\"{s,t}\"] --- N59[\"{t}\"] N47[\"{y,t}\"] --- N59[\"{t}\"] N50[\"{x,t}\"] --- N59[\"{t}\"] N54[\"{input,t}\"] --- N59[\"{t}\"] N44[\"{s,b}\"] --- N60[\"{s}\"] N45[\"{s,t}\"] --- N60[\"{s}\"] N48[\"{y,s}\"] --- N60[\"{s}\"] N51[\"{x,s}\"] --- N60[\"{s}\"] N55[\"{input,s}\"] --- N60[\"{s}\"] N46[\"{y,b}\"] --- N61[\"{y}\"] N47[\"{y,t}\"] --- N61[\"{y}\"] N48[\"{y,s}\"] --- N61[\"{y}\"] N52[\"{x,y}\"] --- N61[\"{y}\"] N56[\"{input,y}\"] --- N61[\"{y}\"] N49[\"{x,b}\"] --- N62[\"{x}\"] N50[\"{x,t}\"] --- N62[\"{x}\"] N51[\"{x,s}\"] --- N62[\"{x}\"] N52[\"{x,y}\"] --- N62[\"{x}\"] N57[\"{input,x}\"] --- N62[\"{x}\"] N53[\"{input,b}\"] --- N63[\"{input}\"] N54[\"{input,t}\"] --- N63[\"{input}\"] N55[\"{input,s}\"] --- N63[\"{input}\"] N56[\"{input,y}\"] --- N63[\"{input}\"] N57[\"{input,x}\"] --- N63[\"{input}\"] N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\) . The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\) . Defining the Monotone Constraint for Liveness Analysis In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed. In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\) . In Sign Analysis the \\(join(s_i)\\) function is defined as the least upper bound of all the states that are preceding \\(s_i\\) in the control flow. In Liveness Analysis, we define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup succ(s_i) \\] where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l:ret\\) , \\(s_l = \\{\\}\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\) . \\[ \\begin{array}{rcl} var(r) & = & \\{ \\} \\\\ var(t) & = & \\{ t \\} \\\\ var(c) & = & \\{ \\} \\end{array} \\] By applying the PA program above we have s11 = {} s10 = join(s10) U {s} = {s} s9 = join(s9) = s4 s8 = (join(s8) - {s}) U {s, y} = (s9 - {s}) U {s, y} s7 = (join(s7) - {t}) U {s} = (s8 - {t}) U {s} s6 = (join(s6) - {y}) U {y} = (s7 - {y}) U {y} s5 = join(s5) U {b} = s6 U s10 U {b} s4 = (join(s4) - {b}) U {y, x} = (s5 - {b}) U {y, x} s3 = join(s3) - {s} = s4 - {s} s2 = join(s2) - {y} = s3 - {y} s1 = (join(s1) - {x}) U {input} = (s2 - {x}) U {input} For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order. By turning the above equation system to a monotonic function \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) & = & \\left ( \\begin{array}{c} \\{\\}, \\\\ \\{s\\}, \\\\ s_4, \\\\ (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\ (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\ (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\ s_6 \\cup s_{10} \\cup \\{b\\}, \\\\ (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\ s_4 - \\{s\\}, \\\\ s_3 - \\{y\\}, \\\\ (s_2 - \\{x\\}) \\cup \\{ input \\} \\end{array} \\right ) \\end{array} \\] Question, can you show that \\(f_1\\) is a monotonic function? By applying the naive fixed point algorithm (or its optimized version) with starting states s1 = ... = s11 = {} , we solve the above constraints and find s11 = {} s10 = {s} s9 = {y,x,s} s8 = {y,x,s} s7 = {y,x,s} s6 = {y,x,s} s5 = {y,x,s,b} s4 = {y,x,s} s3 = {y, x} s2 = {x} s1 = {input} From which we can identify at least two possible optimization opportunities. t is must be dead throughout the entire program. Hence instruction 7 is redundant. input only lives at instruction 1. If it is not holding any heap references, it can be freed. x,y,b lives until instruction 9. If they are not holding any heap references, they can be freed. Forward vs Backward Analysis Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis . Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a backward analysis . For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis. May Analysis vs Must Analysis Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\) ) instead of \\(\\sqcup\\) (which is \\(\\cup\\) ).","title":"50.054 - Liveness Analysis"},{"location":"liveness_analysis/#50054-liveness-analysis","text":"","title":"50.054 - Liveness Analysis"},{"location":"liveness_analysis/#learning-outcomes","text":"Define the liveness analysis problem Apply lattice and fixed point algorithm to solve the liveness analysis problem","title":"Learning Outcomes"},{"location":"liveness_analysis/#recall","text":"// SIMP1 x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; In the above program the statement t = s is redundant as t is not used. It can be statically detected by a liveness analysis.","title":"Recall"},{"location":"liveness_analysis/#liveness-analysis","text":"A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\) . Otherwise, the variable is considered not live or dead . Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite. By applying this analysis to the above program, we can find out at the program locations where variables must be dead.","title":"Liveness Analysis"},{"location":"liveness_analysis/#defining-the-lattice-for-livenesss-analysis","text":"Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\) . Applying this approach the liveness analysis, we consider the powerset the set of all variables in the program. Let's recast the SIMP1 program into pseudo assembly, let's label it as PA1 1: x <- input 2: y <- 0 3: s <- 0 4: b <- y < x 5: ifn b goto 10 6: y <- y + 1 7: t <- s 8: s <- s + y 9: goto 4 10: rret <- s 11: ret In PA1 we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\) , if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\) , we see the following hasse diagram graph TD; N58[\"{b}\"] --- N64[\"{}\"] N59[\"{t}\"] --- N64[\"{}\"] N60[\"{s}\"] --- N64[\"{}\"] N61[\"{y}\"] --- N64[\"{}\"] N62[\"{x}\"] --- N64[\"{}\"] N63[\"{input}\"] --- N64[\"{}\"] N43[\"{t,b}\"] --- N58[\"{b}\"] N44[\"{s,b}\"] --- N58[\"{b}\"] N46[\"{y,b}\"] --- N58[\"{b}\"] N49[\"{x,b}\"] --- N58[\"{b}\"] N53[\"{input,b}\"] --- N58[\"{b}\"] N43[\"{t,b}\"] --- N59[\"{t}\"] N45[\"{s,t}\"] --- N59[\"{t}\"] N47[\"{y,t}\"] --- N59[\"{t}\"] N50[\"{x,t}\"] --- N59[\"{t}\"] N54[\"{input,t}\"] --- N59[\"{t}\"] N44[\"{s,b}\"] --- N60[\"{s}\"] N45[\"{s,t}\"] --- N60[\"{s}\"] N48[\"{y,s}\"] --- N60[\"{s}\"] N51[\"{x,s}\"] --- N60[\"{s}\"] N55[\"{input,s}\"] --- N60[\"{s}\"] N46[\"{y,b}\"] --- N61[\"{y}\"] N47[\"{y,t}\"] --- N61[\"{y}\"] N48[\"{y,s}\"] --- N61[\"{y}\"] N52[\"{x,y}\"] --- N61[\"{y}\"] N56[\"{input,y}\"] --- N61[\"{y}\"] N49[\"{x,b}\"] --- N62[\"{x}\"] N50[\"{x,t}\"] --- N62[\"{x}\"] N51[\"{x,s}\"] --- N62[\"{x}\"] N52[\"{x,y}\"] --- N62[\"{x}\"] N57[\"{input,x}\"] --- N62[\"{x}\"] N53[\"{input,b}\"] --- N63[\"{input}\"] N54[\"{input,t}\"] --- N63[\"{input}\"] N55[\"{input,s}\"] --- N63[\"{input}\"] N56[\"{input,y}\"] --- N63[\"{input}\"] N57[\"{input,x}\"] --- N63[\"{input}\"] N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\) . The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\) .","title":"Defining the Lattice for Livenesss Analysis"},{"location":"liveness_analysis/#defining-the-monotone-constraint-for-liveness-analysis","text":"In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed. In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\) . In Sign Analysis the \\(join(s_i)\\) function is defined as the least upper bound of all the states that are preceding \\(s_i\\) in the control flow. In Liveness Analysis, we define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup succ(s_i) \\] where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l:ret\\) , \\(s_l = \\{\\}\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\) . \\[ \\begin{array}{rcl} var(r) & = & \\{ \\} \\\\ var(t) & = & \\{ t \\} \\\\ var(c) & = & \\{ \\} \\end{array} \\] By applying the PA program above we have s11 = {} s10 = join(s10) U {s} = {s} s9 = join(s9) = s4 s8 = (join(s8) - {s}) U {s, y} = (s9 - {s}) U {s, y} s7 = (join(s7) - {t}) U {s} = (s8 - {t}) U {s} s6 = (join(s6) - {y}) U {y} = (s7 - {y}) U {y} s5 = join(s5) U {b} = s6 U s10 U {b} s4 = (join(s4) - {b}) U {y, x} = (s5 - {b}) U {y, x} s3 = join(s3) - {s} = s4 - {s} s2 = join(s2) - {y} = s3 - {y} s1 = (join(s1) - {x}) U {input} = (s2 - {x}) U {input} For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order. By turning the above equation system to a monotonic function \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) & = & \\left ( \\begin{array}{c} \\{\\}, \\\\ \\{s\\}, \\\\ s_4, \\\\ (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\ (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\ (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\ s_6 \\cup s_{10} \\cup \\{b\\}, \\\\ (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\ s_4 - \\{s\\}, \\\\ s_3 - \\{y\\}, \\\\ (s_2 - \\{x\\}) \\cup \\{ input \\} \\end{array} \\right ) \\end{array} \\] Question, can you show that \\(f_1\\) is a monotonic function? By applying the naive fixed point algorithm (or its optimized version) with starting states s1 = ... = s11 = {} , we solve the above constraints and find s11 = {} s10 = {s} s9 = {y,x,s} s8 = {y,x,s} s7 = {y,x,s} s6 = {y,x,s} s5 = {y,x,s,b} s4 = {y,x,s} s3 = {y, x} s2 = {x} s1 = {input} From which we can identify at least two possible optimization opportunities. t is must be dead throughout the entire program. Hence instruction 7 is redundant. input only lives at instruction 1. If it is not holding any heap references, it can be freed. x,y,b lives until instruction 9. If they are not holding any heap references, they can be freed.","title":"Defining the Monotone Constraint for Liveness Analysis"},{"location":"liveness_analysis/#forward-vs-backward-analysis","text":"Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis . Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a backward analysis . For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis.","title":"Forward vs Backward Analysis"},{"location":"liveness_analysis/#may-analysis-vs-must-analysis","text":"Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\) ) instead of \\(\\sqcup\\) (which is \\(\\cup\\) ).","title":"May Analysis vs Must Analysis"},{"location":"name_analysis/","text":"50.054 - Name Analysis Learning Outcomes Articulate the purpose of name analysis. Describe the properties of the static single assignment forms. Implement the static single assignment construction and deconstruction algorithms. What is Name Analysis Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name). Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement. Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit. What is the scope of the variable? Has the variable been declared before used? Where is the defined variable used? Variable Scope Consider the following Python program, x = -1 def f(): x = 1 return g() def g(): print(x) f() When the program is executed, we observe -1 being printed. The variable x=1 in f() does not modify the x=-1 in the outer scope. Hence when g() is called, the variable x being printed is from the global scope x=-1 . This is known as static scoping . Static Variable Scoping For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree). graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; Main --> g; g --> usex1[\"print(x)\"]; Thus the print(x) of g uses the x defined in its parent node. Dynamic Variable Scoping For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. $x = -1; sub f { local $x = 1; return g(); } sub g { print $x; } f() In the above, it is the same program coded in perl . Except that in perl, variables with local are defined using dynamic scoping. As a result, 1 is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; f --> g; g --> usex1[\"print(x)\"]; As illustrated by the dynamic call graph above, the variable x in print(x) refers to g 's caller, i.e. f , which is 1 . More On Static Variable Scoping Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation. Consider the following Python program. def main(argv): x = 1 if len(argv) == 0: x = 2 else: y = 1 print(y) when the input argv is a non-empty list, the function main prints 1 as results. However when argv is an empty list, a run-time error arises. Consider the \"nearly-the-same\" program in Java. class Main { public static int main(String[] argv) { int x = 1; if (argv.length > 0){ x = 2; } else { int y = 1; } System.out.println(y.toString()); return 1; } } Java returns a compilation error, complaining variable y being use in the System.out.println function can't be resolved. The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable y 's scope is only within the else branch but not outside. In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. However how might we detect the run-time error similar to what we've observed from the last Python example? Let's recast the example in SIMP, let's call it SIMP_ERR1 // SIMP_ERR1 x = 1; if input == 0 { x = 2; } else { y = 1; } return y; The above program will cause an error when input == 0 . It is typeable based on the type inference algorithm we studied in the previous class. Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program. // PA_ERR1 1: x <- 1 2: t <- input == 0 3: ifn t goto 6 4: x <- 2 5: goto 7 6: y <- 1 7: rret <- y 8: ret Same error arises when input == 0 . Static Single Assignment form Static Single Assignment (SSA) form is an intermediate representation widely used in compiler design and program verification. In a static single assignment form, Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. \\(\\phi\\) -assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). SSA form construction is one of the effective ways to analysis the scope of variables the use-def relationship of variables Unstructured SSA Form Suppose we extend the pseudo assembly with \\(\\phi\\) -assignment statements, \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : \\overline{\\phi}\\ i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt PhiAssignment) & \\phi & ::= & d \\leftarrow phi(\\overline{l:s}) \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\mid ... \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\) . (which could be empty) before the actual instruction \\(i\\) . When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax. we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms. Suppose we have the following pseudo assembly program // PA1 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that variables s and c are re-assigned in the loop. The SSA form of the above is // SSA_PA1 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: rret <- s1 10: ret In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable. There are two possible preceding instructions that lead us to the following instruction 4: s1 <- phi(3:s0, 9:s2) c1 <- phi(3:c0, 9:c2) namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign s0 to s1 and c0 to c1 . Otherwise, s2 is assigned to s1 and c2 is assigned to c1 . To cater for the phi assignment, we extend the small step operational semantics from \\( \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) \\) to \\[P \\vdash (L, li, p) \\longrightarrow (L', li', p')\\] The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l: d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\\\ \\\\ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\\\ \\\\ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\\\ \\\\ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l) \\end{array} \\\\ {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l) \\end{array} \\\\ {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l) \\end{array} \\end{array} \\] All the existing rules are required some minor changes to accomodate the third component in the program context. The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments. In the presence of phi-assignments, we need the following rules to guide the execution. \\[ \\begin{array}{rc} {\\tt (pPhi1)} & \\begin{array}{c} (L, l: []\\ i, p) \\longrightarrow (L, l: i, p) \\end{array} \\\\ \\\\ {\\tt (pPhi2)} & \\begin{array}{c} l_i = p\\ \\ \\ c_i = L(s_i) \\\\ j \\in [1,i-1]: l_j \\neq p \\\\ \\hline (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p) \\end{array} \\end{array} \\] The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\) 's operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\) , i.e. \\(c_i\\) . Add the new entry \\((d,c_i)\\) to the local environment \\(L\\) . Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\) . Given \\(input = 1\\) , excuting SSA_PA1 yields the following derivation P |- {(input,1)}, 1: x0 <- input, undef ---> # (pTempVar) P |- {(input,1), (x0,1)}, 2: s0 <- 0, 1 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0)}, 3: c0 <- 0, 2 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 <- c1 < x0, 3 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 <- c1 < x0, 3 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 <- c1 + s1, 5 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 <- c1 + 1, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7 ---> # (pGoto) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 <- c1 < x0, 8 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 <- c1 < x0, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret <- s1, 5 ---> # (pTempVar) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9 Minimality One may argue that instead of generating SSA_PA1 , one might generate the following static single assignment // SSA_PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: s3 <- phi(5:s1) rret <- s3 10: ret which will yield the same output. However we argue that SSA_PA1 is preferred as it has the minimal number of phi assignments. SSA Construction Algorithm The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al. https://doi.org/10.1145/115372.115320 The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form. Control flow graph We can model a Pseudo Assembly program using a graph, namely the contorl flow graph. For example, PA1 can be represented as the following Control flow graph Graph1_PA1 graph TD; B1(\"1: x <- input 2: s <- 0 3: c <- 0\")-->B2; B2-->B3; B2(\"4: t <- c < x 5: ifn t goto 9\")-->B4(\"9: rret <- s 10: ret\"); B3(\"6: s <- c + s 7: c <- c + 1 8: goto 4\")-->B2; For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it Graph2_PA1 graph TD; V1(\"1: x <- input\") --> V2; V2(\"2: s <- 0\") --> V3; V3(\"3: c <- 0\") --> V4; V4(\"4: t <- c < x\") --> V5; V5(\"5: ifn t goto 9\") --> V9; V9(\"9: rret <- s\") --> V10(\"10: ret\") V5(\"5: ifn t goto 9\") --> V6; V6(\"6: s <- c + s\") --> V7; V7(\"7: c <- c + 1\") --> V8; V8(\"8: goto 4\") --> V4; Now we refer to the vertex in a control flow graph by the label. The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG. Identifying the \"right\" locations Definition 1 - Graph Let \\(G\\) be a graph, \\(G = (V, E)\\) , where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\) , \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\) . Occassionally, we also refer to a vertex as a node in the graph. For convenience, we also write \\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and \\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\) . Definition 2 - Path Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say a path from \\(v_1\\) to \\(v_2\\) , written as \\(path(v_1,v_2)\\) , exists iff \\(v_1 = v_2\\) or the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\) . For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\) . Definition 3 - Connectedness Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\) , iff \\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\) . Definition 4 - Source and Sink Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Assumption Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\) , we have \\(connect(v_1,v_2)\\) Has only one source vertex, which means there is only one entry point to the program. Has only one sink vertex, which means there is only one return statement. Definition 5 - Dominance Relation Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) dominates \\(v_2\\) , written as \\(v_1 \\preceq v_2\\) , iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\) . In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\) , we definitely pass through location \\(v_1\\) . For instance, in the earlier control flow graph for Graph2_PA1 , the vertex 1 dominates all vertices. the vertex 4 dominates itself, the vertices 5,6,7,8,9,10 . Lemma 1 - Dominance is transitive \\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\) . Lemma 2 - Domaince is reflexive For any vertex \\(v\\) , we have \\(v \\preceq v\\) . Defintion 6 - Strict Dominance We say \\(v_1\\) stricly domainates \\(v_2\\) , written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\) . Defintion 7 - Immediate Dominator We say \\(v_1\\) is the immediate dominator of \\(v_2\\) , written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\) . Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists. Dominator Tree Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\) . Each vertex \\(v \\in G\\) forms a node in the dominator tree. For vertices \\(v_1, v_2 \\in G\\) , \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\) . For example, from the CFG Graph2_PA1 , we construct a dominator tree Tree2_PA1 , as follows, graph TD; 1 --> 2; 2 --> 3; 3 --> 4; 4 --> 5; 5 --> 6; 6 --> 7; 7 --> 8; 5 --> 9; 9 --> 10; Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\) . Definition 8 - Dominance Frontier Let \\(v\\) be vertex in a graph \\(G\\) , we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$ In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\) ). For instance, in our running example, the dominance frontier of vertex 6 is the set containing vertex 4 This is because vertex 8 is one of the predecesors of the vertex 4 and vertex 8 is dominated by vertex 6 , but not the vertex 4 is not domainated by vertex 6 . Question: what is the dominance frontier of vertex 5 ? Computing Dominance Frontier The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG. Re-definining Dominance Frontier The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\) . We define $$ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G) ~~~(E1) $$ where \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] and \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] \\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\) \\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\) ) that are not dominated by \\(v\\) . \\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\) , by finding vertices \\(w\\) in \\(v\\) 's dominance frontier, such that \\(w\\) is not dominated by \\(v\\) 's immediate dominator (i.e. \\(v\\) 's parent in the dominator tree). Cytron et al shows that \\((E1)\\) defines the same result as Definition 6. Dominance frontier algorithm As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex). The algorithm is structured as follows For each vertex \\(v\\) by traversing the dominator tree bottom up: compute \\(df_{local}(v,G)\\) compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\) , which can be looked up from the a memoization table. save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table. For instance, we make use of Graph2_PA1 and Tree2_PA1 to construct the following memoization table Table2_PA1 vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} From the above table, we conclude that variables that are updated in vertices 5,6,7,8 should be merged via phi-assignments at the entry point of vertex 4 . As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid (x,y)\\in G \\wedge idom(y) \\neq x \\}\\) Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\) Note that in Cytron's paper, they include two special vertices, entry the entry vertex, and exit as the exit, and entry dominates everything, and exit is only dominated by entry . The purpose is to handle langugage allowing multiple return statements. Definition 9 - Iterative Dominance Frontier As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\) , a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\) . However inserting a phi assignment at the dominance fronter of \\(v\\) introduces a new location of modifying the variable \\(x\\) . This leads to some \"cascading effect\" in computing the phi-assignment locations. We extend the dominance frontier to handle a set of vertices. Let \\(S\\) denote a set of vertices of a graph \\(G\\) . We define \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] We define the iterative dominance frontier recursively as follows \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\) , i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound. It follows that if a variable \\(x\\) is modified in locations \\(S\\) , then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\) . SSA construction algorithm Given the control flow graph \\(G\\) , the dominator tree \\(T\\) , and the dominance frontier table \\(DFT\\) , the SSA construction algorithm consists of two steps. insert phi assignments to the original program \\(P\\) . rename variables to ensure the single assignment property. Inserting Phi assignments Before inserting the phi assignments to \\(P\\) , we need some intermediate data structure. A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables. \\((l, S) \\in E\\) implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\) . \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation. Input: the original program P , can be viewed as a list of labeled instructions. Output: the modified program Q . can be viewed as a list of labeled instructions. The phi-assignment insertion process can be described as follows, Q = List() for each l:i in P match E.get(l) with case None add l:i to Q case Some(xs) phis = xs.map( x => x <- phi( k:x | (k in pred(l,G))) if phis has more than 1 operand, add l:phis i to Q \\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\) . For example, given PA1 , variable \\(x\\) is modified at 1 variable \\(s\\) is modified at 2,6 variable \\(c\\) is modified at 3,7 variable \\(t\\) is modified at 4 We construct \\(E\\) by consulting the dominance frontier table Table2_PA1 . E = Map( 4 -> Set(\"s\",\"c\", \"t\") ) which says that in node/vertex 4 , we should insert the phi-assignments for variable s and c . Now we apply the above algorithm to PA1 which generates // PRE_SSA_PA1 1: x <- input 2: s <- 0 3: c <- 0 4: s <- phi(3:s, 8:s) c <- phi(3:c, 8:c) t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that when we try to insert the phi assignment for t at 4 , we realize that there is only one operand. This is because t is not defined before label 4 . In this case we remove the phi assignment for t . Renaming Variables Given an intermediate output like PRE_SSA_PA1 , we need to rename the variable so that there is only one assignment for each variable. Inputs: a dictionary of stacks K where the keys are the variable names in the original PA program. e.g. K(x) returns the stack for variable x . the input program in with phi assignment but oweing the variable renaming, e.g. PRE_SSA_PA1 . We view the program as a dictionary mapping labels to labeled instructions. For each variable x in the program, initialize K(x) = Stack() . Let label l be the root of the dominator tree \\(T\\) . Let vars be an empty list Match P(l) with case l: phis r <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) set Q(l) to l: phis' r <- s' case l: phis r <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) set Q(l) to l: phis' r <- s1' op s2' case l: phis x <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s' case l: phis x <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s1' op s2' case l: phis ifn t goto l' (phis', K, result_list) = processphi(phis, K, vars) t' = ren(K, t) set Q(l) to l: phis' ifn t' goto l' case l: phis ret (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' ret case l: phis goto l' (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' goto l' For each successor k of l in the CFG \\(G\\) R = if k in Q { Q } else { R } Pattern match R(k) case k: phis i for each x <- phi(j:x', m:x'') in phis if K(origin(x)) is empty, do not add this phi assignment in the result list**. if j == l , x <- phi(j:ren(K,x'), m:x'') into the result list if m == l , x <- phi(j:x', m:ren(K,x'')) into the result list the result list is phis' update R(k) to k: phis' i case others , no change Recursively apply step 3 to the children of l in the \\(T\\) . For each x in vars , K(x).pop() Where ren(K, s) is defined as ren(K,c) = c ren(K, input) = input ren(K, r) = r ren(K, t) = K(t).peek() match case None => error(\"variable use before being defined.\") case Some(i) => t_i and next(K, x) is defined as next(K, x) = K(x).peek() match case None => K(x).push(1) 0 case Some(i) => K(x).push(i+1) i and processphi(phis, K) is defined as prcessphi(phis, K, vars) = foreach x <- phi(j:x', k:x'') in phis i = K(x).peek() + 1 K(x).push(i) append x to vars put x_i <- phi(j:x', k:x'') into result_list return (result_list, K, vars) and stem(x) returns the original version of x before renaming, e.g. stem(x) = x and stem(x1) = x . We assume there exists some book-keeping mechanism to keep track of that the fact that x is the origin form of x_1 . Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch. We describe the application the algorithm to PRE_SSA_PA1 (with the dominator tree Tree2_PA1 and CFG Graph1_PA1 ) with the following table. label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 1:x<-input 1:x0<-input {x:[0], s:[], c:[], t:[]} {1:{x}} 2 2:s<-0 2:s0<-0 {x:[0], s:[0], c:[], t:[]} {1:{x}, 2:{s}} 3 3:c<-0 3:c0<-0 {x:[0], s:[0], c:[0], t:[]} 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x {1:{x}, 2:{s}. 3:{c}} 4 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x 4:s1<-phi(3:s0,8:s);c1<-phi(3:c0,8:c);t0<-c1<x0 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 5 5:ifn t goto 9 5:ifn t0 goto 9 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 6 6:s<-c+s 6:s2<-c1+s1 {x:[0], s:[0,1,2], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}} 7 7:c<-c+1 7:c2<-c1+1 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}} 8 8:goto 4 8:goto 4 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} 4:s1<-phi(3:s0,8:s2);c1<-phi(3:c0,8:c2);t0<-c1<x0 9 9:rret<-s 9:rret<-s1 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 10 10:ret 10:ret {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} The label column denotes the current label being considered. The P(l) column denotes the input labeled instruction being considered. The Q(l) column denotes the output labeled instruction. The K column denotes the set of stacks after the current recursive call. The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q) The Q(succ(l)) column denotes the modified successor instruction in Q. The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call). The above derivation eventually yield SSA_PA1 . Note that in case of a variable being use before initialized, ren(K, t) will raise an error. SSA back to Pseudo Assembly To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating SSA_PA1 back to PA while keeping the renamed variables, we have // PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 4: t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: s1 <- s2 8: c1 <- c2 8: goto 4 9: rret <- s1 10: ret In the above we break the phi-assignments found in 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 into PhiFor3 s1 <- s0 // for label 3 c1 <- c0 and PhiFor8 s1 <- s2 // for label 8 c1 <- c2 We move PhiFor3 to label 3 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 and PhiFor8 to label 8 8: s1 <- s2 8: c1 <- c2 8: goto 4 The \"moving\" phi-assignment operation can be defined in the following algorithm. Relocating the phi-assignments Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\) ) For each \\(l: \\overline{\\phi}\\ i \\in P\\) , append \\(l: i\\) to \\(Q\\) . For each \\(l: \\overline{\\phi}\\ i\\) . For each x = phi(l1:x1, l2:x2) in \\(\\overline{\\phi}\\) append l1:x <- x1 and l2:x <- x2 to \\(Q\\) . note that the relocated assignment must be placed before the control flow transition from l1 to succ(l1) (and l2 to succ(l2) ) Sort \\(Q\\) by labels using a stable sorting algorithm. Now since there are repeated labels in PA2 , we need an extra relabelling step to convert PA2 to PA3 // PA3 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- s0 5: c1 <- c0 6: t0 <- c1 < x0 7: ifn t0 goto 11 8: s2 <- c1 + s1 9: c2 <- c1 + 1 10: s1 <- s2 11: c1 <- c2 12: goto 4 13: rret <- s1 14: ret Relabelling This re-labeling step can be described in the following algorithm. Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\) ) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. Initialize a counter c = 1 , Initialize a mapping from old label to new label, M = Map() . Initialize \\(Q\\) as an empty list For each l: i \\(\\in P\\) M = M + (l -> c) incremeant c by 1 For each l: i \\(\\in P\\) append M(l): relabel(i, M) to \\(Q\\) where relabel(i, M) is defined as follows relabel(ifn t goto l,M) = ifn t goto M(l) relabel(goto l, M) = goto M(l) relabel(i, M) = i Structured SSA Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Can be converted into a structured SSA x1 = input; s1 = 0; c1 = 0; join { s2 = phi(s1,s3); c2 = phi(c1,c3); } while c2 < x1 { s3 = c2 + s2; c3 = c2 + 1; } return s2; In the above SSA form, we have a join ... while ... loop. The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and also the body of the loop. (Similarly we can introduce a if ... else ... join ... statement). Structured SSA allows us to conduct name analysis closer to the source language. conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. perform code obfuscation. Futher Readings https://dl.acm.org/doi/10.1145/2955811.2955813 https://dl.acm.org/doi/abs/10.1145/3605156.3606457 https://dl.acm.org/doi/10.1145/202530.202532","title":"50.054 - Name Analysis"},{"location":"name_analysis/#50054-name-analysis","text":"","title":"50.054 - Name Analysis"},{"location":"name_analysis/#learning-outcomes","text":"Articulate the purpose of name analysis. Describe the properties of the static single assignment forms. Implement the static single assignment construction and deconstruction algorithms.","title":"Learning Outcomes"},{"location":"name_analysis/#what-is-name-analysis","text":"Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name). Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement. Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit. What is the scope of the variable? Has the variable been declared before used? Where is the defined variable used?","title":"What is Name Analysis"},{"location":"name_analysis/#variable-scope","text":"Consider the following Python program, x = -1 def f(): x = 1 return g() def g(): print(x) f() When the program is executed, we observe -1 being printed. The variable x=1 in f() does not modify the x=-1 in the outer scope. Hence when g() is called, the variable x being printed is from the global scope x=-1 . This is known as static scoping .","title":"Variable Scope"},{"location":"name_analysis/#static-variable-scoping","text":"For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree). graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; Main --> g; g --> usex1[\"print(x)\"]; Thus the print(x) of g uses the x defined in its parent node.","title":"Static Variable Scoping"},{"location":"name_analysis/#dynamic-variable-scoping","text":"For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. $x = -1; sub f { local $x = 1; return g(); } sub g { print $x; } f() In the above, it is the same program coded in perl . Except that in perl, variables with local are defined using dynamic scoping. As a result, 1 is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; f --> g; g --> usex1[\"print(x)\"]; As illustrated by the dynamic call graph above, the variable x in print(x) refers to g 's caller, i.e. f , which is 1 .","title":"Dynamic Variable Scoping"},{"location":"name_analysis/#more-on-static-variable-scoping","text":"Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation. Consider the following Python program. def main(argv): x = 1 if len(argv) == 0: x = 2 else: y = 1 print(y) when the input argv is a non-empty list, the function main prints 1 as results. However when argv is an empty list, a run-time error arises. Consider the \"nearly-the-same\" program in Java. class Main { public static int main(String[] argv) { int x = 1; if (argv.length > 0){ x = 2; } else { int y = 1; } System.out.println(y.toString()); return 1; } } Java returns a compilation error, complaining variable y being use in the System.out.println function can't be resolved. The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable y 's scope is only within the else branch but not outside. In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. However how might we detect the run-time error similar to what we've observed from the last Python example? Let's recast the example in SIMP, let's call it SIMP_ERR1 // SIMP_ERR1 x = 1; if input == 0 { x = 2; } else { y = 1; } return y; The above program will cause an error when input == 0 . It is typeable based on the type inference algorithm we studied in the previous class. Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program. // PA_ERR1 1: x <- 1 2: t <- input == 0 3: ifn t goto 6 4: x <- 2 5: goto 7 6: y <- 1 7: rret <- y 8: ret Same error arises when input == 0 .","title":"More On Static Variable Scoping"},{"location":"name_analysis/#static-single-assignment-form","text":"Static Single Assignment (SSA) form is an intermediate representation widely used in compiler design and program verification. In a static single assignment form, Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. \\(\\phi\\) -assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). SSA form construction is one of the effective ways to analysis the scope of variables the use-def relationship of variables","title":"Static Single Assignment form"},{"location":"name_analysis/#unstructured-ssa-form","text":"Suppose we extend the pseudo assembly with \\(\\phi\\) -assignment statements, \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : \\overline{\\phi}\\ i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt PhiAssignment) & \\phi & ::= & d \\leftarrow phi(\\overline{l:s}) \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\mid ... \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\) . (which could be empty) before the actual instruction \\(i\\) . When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax. we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms. Suppose we have the following pseudo assembly program // PA1 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that variables s and c are re-assigned in the loop. The SSA form of the above is // SSA_PA1 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: rret <- s1 10: ret In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable. There are two possible preceding instructions that lead us to the following instruction 4: s1 <- phi(3:s0, 9:s2) c1 <- phi(3:c0, 9:c2) namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign s0 to s1 and c0 to c1 . Otherwise, s2 is assigned to s1 and c2 is assigned to c1 . To cater for the phi assignment, we extend the small step operational semantics from \\( \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) \\) to \\[P \\vdash (L, li, p) \\longrightarrow (L', li', p')\\] The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l: d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\\\ \\\\ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\\\ \\\\ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\\\ \\\\ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l) \\end{array} \\\\ {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l) \\end{array} \\\\ {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l) \\end{array} \\end{array} \\] All the existing rules are required some minor changes to accomodate the third component in the program context. The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments. In the presence of phi-assignments, we need the following rules to guide the execution. \\[ \\begin{array}{rc} {\\tt (pPhi1)} & \\begin{array}{c} (L, l: []\\ i, p) \\longrightarrow (L, l: i, p) \\end{array} \\\\ \\\\ {\\tt (pPhi2)} & \\begin{array}{c} l_i = p\\ \\ \\ c_i = L(s_i) \\\\ j \\in [1,i-1]: l_j \\neq p \\\\ \\hline (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p) \\end{array} \\end{array} \\] The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\) 's operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\) , i.e. \\(c_i\\) . Add the new entry \\((d,c_i)\\) to the local environment \\(L\\) . Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\) . Given \\(input = 1\\) , excuting SSA_PA1 yields the following derivation P |- {(input,1)}, 1: x0 <- input, undef ---> # (pTempVar) P |- {(input,1), (x0,1)}, 2: s0 <- 0, 1 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0)}, 3: c0 <- 0, 2 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 <- c1 < x0, 3 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 <- c1 < x0, 3 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 <- c1 + s1, 5 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 <- c1 + 1, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7 ---> # (pGoto) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 <- c1 < x0, 8 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 <- c1 < x0, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret <- s1, 5 ---> # (pTempVar) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9","title":"Unstructured SSA Form"},{"location":"name_analysis/#minimality","text":"One may argue that instead of generating SSA_PA1 , one might generate the following static single assignment // SSA_PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: s3 <- phi(5:s1) rret <- s3 10: ret which will yield the same output. However we argue that SSA_PA1 is preferred as it has the minimal number of phi assignments.","title":"Minimality"},{"location":"name_analysis/#ssa-construction-algorithm","text":"The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al. https://doi.org/10.1145/115372.115320 The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form.","title":"SSA Construction Algorithm"},{"location":"name_analysis/#control-flow-graph","text":"We can model a Pseudo Assembly program using a graph, namely the contorl flow graph. For example, PA1 can be represented as the following Control flow graph Graph1_PA1 graph TD; B1(\"1: x <- input 2: s <- 0 3: c <- 0\")-->B2; B2-->B3; B2(\"4: t <- c < x 5: ifn t goto 9\")-->B4(\"9: rret <- s 10: ret\"); B3(\"6: s <- c + s 7: c <- c + 1 8: goto 4\")-->B2; For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it Graph2_PA1 graph TD; V1(\"1: x <- input\") --> V2; V2(\"2: s <- 0\") --> V3; V3(\"3: c <- 0\") --> V4; V4(\"4: t <- c < x\") --> V5; V5(\"5: ifn t goto 9\") --> V9; V9(\"9: rret <- s\") --> V10(\"10: ret\") V5(\"5: ifn t goto 9\") --> V6; V6(\"6: s <- c + s\") --> V7; V7(\"7: c <- c + 1\") --> V8; V8(\"8: goto 4\") --> V4; Now we refer to the vertex in a control flow graph by the label. The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG.","title":"Control flow graph"},{"location":"name_analysis/#identifying-the-right-locations","text":"","title":"Identifying the \"right\" locations"},{"location":"name_analysis/#definition-1-graph","text":"Let \\(G\\) be a graph, \\(G = (V, E)\\) , where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\) , \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\) . Occassionally, we also refer to a vertex as a node in the graph. For convenience, we also write \\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and \\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\) .","title":"Definition 1 - Graph"},{"location":"name_analysis/#definition-2-path","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say a path from \\(v_1\\) to \\(v_2\\) , written as \\(path(v_1,v_2)\\) , exists iff \\(v_1 = v_2\\) or the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\) . For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\) .","title":"Definition 2 - Path"},{"location":"name_analysis/#definition-3-connectedness","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\) , iff \\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\) .","title":"Definition 3 - Connectedness"},{"location":"name_analysis/#definition-4-source-and-sink","text":"Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\) .","title":"Definition 4 - Source and Sink"},{"location":"name_analysis/#assumption","text":"Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\) , we have \\(connect(v_1,v_2)\\) Has only one source vertex, which means there is only one entry point to the program. Has only one sink vertex, which means there is only one return statement.","title":"Assumption"},{"location":"name_analysis/#definition-5-dominance-relation","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) dominates \\(v_2\\) , written as \\(v_1 \\preceq v_2\\) , iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\) . In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\) , we definitely pass through location \\(v_1\\) . For instance, in the earlier control flow graph for Graph2_PA1 , the vertex 1 dominates all vertices. the vertex 4 dominates itself, the vertices 5,6,7,8,9,10 .","title":"Definition 5 - Dominance Relation"},{"location":"name_analysis/#lemma-1-dominance-is-transitive","text":"\\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\) .","title":"Lemma 1 - Dominance is transitive"},{"location":"name_analysis/#lemma-2-domaince-is-reflexive","text":"For any vertex \\(v\\) , we have \\(v \\preceq v\\) .","title":"Lemma 2 - Domaince is reflexive"},{"location":"name_analysis/#defintion-6-strict-dominance","text":"We say \\(v_1\\) stricly domainates \\(v_2\\) , written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\) .","title":"Defintion 6 - Strict Dominance"},{"location":"name_analysis/#defintion-7-immediate-dominator","text":"We say \\(v_1\\) is the immediate dominator of \\(v_2\\) , written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\) . Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists.","title":"Defintion 7 - Immediate Dominator"},{"location":"name_analysis/#dominator-tree","text":"Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\) . Each vertex \\(v \\in G\\) forms a node in the dominator tree. For vertices \\(v_1, v_2 \\in G\\) , \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\) . For example, from the CFG Graph2_PA1 , we construct a dominator tree Tree2_PA1 , as follows, graph TD; 1 --> 2; 2 --> 3; 3 --> 4; 4 --> 5; 5 --> 6; 6 --> 7; 7 --> 8; 5 --> 9; 9 --> 10; Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\) .","title":"Dominator Tree"},{"location":"name_analysis/#definition-8-dominance-frontier","text":"Let \\(v\\) be vertex in a graph \\(G\\) , we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$ In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\) ). For instance, in our running example, the dominance frontier of vertex 6 is the set containing vertex 4 This is because vertex 8 is one of the predecesors of the vertex 4 and vertex 8 is dominated by vertex 6 , but not the vertex 4 is not domainated by vertex 6 . Question: what is the dominance frontier of vertex 5 ?","title":"Definition 8 - Dominance Frontier"},{"location":"name_analysis/#computing-dominance-frontier","text":"The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG.","title":"Computing Dominance Frontier"},{"location":"name_analysis/#re-definining-dominance-frontier","text":"The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\) . We define $$ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G) ~~~(E1) $$ where \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] and \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] \\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\) \\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\) ) that are not dominated by \\(v\\) . \\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\) , by finding vertices \\(w\\) in \\(v\\) 's dominance frontier, such that \\(w\\) is not dominated by \\(v\\) 's immediate dominator (i.e. \\(v\\) 's parent in the dominator tree). Cytron et al shows that \\((E1)\\) defines the same result as Definition 6.","title":"Re-definining Dominance Frontier"},{"location":"name_analysis/#dominance-frontier-algorithm","text":"As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex). The algorithm is structured as follows For each vertex \\(v\\) by traversing the dominator tree bottom up: compute \\(df_{local}(v,G)\\) compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\) , which can be looked up from the a memoization table. save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table. For instance, we make use of Graph2_PA1 and Tree2_PA1 to construct the following memoization table Table2_PA1 vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} From the above table, we conclude that variables that are updated in vertices 5,6,7,8 should be merged via phi-assignments at the entry point of vertex 4 . As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid (x,y)\\in G \\wedge idom(y) \\neq x \\}\\) Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\) Note that in Cytron's paper, they include two special vertices, entry the entry vertex, and exit as the exit, and entry dominates everything, and exit is only dominated by entry . The purpose is to handle langugage allowing multiple return statements.","title":"Dominance frontier algorithm"},{"location":"name_analysis/#definition-9-iterative-dominance-frontier","text":"As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\) , a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\) . However inserting a phi assignment at the dominance fronter of \\(v\\) introduces a new location of modifying the variable \\(x\\) . This leads to some \"cascading effect\" in computing the phi-assignment locations. We extend the dominance frontier to handle a set of vertices. Let \\(S\\) denote a set of vertices of a graph \\(G\\) . We define \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] We define the iterative dominance frontier recursively as follows \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\) , i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound. It follows that if a variable \\(x\\) is modified in locations \\(S\\) , then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\) .","title":"Definition 9 - Iterative Dominance Frontier"},{"location":"name_analysis/#ssa-construction-algorithm_1","text":"Given the control flow graph \\(G\\) , the dominator tree \\(T\\) , and the dominance frontier table \\(DFT\\) , the SSA construction algorithm consists of two steps. insert phi assignments to the original program \\(P\\) . rename variables to ensure the single assignment property.","title":"SSA construction algorithm"},{"location":"name_analysis/#inserting-phi-assignments","text":"Before inserting the phi assignments to \\(P\\) , we need some intermediate data structure. A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables. \\((l, S) \\in E\\) implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\) . \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation. Input: the original program P , can be viewed as a list of labeled instructions. Output: the modified program Q . can be viewed as a list of labeled instructions. The phi-assignment insertion process can be described as follows, Q = List() for each l:i in P match E.get(l) with case None add l:i to Q case Some(xs) phis = xs.map( x => x <- phi( k:x | (k in pred(l,G))) if phis has more than 1 operand, add l:phis i to Q \\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\) . For example, given PA1 , variable \\(x\\) is modified at 1 variable \\(s\\) is modified at 2,6 variable \\(c\\) is modified at 3,7 variable \\(t\\) is modified at 4 We construct \\(E\\) by consulting the dominance frontier table Table2_PA1 . E = Map( 4 -> Set(\"s\",\"c\", \"t\") ) which says that in node/vertex 4 , we should insert the phi-assignments for variable s and c . Now we apply the above algorithm to PA1 which generates // PRE_SSA_PA1 1: x <- input 2: s <- 0 3: c <- 0 4: s <- phi(3:s, 8:s) c <- phi(3:c, 8:c) t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that when we try to insert the phi assignment for t at 4 , we realize that there is only one operand. This is because t is not defined before label 4 . In this case we remove the phi assignment for t .","title":"Inserting Phi assignments"},{"location":"name_analysis/#renaming-variables","text":"Given an intermediate output like PRE_SSA_PA1 , we need to rename the variable so that there is only one assignment for each variable. Inputs: a dictionary of stacks K where the keys are the variable names in the original PA program. e.g. K(x) returns the stack for variable x . the input program in with phi assignment but oweing the variable renaming, e.g. PRE_SSA_PA1 . We view the program as a dictionary mapping labels to labeled instructions. For each variable x in the program, initialize K(x) = Stack() . Let label l be the root of the dominator tree \\(T\\) . Let vars be an empty list Match P(l) with case l: phis r <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) set Q(l) to l: phis' r <- s' case l: phis r <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) set Q(l) to l: phis' r <- s1' op s2' case l: phis x <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s' case l: phis x <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s1' op s2' case l: phis ifn t goto l' (phis', K, result_list) = processphi(phis, K, vars) t' = ren(K, t) set Q(l) to l: phis' ifn t' goto l' case l: phis ret (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' ret case l: phis goto l' (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' goto l' For each successor k of l in the CFG \\(G\\) R = if k in Q { Q } else { R } Pattern match R(k) case k: phis i for each x <- phi(j:x', m:x'') in phis if K(origin(x)) is empty, do not add this phi assignment in the result list**. if j == l , x <- phi(j:ren(K,x'), m:x'') into the result list if m == l , x <- phi(j:x', m:ren(K,x'')) into the result list the result list is phis' update R(k) to k: phis' i case others , no change Recursively apply step 3 to the children of l in the \\(T\\) . For each x in vars , K(x).pop() Where ren(K, s) is defined as ren(K,c) = c ren(K, input) = input ren(K, r) = r ren(K, t) = K(t).peek() match case None => error(\"variable use before being defined.\") case Some(i) => t_i and next(K, x) is defined as next(K, x) = K(x).peek() match case None => K(x).push(1) 0 case Some(i) => K(x).push(i+1) i and processphi(phis, K) is defined as prcessphi(phis, K, vars) = foreach x <- phi(j:x', k:x'') in phis i = K(x).peek() + 1 K(x).push(i) append x to vars put x_i <- phi(j:x', k:x'') into result_list return (result_list, K, vars) and stem(x) returns the original version of x before renaming, e.g. stem(x) = x and stem(x1) = x . We assume there exists some book-keeping mechanism to keep track of that the fact that x is the origin form of x_1 . Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch. We describe the application the algorithm to PRE_SSA_PA1 (with the dominator tree Tree2_PA1 and CFG Graph1_PA1 ) with the following table. label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 1:x<-input 1:x0<-input {x:[0], s:[], c:[], t:[]} {1:{x}} 2 2:s<-0 2:s0<-0 {x:[0], s:[0], c:[], t:[]} {1:{x}, 2:{s}} 3 3:c<-0 3:c0<-0 {x:[0], s:[0], c:[0], t:[]} 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x {1:{x}, 2:{s}. 3:{c}} 4 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x 4:s1<-phi(3:s0,8:s);c1<-phi(3:c0,8:c);t0<-c1<x0 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 5 5:ifn t goto 9 5:ifn t0 goto 9 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 6 6:s<-c+s 6:s2<-c1+s1 {x:[0], s:[0,1,2], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}} 7 7:c<-c+1 7:c2<-c1+1 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}} 8 8:goto 4 8:goto 4 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} 4:s1<-phi(3:s0,8:s2);c1<-phi(3:c0,8:c2);t0<-c1<x0 9 9:rret<-s 9:rret<-s1 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 10 10:ret 10:ret {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} The label column denotes the current label being considered. The P(l) column denotes the input labeled instruction being considered. The Q(l) column denotes the output labeled instruction. The K column denotes the set of stacks after the current recursive call. The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q) The Q(succ(l)) column denotes the modified successor instruction in Q. The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call). The above derivation eventually yield SSA_PA1 . Note that in case of a variable being use before initialized, ren(K, t) will raise an error.","title":"Renaming Variables"},{"location":"name_analysis/#ssa-back-to-pseudo-assembly","text":"To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating SSA_PA1 back to PA while keeping the renamed variables, we have // PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 4: t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: s1 <- s2 8: c1 <- c2 8: goto 4 9: rret <- s1 10: ret In the above we break the phi-assignments found in 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 into PhiFor3 s1 <- s0 // for label 3 c1 <- c0 and PhiFor8 s1 <- s2 // for label 8 c1 <- c2 We move PhiFor3 to label 3 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 and PhiFor8 to label 8 8: s1 <- s2 8: c1 <- c2 8: goto 4 The \"moving\" phi-assignment operation can be defined in the following algorithm.","title":"SSA back to Pseudo Assembly"},{"location":"name_analysis/#relocating-the-phi-assignments","text":"Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\) ) For each \\(l: \\overline{\\phi}\\ i \\in P\\) , append \\(l: i\\) to \\(Q\\) . For each \\(l: \\overline{\\phi}\\ i\\) . For each x = phi(l1:x1, l2:x2) in \\(\\overline{\\phi}\\) append l1:x <- x1 and l2:x <- x2 to \\(Q\\) . note that the relocated assignment must be placed before the control flow transition from l1 to succ(l1) (and l2 to succ(l2) ) Sort \\(Q\\) by labels using a stable sorting algorithm. Now since there are repeated labels in PA2 , we need an extra relabelling step to convert PA2 to PA3 // PA3 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- s0 5: c1 <- c0 6: t0 <- c1 < x0 7: ifn t0 goto 11 8: s2 <- c1 + s1 9: c2 <- c1 + 1 10: s1 <- s2 11: c1 <- c2 12: goto 4 13: rret <- s1 14: ret","title":"Relocating the phi-assignments"},{"location":"name_analysis/#relabelling","text":"This re-labeling step can be described in the following algorithm. Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\) ) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. Initialize a counter c = 1 , Initialize a mapping from old label to new label, M = Map() . Initialize \\(Q\\) as an empty list For each l: i \\(\\in P\\) M = M + (l -> c) incremeant c by 1 For each l: i \\(\\in P\\) append M(l): relabel(i, M) to \\(Q\\) where relabel(i, M) is defined as follows relabel(ifn t goto l,M) = ifn t goto M(l) relabel(goto l, M) = goto M(l) relabel(i, M) = i","title":"Relabelling"},{"location":"name_analysis/#structured-ssa","text":"Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Can be converted into a structured SSA x1 = input; s1 = 0; c1 = 0; join { s2 = phi(s1,s3); c2 = phi(c1,c3); } while c2 < x1 { s3 = c2 + s2; c3 = c2 + 1; } return s2; In the above SSA form, we have a join ... while ... loop. The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and also the body of the loop. (Similarly we can introduce a if ... else ... join ... statement). Structured SSA allows us to conduct name analysis closer to the source language. conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. perform code obfuscation.","title":"Structured SSA"},{"location":"name_analysis/#futher-readings","text":"https://dl.acm.org/doi/10.1145/2955811.2955813 https://dl.acm.org/doi/abs/10.1145/3605156.3606457 https://dl.acm.org/doi/10.1145/202530.202532","title":"Futher Readings"},{"location":"semantic_analysis/","text":"50.054 - Semantic Analysis Learning Outcomes Articulate the meaning of program semantics List different types of program semantics. Explain the limitation of static analysis. What is Program Semantic In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program. Dynamic Semantics Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean How does the program get executed? What does the program compute / return? Static Semantics Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value. Semantics Analysis Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] But in fact it could be graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Semantic Analysis Input: A parse tree or an internal representation a source parse tree is considered an internal representation Output: if succeeds, a parse tree or an internal representation otherwise, an error report Goal of Semantic Analysis There mainly two goals of semantic analysis. Optimization x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; Fault Detection x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number. return y; Dynamic Semantics Analysis Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules). Testing Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification. Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests. Static Semantic Analysis Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it. Type checking and type inference Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced. Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location. Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints. The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation Limitation of Static Semantic Analysis It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs. For example, assume we can find an algorithm that determine whether the variable x in the following function is positive or negative without executing it. def f(path): p = open(path, \"r\") x = 1 if eval(p): x = -1 return x In the above program the analysis of x 's sign (positive or negative) is subject to whether eval(p) is true or false . If such an algorithm exists, as a side effect we can also statically detect whether the given program in path is terminating, which is of course undecidable.","title":"50.054 - Semantic Analysis"},{"location":"semantic_analysis/#50054-semantic-analysis","text":"","title":"50.054 - Semantic Analysis"},{"location":"semantic_analysis/#learning-outcomes","text":"Articulate the meaning of program semantics List different types of program semantics. Explain the limitation of static analysis.","title":"Learning Outcomes"},{"location":"semantic_analysis/#what-is-program-semantic","text":"In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program.","title":"What is Program Semantic"},{"location":"semantic_analysis/#dynamic-semantics","text":"Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean How does the program get executed? What does the program compute / return?","title":"Dynamic Semantics"},{"location":"semantic_analysis/#static-semantics","text":"Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value.","title":"Static Semantics"},{"location":"semantic_analysis/#semantics-analysis","text":"Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] But in fact it could be graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Semantic Analysis Input: A parse tree or an internal representation a source parse tree is considered an internal representation Output: if succeeds, a parse tree or an internal representation otherwise, an error report","title":"Semantics Analysis"},{"location":"semantic_analysis/#goal-of-semantic-analysis","text":"There mainly two goals of semantic analysis.","title":"Goal of Semantic Analysis"},{"location":"semantic_analysis/#optimization","text":"x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s;","title":"Optimization"},{"location":"semantic_analysis/#fault-detection","text":"x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number. return y;","title":"Fault Detection"},{"location":"semantic_analysis/#dynamic-semantics-analysis","text":"Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules). Testing Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification. Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests.","title":"Dynamic Semantics Analysis"},{"location":"semantic_analysis/#static-semantic-analysis","text":"Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it. Type checking and type inference Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced. Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location. Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints. The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation","title":"Static Semantic Analysis"},{"location":"semantic_analysis/#limitation-of-static-semantic-analysis","text":"It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs. For example, assume we can find an algorithm that determine whether the variable x in the following function is positive or negative without executing it. def f(path): p = open(path, \"r\") x = 1 if eval(p): x = -1 return x In the above program the analysis of x 's sign (positive or negative) is subject to whether eval(p) is true or false . If such an algorithm exists, as a side effect we can also statically detect whether the given program in path is terminating, which is of course undecidable.","title":"Limitation of Static Semantic Analysis"},{"location":"sign_analysis_lattice/","text":"50.054 - Sign Analysis and Lattice Theory Learning Outcomes Explain the objective of Sign Analysis Define Lattice and Complete Lattice Define Monotonic Functions Explain the fixed point theorem Apply the fixed pointed theorem to solve equation constraints of sign analysis Recap Recall that one of the goals of semantic analyses is to detect faults without executing the program. // SIMP1 x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number return y; Note that our current SIMP syntax does not support >= . We could extend both SIMP and Pseudo Assembly to support a new binary operator || so that we can x>=0 into (x > 0) || (x == 0) Note that for In Pseudo Assembly we use 0 to encode false and 1 to encode true . Hence || can be encoded as + . To detect that the application of sqrt(x) is causing an error, we could apply the sign analysis. Sign Analysis Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example // SIMP1 x = input; // x could be +, - or 0 while (x >= 0) { // x could be +, - or 0 x = x - 1; // x could be +, - or 0 } // x must be - y = Math.sqrt(x); // x must be -, y could be +, - or 0 return y; // x must be -, y could be +, - or 0 We put the comments as the results of the analysis. Can we turn Sign Analysis into a type inference problem? The answer is yes, but it is rather imprecise. Let's consider a simple example. // SIMP2 x = 0; x = x + 1; return x; Suppose we introduce 3 subtypes of the Int type, namely Zero , PosInt and NegInt The first statement, we infer x has type Zero . The second statement, we infer x on the RHS, has type Int , the LHS x has type Int . Unification would fail when we try to combine the result of (x : Zero) and (x : Int) . It is also unsound to conclude that Zero is the final type. This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom. Abstract Domain To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. For example, we may use \\(\\{\\}\\) to denote the empty set \\(+\\) to denote the set of all positive integers \\(-\\) to denote the set of all ngative integers \\(\\{0\\}\\) to denote the set containing 0 \\(+ \\cup - \\cup \\{0\\}\\) to denote all integers . For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\) , \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\) . These symbols are the abstract values of the sign property. Since they are sets of values, we can define the subset relation among them. \\[ \\begin{array}{c} \\bot \\subseteq 0 \\\\ \\bot \\subseteq + \\\\ \\bot \\subseteq - \\\\ 0 \\subseteq \\top \\\\ {+} \\subseteq \\top \\\\ {-} \\subseteq \\top \\end{array} \\] If we put each abstract domain values in a graph we have the following graph Graph1 graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] informally the above graph structure is called a lattice in math. We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of SIMP2 . For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.) // PA2 // x -> top 1: x <- 0 // x -> 0 2: x <- x + 1 // x -> 0 ++ + -> + 3: rret <- x // x -> + 4: ret we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to x , as x could be any value. After instruction 1, we deduce that x must be having the abstract value 0 , since we assign 0 to x . After instruction 2, we deduce that x has the abstract value + because we add ( ++ ) 1 to an abstract value 0 . (Note that the 0 , 1 and ++ in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable input (which is always \\(\\top\\) ) and the register rret (whose sign is not useful.) Let's consider another example // PA3 // x -> top, t -> top 1: x <- 0 // x -> 0, t -> top 2: t <- input < 0 // x -> 0, t -> top 3: ifn t goto 6 // x -> 0, t -> top 4: x <- x + 1 // x -> +, t -> top 5: goto 6 // x -> +, t -> top 6: rret <- x // x -> upperbound(+, 0) -> top, t -> top 7: ret We start off by assigning \\(\\top\\) to x , then 0 to x at the instruction 1. At instruction 2, we assign the result of the boolean condition to t which could be 0 or 1 hence top is the abstract value associated with t . Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update x 's sign to + . Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of x 's sign. If t 's value is 0, x 's sign is 0 , otherwise x 's sign is + . Hence we take the upperbound of + , 0 according to Graph1 which is \\(\\top\\) . Let's consider the formalism of the lattice and this approach we just presented. Lattice Theory Definition 1 - Partial Order A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition. reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\) transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\) . anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\) . For instance, the set of abstract values in Graph1 forms a partial order if we define \\(x \\sqsubseteq y\\) as \" \\(x\\) is at least as precise than \\(y\\) \", (i.e. \\(x\\) is the same or more precise than \\(y\\) ). Definition 2 - Upper Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\) ) iff \\(\\forall x \\in T, x \\sqsubseteq y\\) . Definition 3 - Least Upper Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\) ) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\) . For example, in Graph1 , 0 is an upper bound of \\(\\{\\bot\\}\\) , but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\) . Definition 4 - Lower Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\) ) iff \\(\\forall x \\in T, y \\sqsubseteq x\\) . Definition 5 - Greatest Lower Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\) ) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\) . For example, in Graph2 , 0 is a lower bound of \\(\\{\\top\\}\\) , but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\) . Definition 6 - Join and Meet Let \\(S\\) be a partial order, and \\(x, y \\in S\\) . We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\) . We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\) . Definition 7 - Lattice A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\) , \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist. Definition 8 - Complete Lattice and Semi-Lattice A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist. A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) exists. A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\) , \\({\\Large \\sqcap} X\\) exists. For example the set of abstract values in Graph1 and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. Graph1 is the Hasse diagram of this complete lattice. Lemma 9 Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice. In the next few subsections, we introduce a few commonly use lattices. Powerset Lattice Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\) . Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice. We call it powerset lattice . The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\) , we find that for any \\(T \\subseteq {\\cal P}(A)\\) . \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\) . Can you show that the power set of {1,2,3,4} and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram? Product Lattice Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times L_n)\\) . For example in PA3 , to analyse the signs for variables we need two lattices, one for variable x and the other for variable t , which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . graph TD; tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"] tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"] tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"] tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"] t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"] t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"] t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"] t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"] t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"] t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"] t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"] t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"] tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"] tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"] tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"] tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"] +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"] +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"] +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"] 0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] 0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"] 0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"] 0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"] mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"] ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"] 0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"] 0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"] m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"] m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"] 00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"] 00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"] +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"] +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"] m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"] m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"] +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"] +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"] 0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"] 0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"] mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"] bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"] bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"] bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"] b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"] b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"] bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"] +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] 0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] Map Lattice Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] Then \\(A \\rightarrow L\\) is a complete lattice. Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Scala Map[A,L] object where elements of \\(A\\) are keys and elements of \\(L\\) are the values associated with the keys. Map lattice offers a compact alternative to lattices for sign analysis of variables in program like PA3 when there are many variables. We can define a map lattice consisting of functions that map variables ( x or t ) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . For instance, one of the element \"functions\" in the above-mentioned map lattice could be \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ] \\] another element function could be \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ] \\] We conclude that \\(m_1\\sqsubseteq m_2\\) . Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . m1 and m2 are elements of the complete lattice \\(Var \\rightarrow Sign\\) graph TD; tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"] +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] 0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] 00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] 00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] 0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"] b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"] +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] 0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] Sign analysis with Lattice As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements. // PA2 // s0 = [x -> top] 1: x <- 0 // s1 = s0[x -> 0] 2: x = x + 1 // s2 = s1[x -> s1(x) ++ +] 3: rret <- x // s3 = s2 4: ret In the above, we analyse SIMP2 program's sign by \"packaging\" the variable to sign bindings into some state variables, s1 , s2 , s3 and s4 . Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\) . Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. Note that we could also model the state variables as a tuple of lattice as a produce lattice. Next we would like to model the change of variable signs based on the previous instructions. We write s[x -> v] to denote a new state s' which is nearly the same as s except that the mapping of variable x is changed to v. (In Scala style syntax, assuming s is a Map[Var, Sign] object, then s[x->v] is actually s + (x -> v) in Scala.) We write s(x) to denote a query of variable x 's value in state s . (In Scala style syntax, it is s.get(x) match { case Some(v) => v } ) In the above example, we define s2 based on s1 by \"updating\" variable x 's sign to 0 . We update x 's sign in s3 based on s2 by querying x 's sign in s2 and modifying it by increasing by 1 . We define the ++ abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Where the first column from the 2nd rows onwards are the left operand and the first row from the 2nd column onwards are the right operand. Similarly we can define the other abstract operators -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) << \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Given the definitions of the abstract operators, our next task is to solve the equation among the state variable s0 , s1 , s2 and s3 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 Note that we can't use unification here as x is assocated with different sign abstract values at different states (instructions). Question: If we use SSA PA instead of PA, can the generated equations be solved using unification? To solve the set of equation constraints we could process the equations from top to bottom. s0 = [x -> top] s1 = [x -> 0] s2 = [x -> +] s3 = [x -> +] Then we can conclude that the sign of variable x at instruction 3 is positive. Note that all the states, s0 , s1 , s2 and s3 are elements in the map lattice \\(Var \\rightarrow Sign\\) . However, we need a more general solver as the equation systems could be recursive in the presence of loops. For example. // PA4 // s0 = [x -> top, y -> top, t -> top] 1: x <- input // s1 = s0 2: y <- 0 // s2 = s1[y -> 0] 3: t <- x > 0 // s3 = upperbound(s2,s7)[t -> top] 4: ifn t goto 8 // s4 = s3 5: y <- y + 1 // s5 = s4[y -> s4(y) ++ +] 6: x <- x - 1 // s6 = s5[x -> s5(x) -- +] 7: goto 3 // s7 = s6 8: rret <- y // s8 = s4 9: ret In the above the upperbound(s, t) can be define as \\(s \\sqcup t\\) , assuming \\(s\\) and \\(t\\) are elements of a complete lattice. Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\) , hence \\(s \\sqcup t\\) can be defined as \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] To solve equation systems like the above, we need some \"special\" functions that operates on lattices. Definition 10 - Monotonic Function Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\) . Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl. For instance given the lattice described in Graph1 , we define the following function \\[ \\begin{array}{rcl} f_1(x) & = & \\top \\end{array} \\] Function \\(f_1\\) is monotonic because \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\ f_1(+) = \\top \\\\ f_1(-) = \\top \\\\ f_1(\\top) = \\top \\end{array} \\] and \\(\\top \\sqsubseteq \\top\\) Let's consider another function \\(f_2\\) \\[ \\begin{array}{rcl} f_2(x) & = & x \\sqcup + \\end{array} \\] is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\ f_2(+) = + \\sqcup + = + \\\\ f_2(-) = - \\sqcup + = \\top \\\\ f_2(\\top) = \\top \\sqcup + = \\top \\end{array} \\] Note that \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} $$ when we apply $g$ to all the abstract values in the above inequalities, we find that $$ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] hold. Therefore \\(g\\) is monotonic. Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f (v_1', ..., v_n')\\) Lemma 11 - Constant Function is Monotonic. Every constant function \\(f\\) is monotonic. Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic. Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\) , then \\(\\sqcup\\) is monotonic. Similar observation applies to \\(\\sqcap\\) . Definition 13 - Fixed Point and Least Fixed Point Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\) . We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\) , \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\) . For example, for function \\(f_1\\) , \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\) , \\(+\\) , \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point. Theorem 14 - Fixed Point Theorem Let \\(L\\) be a complete lattice with finite height, every monotonic function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\) , defined as \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] Where \\(f^n(x)\\) is a short hand for \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] The height of a complete lattice is the length of the longest path from \\(\\top\\) to \\(\\bot\\) . The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\) , we will reach a fixed point and it must be the only least fixed point. The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result. For example, consider function \\(f_2\\) . If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point. Lemma 15 - Map update with monotonic function is Monotonic Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\) . Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\) . To gain some intuition of this lemma, let's try to think in terms of Scala. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as Map[A, L2] in Scala style, and L2 is a lattice. f : L1 => Map[A, L2] is a Scala function that's monotonic, g: L1=>L2 is another Scala function which is monotonic. Then we can conclude that val a:A = ... // a is an element of A, where A is a ground type. def h[L1,L2](x:L1):Map[A,L2] = f(x) + (a -> g(x)) h is also monotonic. Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\) , we have \\(f(x) \\sqsubseteq f(y)\\) . It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\) . Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\) . With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis. Naive Fixed Point Algorithm input: a function f . initialize x as \\(\\bot\\) apply f(x) as x1 check x1 == x if true return x else, update x = x1 , go back to step 2. For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in Graph1 , we have the following iterations. \\(x = \\bot, x_1 = f_2(x) = +\\) \\(x = +, x_1 = f_2(x) = +\\) fixed point is reached, return \\(x\\) . Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA2 Recall the set of equations generated from PA2 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\) . and \\(Sign\\) to denote the sign lattice described in Graph1 . We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\) . In total. we have four map lattices, one for s0 , one for s1 , and etc. Then we \"package\" these four map lattices into a product lattice \\(L = (Var \\rightarrow Sign)^4\\) . Since \\(Sign\\) is a complete lattice, so is \\(L\\) . Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems. The type of \\(f_3\\) should be \\(L \\rightarrow L\\) , or \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] in its unabridge form. Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\) , but it is like a Map[Var, Sign] . Next we re-model the relations among s0,s1,s2,s3 in above equation system in \\(f_3\\) as follows \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] Thanks to Lemma 15, \\(f_3\\) is monotonic. The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point. \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) . Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA4 Recall the set of equations generated from PA4 's sign analysis s0 = [x -> top, y -> top, t -> top] s1 = s0 s2 = s1[y -> 0] s3 = upperbound(s2,s7)[t -> top] s4 = s3 s5 = s4[y -> s4(y) ++ +] s6 = s5[x -> s5(x) -- +] s7 = s6 s8 = s4 We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_0, \\\\ s_1[y \\mapsto 0], \\\\ (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\ s_3, \\\\ s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\ s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\ s_6, \\\\ s_4 \\end{array} \\right ) \\end{array} \\] \\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} $$ \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] 8 . $$ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) functipn w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice. Optimization This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\) . A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed. Generalizing the monotone constraints for sign analysis We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis. Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\) . let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\) . For sign analysis, we define the following helper function \\[join(s) = \\bigsqcup pred(s)\\] To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\) . Let \\(V\\) denotes the set of variables in the PA program's being analysed. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & \\left \\{ \\begin{array}{cc} 0 & c == 0 \\\\ + & c > 0 \\\\ - & c < 0 \\end{array} \\right . \\\\ \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, \\[ \\begin{array}{rcl} abs(+) & = & ++\\\\ abs(-) & = & -- \\\\ abs(*) & = & ** \\\\ abs(<) & = & << \\\\ abs(==) & = & === \\end{array} \\] We have seen the definitions of \\(++, --, **\\) and \\(<<\\) Question: can you define \\(===\\) ? Question: the abstraction operations are pretty coarse (not accurate). For instance, << and === should return either 0 or 1 hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? Question: Convert SIMP1 into a PA. Can we apply the sign analysis to find out that the sqrt(x) is definifely failing?","title":"50.054 - Sign Analysis and Lattice Theory"},{"location":"sign_analysis_lattice/#50054-sign-analysis-and-lattice-theory","text":"","title":"50.054 - Sign Analysis and Lattice Theory"},{"location":"sign_analysis_lattice/#learning-outcomes","text":"Explain the objective of Sign Analysis Define Lattice and Complete Lattice Define Monotonic Functions Explain the fixed point theorem Apply the fixed pointed theorem to solve equation constraints of sign analysis","title":"Learning Outcomes"},{"location":"sign_analysis_lattice/#recap","text":"Recall that one of the goals of semantic analyses is to detect faults without executing the program. // SIMP1 x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number return y; Note that our current SIMP syntax does not support >= . We could extend both SIMP and Pseudo Assembly to support a new binary operator || so that we can x>=0 into (x > 0) || (x == 0) Note that for In Pseudo Assembly we use 0 to encode false and 1 to encode true . Hence || can be encoded as + . To detect that the application of sqrt(x) is causing an error, we could apply the sign analysis.","title":"Recap"},{"location":"sign_analysis_lattice/#sign-analysis","text":"Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example // SIMP1 x = input; // x could be +, - or 0 while (x >= 0) { // x could be +, - or 0 x = x - 1; // x could be +, - or 0 } // x must be - y = Math.sqrt(x); // x must be -, y could be +, - or 0 return y; // x must be -, y could be +, - or 0 We put the comments as the results of the analysis.","title":"Sign Analysis"},{"location":"sign_analysis_lattice/#can-we-turn-sign-analysis-into-a-type-inference-problem","text":"The answer is yes, but it is rather imprecise. Let's consider a simple example. // SIMP2 x = 0; x = x + 1; return x; Suppose we introduce 3 subtypes of the Int type, namely Zero , PosInt and NegInt The first statement, we infer x has type Zero . The second statement, we infer x on the RHS, has type Int , the LHS x has type Int . Unification would fail when we try to combine the result of (x : Zero) and (x : Int) . It is also unsound to conclude that Zero is the final type. This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom.","title":"Can we turn Sign Analysis into a type inference problem?"},{"location":"sign_analysis_lattice/#abstract-domain","text":"To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. For example, we may use \\(\\{\\}\\) to denote the empty set \\(+\\) to denote the set of all positive integers \\(-\\) to denote the set of all ngative integers \\(\\{0\\}\\) to denote the set containing 0 \\(+ \\cup - \\cup \\{0\\}\\) to denote all integers . For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\) , \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\) . These symbols are the abstract values of the sign property. Since they are sets of values, we can define the subset relation among them. \\[ \\begin{array}{c} \\bot \\subseteq 0 \\\\ \\bot \\subseteq + \\\\ \\bot \\subseteq - \\\\ 0 \\subseteq \\top \\\\ {+} \\subseteq \\top \\\\ {-} \\subseteq \\top \\end{array} \\] If we put each abstract domain values in a graph we have the following graph Graph1 graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] informally the above graph structure is called a lattice in math. We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of SIMP2 . For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.) // PA2 // x -> top 1: x <- 0 // x -> 0 2: x <- x + 1 // x -> 0 ++ + -> + 3: rret <- x // x -> + 4: ret we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to x , as x could be any value. After instruction 1, we deduce that x must be having the abstract value 0 , since we assign 0 to x . After instruction 2, we deduce that x has the abstract value + because we add ( ++ ) 1 to an abstract value 0 . (Note that the 0 , 1 and ++ in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable input (which is always \\(\\top\\) ) and the register rret (whose sign is not useful.) Let's consider another example // PA3 // x -> top, t -> top 1: x <- 0 // x -> 0, t -> top 2: t <- input < 0 // x -> 0, t -> top 3: ifn t goto 6 // x -> 0, t -> top 4: x <- x + 1 // x -> +, t -> top 5: goto 6 // x -> +, t -> top 6: rret <- x // x -> upperbound(+, 0) -> top, t -> top 7: ret We start off by assigning \\(\\top\\) to x , then 0 to x at the instruction 1. At instruction 2, we assign the result of the boolean condition to t which could be 0 or 1 hence top is the abstract value associated with t . Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update x 's sign to + . Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of x 's sign. If t 's value is 0, x 's sign is 0 , otherwise x 's sign is + . Hence we take the upperbound of + , 0 according to Graph1 which is \\(\\top\\) . Let's consider the formalism of the lattice and this approach we just presented.","title":"Abstract Domain"},{"location":"sign_analysis_lattice/#lattice-theory","text":"","title":"Lattice Theory"},{"location":"sign_analysis_lattice/#definition-1-partial-order","text":"A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition. reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\) transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\) . anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\) . For instance, the set of abstract values in Graph1 forms a partial order if we define \\(x \\sqsubseteq y\\) as \" \\(x\\) is at least as precise than \\(y\\) \", (i.e. \\(x\\) is the same or more precise than \\(y\\) ).","title":"Definition 1 - Partial Order"},{"location":"sign_analysis_lattice/#definition-2-upper-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\) ) iff \\(\\forall x \\in T, x \\sqsubseteq y\\) .","title":"Definition 2 - Upper Bound"},{"location":"sign_analysis_lattice/#definition-3-least-upper-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\) ) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\) . For example, in Graph1 , 0 is an upper bound of \\(\\{\\bot\\}\\) , but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\) .","title":"Definition 3 - Least Upper Bound"},{"location":"sign_analysis_lattice/#definition-4-lower-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\) ) iff \\(\\forall x \\in T, y \\sqsubseteq x\\) .","title":"Definition 4 - Lower Bound"},{"location":"sign_analysis_lattice/#definition-5-greatest-lower-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\) ) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\) . For example, in Graph2 , 0 is a lower bound of \\(\\{\\top\\}\\) , but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\) .","title":"Definition 5 - Greatest Lower Bound"},{"location":"sign_analysis_lattice/#definition-6-join-and-meet","text":"Let \\(S\\) be a partial order, and \\(x, y \\in S\\) . We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\) . We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\) .","title":"Definition 6 - Join and Meet"},{"location":"sign_analysis_lattice/#definition-7-lattice","text":"A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\) , \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist.","title":"Definition 7 - Lattice"},{"location":"sign_analysis_lattice/#definition-8-complete-lattice-and-semi-lattice","text":"A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist. A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) exists. A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\) , \\({\\Large \\sqcap} X\\) exists. For example the set of abstract values in Graph1 and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. Graph1 is the Hasse diagram of this complete lattice.","title":"Definition 8 - Complete Lattice and Semi-Lattice"},{"location":"sign_analysis_lattice/#lemma-9","text":"Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice. In the next few subsections, we introduce a few commonly use lattices.","title":"Lemma 9"},{"location":"sign_analysis_lattice/#powerset-lattice","text":"Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\) . Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice. We call it powerset lattice . The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\) , we find that for any \\(T \\subseteq {\\cal P}(A)\\) . \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\) . Can you show that the power set of {1,2,3,4} and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram?","title":"Powerset Lattice"},{"location":"sign_analysis_lattice/#product-lattice","text":"Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times L_n)\\) . For example in PA3 , to analyse the signs for variables we need two lattices, one for variable x and the other for variable t , which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . graph TD; tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"] tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"] tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"] tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"] t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"] t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"] t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"] t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"] t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"] t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"] t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"] t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"] tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"] tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"] tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"] tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"] +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"] +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"] +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"] 0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] 0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"] 0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"] 0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"] mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"] ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"] 0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"] 0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"] m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"] m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"] 00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"] 00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"] +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"] +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"] m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"] m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"] +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"] +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"] 0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"] 0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"] mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"] bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"] bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"] bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"] b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"] b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"] bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"] +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] 0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]","title":"Product Lattice"},{"location":"sign_analysis_lattice/#map-lattice","text":"Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] Then \\(A \\rightarrow L\\) is a complete lattice. Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Scala Map[A,L] object where elements of \\(A\\) are keys and elements of \\(L\\) are the values associated with the keys. Map lattice offers a compact alternative to lattices for sign analysis of variables in program like PA3 when there are many variables. We can define a map lattice consisting of functions that map variables ( x or t ) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . For instance, one of the element \"functions\" in the above-mentioned map lattice could be \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ] \\] another element function could be \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ] \\] We conclude that \\(m_1\\sqsubseteq m_2\\) . Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . m1 and m2 are elements of the complete lattice \\(Var \\rightarrow Sign\\) graph TD; tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"] +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] 0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] 00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] 00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] 0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"] b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"] +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] 0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]","title":"Map Lattice"},{"location":"sign_analysis_lattice/#sign-analysis-with-lattice","text":"As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements. // PA2 // s0 = [x -> top] 1: x <- 0 // s1 = s0[x -> 0] 2: x = x + 1 // s2 = s1[x -> s1(x) ++ +] 3: rret <- x // s3 = s2 4: ret In the above, we analyse SIMP2 program's sign by \"packaging\" the variable to sign bindings into some state variables, s1 , s2 , s3 and s4 . Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\) . Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. Note that we could also model the state variables as a tuple of lattice as a produce lattice. Next we would like to model the change of variable signs based on the previous instructions. We write s[x -> v] to denote a new state s' which is nearly the same as s except that the mapping of variable x is changed to v. (In Scala style syntax, assuming s is a Map[Var, Sign] object, then s[x->v] is actually s + (x -> v) in Scala.) We write s(x) to denote a query of variable x 's value in state s . (In Scala style syntax, it is s.get(x) match { case Some(v) => v } ) In the above example, we define s2 based on s1 by \"updating\" variable x 's sign to 0 . We update x 's sign in s3 based on s2 by querying x 's sign in s2 and modifying it by increasing by 1 . We define the ++ abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Where the first column from the 2nd rows onwards are the left operand and the first row from the 2nd column onwards are the right operand. Similarly we can define the other abstract operators -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) << \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Given the definitions of the abstract operators, our next task is to solve the equation among the state variable s0 , s1 , s2 and s3 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 Note that we can't use unification here as x is assocated with different sign abstract values at different states (instructions). Question: If we use SSA PA instead of PA, can the generated equations be solved using unification? To solve the set of equation constraints we could process the equations from top to bottom. s0 = [x -> top] s1 = [x -> 0] s2 = [x -> +] s3 = [x -> +] Then we can conclude that the sign of variable x at instruction 3 is positive. Note that all the states, s0 , s1 , s2 and s3 are elements in the map lattice \\(Var \\rightarrow Sign\\) . However, we need a more general solver as the equation systems could be recursive in the presence of loops. For example. // PA4 // s0 = [x -> top, y -> top, t -> top] 1: x <- input // s1 = s0 2: y <- 0 // s2 = s1[y -> 0] 3: t <- x > 0 // s3 = upperbound(s2,s7)[t -> top] 4: ifn t goto 8 // s4 = s3 5: y <- y + 1 // s5 = s4[y -> s4(y) ++ +] 6: x <- x - 1 // s6 = s5[x -> s5(x) -- +] 7: goto 3 // s7 = s6 8: rret <- y // s8 = s4 9: ret In the above the upperbound(s, t) can be define as \\(s \\sqcup t\\) , assuming \\(s\\) and \\(t\\) are elements of a complete lattice. Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\) , hence \\(s \\sqcup t\\) can be defined as \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] To solve equation systems like the above, we need some \"special\" functions that operates on lattices.","title":"Sign analysis with Lattice"},{"location":"sign_analysis_lattice/#definition-10-monotonic-function","text":"Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\) . Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl. For instance given the lattice described in Graph1 , we define the following function \\[ \\begin{array}{rcl} f_1(x) & = & \\top \\end{array} \\] Function \\(f_1\\) is monotonic because \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\ f_1(+) = \\top \\\\ f_1(-) = \\top \\\\ f_1(\\top) = \\top \\end{array} \\] and \\(\\top \\sqsubseteq \\top\\) Let's consider another function \\(f_2\\) \\[ \\begin{array}{rcl} f_2(x) & = & x \\sqcup + \\end{array} \\] is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\ f_2(+) = + \\sqcup + = + \\\\ f_2(-) = - \\sqcup + = \\top \\\\ f_2(\\top) = \\top \\sqcup + = \\top \\end{array} \\] Note that \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} $$ when we apply $g$ to all the abstract values in the above inequalities, we find that $$ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] hold. Therefore \\(g\\) is monotonic. Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f (v_1', ..., v_n')\\)","title":"Definition 10 - Monotonic Function"},{"location":"sign_analysis_lattice/#lemma-11-constant-function-is-monotonic","text":"Every constant function \\(f\\) is monotonic.","title":"Lemma 11 - Constant Function is Monotonic."},{"location":"sign_analysis_lattice/#lemma-12-sqcup-and-sqcap-are-monotonic","text":"Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\) , then \\(\\sqcup\\) is monotonic. Similar observation applies to \\(\\sqcap\\) .","title":"Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic."},{"location":"sign_analysis_lattice/#definition-13-fixed-point-and-least-fixed-point","text":"Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\) . We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\) , \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\) . For example, for function \\(f_1\\) , \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\) , \\(+\\) , \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point.","title":"Definition 13 - Fixed Point and Least Fixed Point"},{"location":"sign_analysis_lattice/#theorem-14-fixed-point-theorem","text":"Let \\(L\\) be a complete lattice with finite height, every monotonic function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\) , defined as \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] Where \\(f^n(x)\\) is a short hand for \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] The height of a complete lattice is the length of the longest path from \\(\\top\\) to \\(\\bot\\) . The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\) , we will reach a fixed point and it must be the only least fixed point. The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result. For example, consider function \\(f_2\\) . If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point.","title":"Theorem 14 - Fixed Point Theorem"},{"location":"sign_analysis_lattice/#lemma-15-map-update-with-monotonic-function-is-monotonic","text":"Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\) . Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\) . To gain some intuition of this lemma, let's try to think in terms of Scala. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as Map[A, L2] in Scala style, and L2 is a lattice. f : L1 => Map[A, L2] is a Scala function that's monotonic, g: L1=>L2 is another Scala function which is monotonic. Then we can conclude that val a:A = ... // a is an element of A, where A is a ground type. def h[L1,L2](x:L1):Map[A,L2] = f(x) + (a -> g(x)) h is also monotonic. Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\) , we have \\(f(x) \\sqsubseteq f(y)\\) . It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\) . Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\) . With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis.","title":"Lemma 15 - Map update with monotonic function is Monotonic"},{"location":"sign_analysis_lattice/#naive-fixed-point-algorithm","text":"input: a function f . initialize x as \\(\\bot\\) apply f(x) as x1 check x1 == x if true return x else, update x = x1 , go back to step 2. For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in Graph1 , we have the following iterations. \\(x = \\bot, x_1 = f_2(x) = +\\) \\(x = +, x_1 = f_2(x) = +\\) fixed point is reached, return \\(x\\) .","title":"Naive Fixed Point Algorithm"},{"location":"sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa2","text":"Recall the set of equations generated from PA2 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\) . and \\(Sign\\) to denote the sign lattice described in Graph1 . We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\) . In total. we have four map lattices, one for s0 , one for s1 , and etc. Then we \"package\" these four map lattices into a product lattice \\(L = (Var \\rightarrow Sign)^4\\) . Since \\(Sign\\) is a complete lattice, so is \\(L\\) . Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems. The type of \\(f_3\\) should be \\(L \\rightarrow L\\) , or \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] in its unabridge form. Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\) , but it is like a Map[Var, Sign] . Next we re-model the relations among s0,s1,s2,s3 in above equation system in \\(f_3\\) as follows \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] Thanks to Lemma 15, \\(f_3\\) is monotonic. The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point. \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) .","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA2"},{"location":"sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa4","text":"Recall the set of equations generated from PA4 's sign analysis s0 = [x -> top, y -> top, t -> top] s1 = s0 s2 = s1[y -> 0] s3 = upperbound(s2,s7)[t -> top] s4 = s3 s5 = s4[y -> s4(y) ++ +] s6 = s5[x -> s5(x) -- +] s7 = s6 s8 = s4 We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_0, \\\\ s_1[y \\mapsto 0], \\\\ (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\ s_3, \\\\ s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\ s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\ s_6, \\\\ s_4 \\end{array} \\right ) \\end{array} \\] \\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} $$ \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] 8 . $$ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) functipn w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice.","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA4"},{"location":"sign_analysis_lattice/#optimization","text":"This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\) . A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed.","title":"Optimization"},{"location":"sign_analysis_lattice/#generalizing-the-monotone-constraints-for-sign-analysis","text":"We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis. Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\) . let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\) . For sign analysis, we define the following helper function \\[join(s) = \\bigsqcup pred(s)\\] To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\) . Let \\(V\\) denotes the set of variables in the PA program's being analysed. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & \\left \\{ \\begin{array}{cc} 0 & c == 0 \\\\ + & c > 0 \\\\ - & c < 0 \\end{array} \\right . \\\\ \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, \\[ \\begin{array}{rcl} abs(+) & = & ++\\\\ abs(-) & = & -- \\\\ abs(*) & = & ** \\\\ abs(<) & = & << \\\\ abs(==) & = & === \\end{array} \\] We have seen the definitions of \\(++, --, **\\) and \\(<<\\) Question: can you define \\(===\\) ? Question: the abstraction operations are pretty coarse (not accurate). For instance, << and === should return either 0 or 1 hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? Question: Convert SIMP1 into a PA. Can we apply the sign analysis to find out that the sqrt(x) is definifely failing?","title":"Generalizing the monotone constraints for sign analysis"},{"location":"static_semantics/","text":"50.054 Static Semantics For SIMP Learning Outcomes Explain what static semantics is. Apply type checking rules to verify the type correctness property of a SIMP program. Explain the relation between type system and operational semantics. Apply type inference algorithm to generate a type environment given a SIMP program. What is static semantics? While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program. For example, a statically correct program, must satisfy some properties all uses of variables in it must be defined somewhere earlier. all the use of variables, the types must be matching with the expected type in the context. ... Here is a statically correct SIMP program, x = 0; y = input; if y > x { y = 0; } return y; because it satifies the first two properties. The following program is not statically correct. x = 0; y = input; if y + x { // type error x = z; // the use of an undefined variable z } return x; Static checking is to rule out the statically incorrect programs. Type Checking for SIMP We consider the type checking for SIMP programs. Recall the syntax rules for SIMP \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (X \\times T) \\end{array} \\] We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . We define two different relations, \\(\\Gamma \\vdash E : T\\) , which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\) . \\(\\Gamma \\vdash \\overline{S}\\) , which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\) . Type checking rules for SIMP Expressions $$ \\begin{array}{rc} {\\tt (tVar)} & \\begin{array}{c} (X,T) \\in \\Gamma \\ \\hline \\Gamma \\vdash X : T \\end{array} \\end{array} $$ In the rule \\({\\tt (tVar)}\\) , we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\) . $$ \\begin{array}{rc} {\\tt (tInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash C : int \\end{array} \\ \\ {\\tt (tBool)} & \\begin{array}{c} C \\in {true,false} \\ \\hline \\Gamma \\vdash C : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tInt)}\\) , we type check an integer constant having type \\(int\\) . Similarly, we type check a boolean constant having type \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tOp1)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { +, -, * } \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : int \\end{array} \\ \\ {\\tt (tOp2)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP E_2 : bool \\end{array} \\ \\ {\\tt (tOp3)} & \\begin{array}{c} \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tOp1)}\\) , we type check an integer arithmetic operation having type \\(int\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp2)}\\) , we type check an integer comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp3)}\\) , we type check a boolean comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tParen)} & \\begin{array}{c} \\Gamma \\vdash E :T \\ \\hline \\Gamma \\vdash (E) :T \\end{array} \\end{array} $$ Lastly in rule \\({\\tt (tParen)}\\) , we type check a parenthesized expression by type-checking the inner expression. Type Checking rules for SIMP Statements The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of \\(\\Gamma \\vdash \\overline{S} : T\\) , this is because statements do not return a value (except for return statement, which returns a value for the entire program.) \\[ \\begin{array}{rc} {\\tt (tSeq)} & \\begin{array}{c} \\Gamma \\vdash S \\ \\ \\ \\Gamma \\vdash \\overline{S} \\\\ \\hline \\Gamma \\vdash S \\overline{S} \\end{array} \\end{array} \\] The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\) . It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) . \\[ \\begin{array}{rc} {\\tt (tAssign)} & \\begin{array}{c} \\Gamma \\vdash E : T \\ \\ \\ \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash X = E \\end{array} \\end{array} \\] The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\) . It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree. \\[ \\begin{array}{rc} {\\tt (tReturn)} & \\begin{array}{c} \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash return\\ X \\end{array} \\\\ \\\\ {\\tt (tNop)} & \\Gamma \\vdash nop \\end{array} \\] The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable. $$ \\begin{array}{rc} {\\tt (tIf)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2} \\ \\hline \\Gamma \\vdash if\\ E\\ {\\overline{S_1}}\\ else\\ { \\overline{S_2} } \\end{array} \\ \\ {\\tt (tWhile)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S} \\ \\hline \\Gamma \\vdash while\\ E\\ {\\overline{S}} \\end{array} \\end{array} $$ The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\) . It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\) . The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way. We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) , i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\) . On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\) , we consider the type checking derivation of \\[x = input; s = 0; while\\ s<x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] \u0393 |- s:int (tVar) \u0393 |- 0:int (tInt) \u0393 |- input:int (tVar) -----------------(tAssign) [sub tree 1] \u0393 |- x:int (tVar) \u0393 |- s=0 ------------------(tAssign) --------------------------------------(tSeq) \u0393 |- x=input; \u0393 |- s=0; while s<x { s = s + 1;} return s; ---------------------------------------------------------------------(tSeq) \u0393 |- x=input; s=0; while s<x { s = s + 1;} return s; Where [sub tree 1] is \u0393 |- 0:int (tInt) \u0393 |- s:int (tVar) \u0393 |- s:int (tVar) -----------------(tOp1) \u0393 |- x:int (tVar) \u0393 |-s:int (tVar) \u0393 |- s+1:int --------------(tOp2) -------------------------------(tAssign) \u0393 |- s<x:bool \u0393 |- s = s + 1 \u0393 |- s:int (tVar) ---------------------------------------------(tWhile) ---------------(tReturn) \u0393 |- while s<x { s = s + 1;} \u0393 |- return s --------------------------------------------------------------------(tSeq) \u0393 |- while s<x { s = s + 1;} return s; Note that the following two programs are not typeable. // untypeable 1 x = 1; y = 0; if x { y = 0; } else { y = 1; } return y; The above is untypeable because we use x of type int in a context where it is also expected as bool . // untypeable 2 x = input; if (x > 1) { y = true; } else { y = 0; } return y; The above is unteable because we can't find a type environment which has both (y,int) and (y,bool) . So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative . // untypeable 3 x = input; if (x > 1) { if ( x * x * x < x * x) { y = true; } else { y = 1; } } else { y = 0; } return y; Even though we note that when x > 1 , we have x * x * x < x * x == false hence the statement y = true is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units. Let's connect the type-checking rules for SIMP with it dynamic semantics. Definition 1 - Type and Value Environments Consistency We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\) . It means the type environments and value environments are consistent. Property 2 - Progress The following property says that a well typed SIMP program must not be stuck until it reaches the return statement. Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\overline{S}\\) is either 1. a return statement, or 1. a sequence of statements, and there exist \\(\\Delta\\) , \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Property 3 - Preservation The following property says that the evaluation of a SIMP program does not change its typeability. Let \\(\\Delta\\) , \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\) . What is Type Inference Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. For example, given the Scala program def f(x:Int) = x + 1 the Scala compiler is able to deduce that the return type of f is Int . Likewise for the following SIMP program y = y + 1 we can also deduce that y is a of type int . What we aim to achieve is a sound and systematic process to deduce the omitted type information. Type inference for SIMP program Given a SIMP program \\(\\overline{S}\\) , the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution. Definition - Most general type (envrionment) Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\) . \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\) . Type Inference Rules We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures. \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} & \\hat{T} & ::= &\\alpha \\mid T \\\\ {\\tt (Constraints)} & \\kappa & \\subseteq & (\\hat{T} \\times \\hat{T}) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\) . Type substititution replace type variable to some other type. \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta & = & \\beta & if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T & = & T \\end{array} \\] Type substiution can be compositional . \\[ \\begin{array}{rcll} (\\Psi_1 \\circ \\Psi_2) \\hat{T} & = & \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms. Type Inference Rules for SIMP statements The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\) , which reads give a sequence of statements \\(\\overline{S}\\) , we generate a set of type constraints \\(\\kappa\\) . $$ \\begin{array}{rc} {\\tt (tiNOP)} & nop\\vDash {} \\ \\ {\\tt (tiReturn)} & return\\ X \\vDash {} \\end{array} $$ The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned. Similar observation applies to the return statement. $$ \\begin{array}{rc} {\\tt (tiSeq)} & \\begin{array}{c} S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2 \\ \\hline S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} $$ The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\) . We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\) . $$ \\begin{array}{rc} {\\tt (tiAssign)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline X = E \\vDash { (\\alpha_X, \\hat{T}) } \\cup \\kappa \\end{array} \\ \\ \\end{array} $$ The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\) , the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\) , which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\) , it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\) 's type and the type of the assignment's RHS must agree. $$ \\begin{array}{rc} {\\tt (tiIf)} & \\begin{array}{c} E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3 \\ \\hline if\\ E\\ {\\overline{S_2}}\\ else {\\overline{S_3}} \\vDash {(\\hat{T_1}, bool)} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3 \\end{array} \\ \\ \\end{array} $$ The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\) 's type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\) . \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\) , \\(\\kappa_2\\) and \\(\\kappa_3\\) , in addition, requiring \\(E\\) 's type must be \\(bool\\) . \\[ \\begin{array}{rc} {\\tt (tiWhile)} & \\begin{array}{c} E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\\\ \\hline while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The inference for while statement is very similar to if-else statement. We skip the explanation. Type Inference Rules for SIMP expressions The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\) . \\[ \\begin{array}{rc} {\\tt (tiInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\\\ \\hline C \\vDash int, \\{\\} \\end{array} \\\\ \\\\ {\\tt (tiBool)} & \\begin{array}{c} C\\ \\in \\{true, false\\} \\\\ \\hline C \\vDash bool, \\{\\} \\end{array} \\end{array} \\] When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\) . \\[ \\begin{array}{rc} {\\tt (tiVar)} & X \\vDash \\alpha_X, \\{\\} \\end{array} \\] The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\) . A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\". For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form . \\[ \\begin{array}{rc} {\\tt (tiOp1)} & \\begin{array}{c} OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\\\ \\\\ {\\tt (tiOp2)} & \\begin{array}{c} OP \\in \\{<, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\) . \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. $$ \\begin{array}{rc} {\\tt (tiParen)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline (E) \\vDash \\hat{T}, \\kappa \\end{array} \\end{array} $$ The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression. Unification To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. \\[ \\begin{array}{rcl} mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(\\alpha, \\hat{T}) & = & [\\hat{T}/\\alpha] \\\\ mgu(\\hat{T}, \\alpha) & = & [\\hat{T}/\\alpha] \\\\ \\end{array} \\] The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier . Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows \\[ \\begin{array}{rcl} mgu(\\{\\}) & = & [] \\\\ mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) & = & let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\ & & \\ \\ \\ \\ \\ \\ \\kappa' = \\Psi_1(\\kappa) \\\\ & & \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa') \\\\ & & in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] There are two cases. the constraint set is empty, we return the empty (identity) substitution. the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\) , which yields a subsitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\) . Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\) . The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\) . Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\) . We see that in an example shortly. An Example Consider the following SIMP program x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < x) { // (\u03b1_y, \u03b1_x) y = y + 1; // (\u03b1_y, int) } For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows input|=\u03b1_input,{} (tiVar) -------------------------(tiAssign) [subtree 1] x=input|={(\u03b1_x,\u03b1_input)} -----------------------------------------------------------------------------(tiSeq) x=input; y=0; while (y<x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 1] is as follows y|=\u03b1_y,{} (tiVar) 0|=int,{} (tiInt) ------------------(tiAssign) [subtree 2] y=0|={(\u03b1_y,int)} --------------------------------------------------------(tiSeq) y=0; while (y<x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 2] is as follows y|=\u03b1_y,{} (tiVar) 1|=int,{} (tiInt) y|=\u03b1_y,{} (tiVar) --------------(tiOp1) x|=\u03b1_x,{} (tiVar) y+1|=int,{(\u03b1_y,int)} --------------(tiOp2) ----------------------(tiAssign) y<x|=bool,{(\u03b1_y,\u03b1_x)} y=y+1|= {(\u03b1_y,int)} ---------------------------------------------(tiWhile) --------------(tiReturn) while (y<x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} return y|= {} ---------------------------------------------------------------------(tiSeq) while (y<x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} From Type Substitution to Type Environment To derive the inferred type environment, we apply the type substitution to all the type variabales we created. Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\) . Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows, \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] Recall that the set of constraints generated from the running example is \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\] Unification from left to right Suppose the unification progress pick the entries from left to right \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) $$ \\begin{array}{ll} mgu({(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})}) & \\longrightarrow \\ let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} $$ Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] Unification from right to left Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) & \\longrightarrow \\\\ let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred the same type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\) . If you have time, you can try another order. Input's type In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\) . This is not always possible. Let's consider the following program. x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } In the genereated constraints, our algorithm can construct the subtitution \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\) . We may argue that this is an ill-defined program as input and x are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\) , we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\) . Uninitialized Variable There is another situatoin in which the inference algorithm fails to ground all the type variables. x = z; // (\u03b1_x, \u03b1_z) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as z is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis. Property 4: Type Inference Soundness The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\) . Property 5: Principality The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\) .","title":"50.054 Static Semantics For SIMP"},{"location":"static_semantics/#50054-static-semantics-for-simp","text":"","title":"50.054 Static Semantics For SIMP"},{"location":"static_semantics/#learning-outcomes","text":"Explain what static semantics is. Apply type checking rules to verify the type correctness property of a SIMP program. Explain the relation between type system and operational semantics. Apply type inference algorithm to generate a type environment given a SIMP program.","title":"Learning Outcomes"},{"location":"static_semantics/#what-is-static-semantics","text":"While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program. For example, a statically correct program, must satisfy some properties all uses of variables in it must be defined somewhere earlier. all the use of variables, the types must be matching with the expected type in the context. ... Here is a statically correct SIMP program, x = 0; y = input; if y > x { y = 0; } return y; because it satifies the first two properties. The following program is not statically correct. x = 0; y = input; if y + x { // type error x = z; // the use of an undefined variable z } return x; Static checking is to rule out the statically incorrect programs.","title":"What is static semantics?"},{"location":"static_semantics/#type-checking-for-simp","text":"We consider the type checking for SIMP programs. Recall the syntax rules for SIMP \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (X \\times T) \\end{array} \\] We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . We define two different relations, \\(\\Gamma \\vdash E : T\\) , which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\) . \\(\\Gamma \\vdash \\overline{S}\\) , which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\) .","title":"Type Checking for SIMP"},{"location":"static_semantics/#type-checking-rules-for-simp-expressions","text":"$$ \\begin{array}{rc} {\\tt (tVar)} & \\begin{array}{c} (X,T) \\in \\Gamma \\ \\hline \\Gamma \\vdash X : T \\end{array} \\end{array} $$ In the rule \\({\\tt (tVar)}\\) , we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\) . $$ \\begin{array}{rc} {\\tt (tInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash C : int \\end{array} \\ \\ {\\tt (tBool)} & \\begin{array}{c} C \\in {true,false} \\ \\hline \\Gamma \\vdash C : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tInt)}\\) , we type check an integer constant having type \\(int\\) . Similarly, we type check a boolean constant having type \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tOp1)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { +, -, * } \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : int \\end{array} \\ \\ {\\tt (tOp2)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP E_2 : bool \\end{array} \\ \\ {\\tt (tOp3)} & \\begin{array}{c} \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tOp1)}\\) , we type check an integer arithmetic operation having type \\(int\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp2)}\\) , we type check an integer comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp3)}\\) , we type check a boolean comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tParen)} & \\begin{array}{c} \\Gamma \\vdash E :T \\ \\hline \\Gamma \\vdash (E) :T \\end{array} \\end{array} $$ Lastly in rule \\({\\tt (tParen)}\\) , we type check a parenthesized expression by type-checking the inner expression.","title":"Type checking rules for SIMP Expressions"},{"location":"static_semantics/#type-checking-rules-for-simp-statements","text":"The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of \\(\\Gamma \\vdash \\overline{S} : T\\) , this is because statements do not return a value (except for return statement, which returns a value for the entire program.) \\[ \\begin{array}{rc} {\\tt (tSeq)} & \\begin{array}{c} \\Gamma \\vdash S \\ \\ \\ \\Gamma \\vdash \\overline{S} \\\\ \\hline \\Gamma \\vdash S \\overline{S} \\end{array} \\end{array} \\] The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\) . It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) . \\[ \\begin{array}{rc} {\\tt (tAssign)} & \\begin{array}{c} \\Gamma \\vdash E : T \\ \\ \\ \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash X = E \\end{array} \\end{array} \\] The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\) . It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree. \\[ \\begin{array}{rc} {\\tt (tReturn)} & \\begin{array}{c} \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash return\\ X \\end{array} \\\\ \\\\ {\\tt (tNop)} & \\Gamma \\vdash nop \\end{array} \\] The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable. $$ \\begin{array}{rc} {\\tt (tIf)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2} \\ \\hline \\Gamma \\vdash if\\ E\\ {\\overline{S_1}}\\ else\\ { \\overline{S_2} } \\end{array} \\ \\ {\\tt (tWhile)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S} \\ \\hline \\Gamma \\vdash while\\ E\\ {\\overline{S}} \\end{array} \\end{array} $$ The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\) . It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\) . The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way. We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) , i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\) . On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\) , we consider the type checking derivation of \\[x = input; s = 0; while\\ s<x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] \u0393 |- s:int (tVar) \u0393 |- 0:int (tInt) \u0393 |- input:int (tVar) -----------------(tAssign) [sub tree 1] \u0393 |- x:int (tVar) \u0393 |- s=0 ------------------(tAssign) --------------------------------------(tSeq) \u0393 |- x=input; \u0393 |- s=0; while s<x { s = s + 1;} return s; ---------------------------------------------------------------------(tSeq) \u0393 |- x=input; s=0; while s<x { s = s + 1;} return s; Where [sub tree 1] is \u0393 |- 0:int (tInt) \u0393 |- s:int (tVar) \u0393 |- s:int (tVar) -----------------(tOp1) \u0393 |- x:int (tVar) \u0393 |-s:int (tVar) \u0393 |- s+1:int --------------(tOp2) -------------------------------(tAssign) \u0393 |- s<x:bool \u0393 |- s = s + 1 \u0393 |- s:int (tVar) ---------------------------------------------(tWhile) ---------------(tReturn) \u0393 |- while s<x { s = s + 1;} \u0393 |- return s --------------------------------------------------------------------(tSeq) \u0393 |- while s<x { s = s + 1;} return s; Note that the following two programs are not typeable. // untypeable 1 x = 1; y = 0; if x { y = 0; } else { y = 1; } return y; The above is untypeable because we use x of type int in a context where it is also expected as bool . // untypeable 2 x = input; if (x > 1) { y = true; } else { y = 0; } return y; The above is unteable because we can't find a type environment which has both (y,int) and (y,bool) . So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative . // untypeable 3 x = input; if (x > 1) { if ( x * x * x < x * x) { y = true; } else { y = 1; } } else { y = 0; } return y; Even though we note that when x > 1 , we have x * x * x < x * x == false hence the statement y = true is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units. Let's connect the type-checking rules for SIMP with it dynamic semantics.","title":"Type Checking rules for SIMP Statements"},{"location":"static_semantics/#definition-1-type-and-value-environments-consistency","text":"We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\) . It means the type environments and value environments are consistent.","title":"Definition 1 - Type and Value Environments Consistency"},{"location":"static_semantics/#property-2-progress","text":"The following property says that a well typed SIMP program must not be stuck until it reaches the return statement. Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\overline{S}\\) is either 1. a return statement, or 1. a sequence of statements, and there exist \\(\\Delta\\) , \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) .","title":"Property 2 - Progress"},{"location":"static_semantics/#property-3-preservation","text":"The following property says that the evaluation of a SIMP program does not change its typeability. Let \\(\\Delta\\) , \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\) .","title":"Property 3 - Preservation"},{"location":"static_semantics/#what-is-type-inference","text":"Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. For example, given the Scala program def f(x:Int) = x + 1 the Scala compiler is able to deduce that the return type of f is Int . Likewise for the following SIMP program y = y + 1 we can also deduce that y is a of type int . What we aim to achieve is a sound and systematic process to deduce the omitted type information.","title":"What is Type Inference"},{"location":"static_semantics/#type-inference-for-simp-program","text":"Given a SIMP program \\(\\overline{S}\\) , the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution.","title":"Type inference for SIMP program"},{"location":"static_semantics/#definition-most-general-type-envrionment","text":"Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\) . \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\) .","title":"Definition - Most general type (envrionment)"},{"location":"static_semantics/#type-inference-rules","text":"We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures. \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} & \\hat{T} & ::= &\\alpha \\mid T \\\\ {\\tt (Constraints)} & \\kappa & \\subseteq & (\\hat{T} \\times \\hat{T}) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\) . Type substititution replace type variable to some other type. \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta & = & \\beta & if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T & = & T \\end{array} \\] Type substiution can be compositional . \\[ \\begin{array}{rcll} (\\Psi_1 \\circ \\Psi_2) \\hat{T} & = & \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms.","title":"Type Inference Rules"},{"location":"static_semantics/#type-inference-rules-for-simp-statements","text":"The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\) , which reads give a sequence of statements \\(\\overline{S}\\) , we generate a set of type constraints \\(\\kappa\\) . $$ \\begin{array}{rc} {\\tt (tiNOP)} & nop\\vDash {} \\ \\ {\\tt (tiReturn)} & return\\ X \\vDash {} \\end{array} $$ The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned. Similar observation applies to the return statement. $$ \\begin{array}{rc} {\\tt (tiSeq)} & \\begin{array}{c} S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2 \\ \\hline S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} $$ The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\) . We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\) . $$ \\begin{array}{rc} {\\tt (tiAssign)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline X = E \\vDash { (\\alpha_X, \\hat{T}) } \\cup \\kappa \\end{array} \\ \\ \\end{array} $$ The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\) , the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\) , which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\) , it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\) 's type and the type of the assignment's RHS must agree. $$ \\begin{array}{rc} {\\tt (tiIf)} & \\begin{array}{c} E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3 \\ \\hline if\\ E\\ {\\overline{S_2}}\\ else {\\overline{S_3}} \\vDash {(\\hat{T_1}, bool)} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3 \\end{array} \\ \\ \\end{array} $$ The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\) 's type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\) . \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\) , \\(\\kappa_2\\) and \\(\\kappa_3\\) , in addition, requiring \\(E\\) 's type must be \\(bool\\) . \\[ \\begin{array}{rc} {\\tt (tiWhile)} & \\begin{array}{c} E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\\\ \\hline while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The inference for while statement is very similar to if-else statement. We skip the explanation.","title":"Type Inference Rules for SIMP statements"},{"location":"static_semantics/#type-inference-rules-for-simp-expressions","text":"The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\) . \\[ \\begin{array}{rc} {\\tt (tiInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\\\ \\hline C \\vDash int, \\{\\} \\end{array} \\\\ \\\\ {\\tt (tiBool)} & \\begin{array}{c} C\\ \\in \\{true, false\\} \\\\ \\hline C \\vDash bool, \\{\\} \\end{array} \\end{array} \\] When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\) . \\[ \\begin{array}{rc} {\\tt (tiVar)} & X \\vDash \\alpha_X, \\{\\} \\end{array} \\] The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\) . A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\". For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form . \\[ \\begin{array}{rc} {\\tt (tiOp1)} & \\begin{array}{c} OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\\\ \\\\ {\\tt (tiOp2)} & \\begin{array}{c} OP \\in \\{<, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\) . \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. $$ \\begin{array}{rc} {\\tt (tiParen)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline (E) \\vDash \\hat{T}, \\kappa \\end{array} \\end{array} $$ The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression.","title":"Type Inference Rules for SIMP expressions"},{"location":"static_semantics/#unification","text":"To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. \\[ \\begin{array}{rcl} mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(\\alpha, \\hat{T}) & = & [\\hat{T}/\\alpha] \\\\ mgu(\\hat{T}, \\alpha) & = & [\\hat{T}/\\alpha] \\\\ \\end{array} \\] The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier . Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows \\[ \\begin{array}{rcl} mgu(\\{\\}) & = & [] \\\\ mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) & = & let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\ & & \\ \\ \\ \\ \\ \\ \\kappa' = \\Psi_1(\\kappa) \\\\ & & \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa') \\\\ & & in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] There are two cases. the constraint set is empty, we return the empty (identity) substitution. the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\) , which yields a subsitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\) . Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\) . The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\) . Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\) . We see that in an example shortly.","title":"Unification"},{"location":"static_semantics/#an-example","text":"Consider the following SIMP program x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < x) { // (\u03b1_y, \u03b1_x) y = y + 1; // (\u03b1_y, int) } For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows input|=\u03b1_input,{} (tiVar) -------------------------(tiAssign) [subtree 1] x=input|={(\u03b1_x,\u03b1_input)} -----------------------------------------------------------------------------(tiSeq) x=input; y=0; while (y<x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 1] is as follows y|=\u03b1_y,{} (tiVar) 0|=int,{} (tiInt) ------------------(tiAssign) [subtree 2] y=0|={(\u03b1_y,int)} --------------------------------------------------------(tiSeq) y=0; while (y<x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 2] is as follows y|=\u03b1_y,{} (tiVar) 1|=int,{} (tiInt) y|=\u03b1_y,{} (tiVar) --------------(tiOp1) x|=\u03b1_x,{} (tiVar) y+1|=int,{(\u03b1_y,int)} --------------(tiOp2) ----------------------(tiAssign) y<x|=bool,{(\u03b1_y,\u03b1_x)} y=y+1|= {(\u03b1_y,int)} ---------------------------------------------(tiWhile) --------------(tiReturn) while (y<x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} return y|= {} ---------------------------------------------------------------------(tiSeq) while (y<x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)}","title":"An Example"},{"location":"static_semantics/#from-type-substitution-to-type-environment","text":"To derive the inferred type environment, we apply the type substitution to all the type variabales we created. Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\) . Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows, \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] Recall that the set of constraints generated from the running example is \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\]","title":"From Type Substitution to Type Environment"},{"location":"static_semantics/#unification-from-left-to-right","text":"Suppose the unification progress pick the entries from left to right \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) $$ \\begin{array}{ll} mgu({(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})}) & \\longrightarrow \\ let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} $$ Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\]","title":"Unification from left to right"},{"location":"static_semantics/#unification-from-right-to-left","text":"Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) & \\longrightarrow \\\\ let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred the same type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\) . If you have time, you can try another order.","title":"Unification from right to left"},{"location":"static_semantics/#inputs-type","text":"In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\) . This is not always possible. Let's consider the following program. x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } In the genereated constraints, our algorithm can construct the subtitution \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\) . We may argue that this is an ill-defined program as input and x are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\) , we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\) .","title":"Input's type"},{"location":"static_semantics/#uninitialized-variable","text":"There is another situatoin in which the inference algorithm fails to ground all the type variables. x = z; // (\u03b1_x, \u03b1_z) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as z is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis.","title":"Uninitialized Variable"},{"location":"static_semantics/#property-4-type-inference-soundness","text":"The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\) .","title":"Property 4: Type Inference Soundness"},{"location":"static_semantics/#property-5-principality","text":"The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\) .","title":"Property 5: Principality"},{"location":"static_semantics_2/","text":"50.054 Static Semantics for Lambda Calculus Learning Outcomes Apply type checking algorithm to type check a simply typed lambda calculus expression. Apply Hindley Milner algorithm to type check lambda calculus expressions. Apply Algorithm W to infer type for lambda calculus. Type Checking for Lambda Calculus To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. Recall the lambda calculus syntax, with the following adaptation $$ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times T) \\end{array} $$ The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\) . The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\) . Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus . Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details. We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\) , where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\) , \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (lctBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean. $$ \\begin{array}{cc} {\\tt (lctVar)} & \\begin{array}{c} (x, T) \\in \\Gamma \\ \\hline \\Gamma \\vdash x : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctVar)}\\) , we type check a variable \\(x\\) against a type \\(T\\) , which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\ \\hline \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T' \\end{array} \\end{array} $$ In rule \\({\\tt (lctLam)}\\) , we type check a lambda abstraction against a type \\(T\\rightarrow T'\\) . This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\) . $$ \\begin{array}{cc} {\\tt (lctApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctApp)}\\) , we type check a function application, applying \\(t_1\\) to \\(t_2\\) , against a type \\(T_2\\) . This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\) . $$ \\begin{array}{cc} {\\tt (lctLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\ \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x:T_1 = t_1\\ in\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctLet)}\\) , we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\) . This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment \\(\\Gamma \\oplus (x, T_1)\\) . $$ \\begin{array}{cc} {\\tt (lctIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\ \\hline \\Gamma \\vdash if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctIf)}\\) , we type check a if-then-else expression against type \\(T\\) . This is only valid if \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\) . \\[ \\begin{array}{cc} {\\tt (lctOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\\\ \\\\ {\\tt (lctOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ {\\tt (lctOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ \\end{array} \\] The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\) . \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree. \\[ \\begin{array}{cc} {\\tt (lctFix)} & \\begin{array}{c} \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2 \\\\ \\hline \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2 \\end{array} \\end{array} \\] The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\) . We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\) . For example, we would like to type check the following simply typed lambda term. $$ fix\\ (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if\\ x == 0\\ then\\ 1\\ else\\ (f\\ (x-1))* x))) $$ against the type \\(int \\rightarrow int\\) We added the optional parantheses for readability. We find the the following type checking derivation (proof tree). Let \u0393 be the initial type environment. \u0393\u2295(f:int->int)\u2295(x:int)|- x:int (lctVar) \u0393\u2295(f:int->int)\u2295(x:int)|- 0:int (lctInt) ---------------------------------------(lctOp2) [sub tree 1] [sub tree 2] \u0393\u2295(f:int->int)\u2295(x:int)|- x == 0: bool ------------------------------------------------------------------------------- (lctIf) \u0393\u2295(f:int->int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int --------------------------------------------------------------------(lctLam) \u0393\u2295(f:int->int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int->int --------------------------------------------------------------------------------(lctLam) \u0393 |- \u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int->int)->int->int ---------------------------------------------------------------------------------(lctFix) \u0393 |- fix (\u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int->int Let \u03931=\u0393\u2295(f:int->int)\u2295(x:int) Where [sub tree 1] is \u03931|- 1:int (lctInt) and [sub tree 2] is \u03931|-x:int (lctVar) \u03931|-1:int (lctInt) -----------------(lctOp1) \u03931|- f:int->int (lctVar) \u03931|- x-1:int -------------------------------------------------(lctApp) \u03931|- f (x-1):int \u03931 |- x:int (lctVar) -------------------------------------------------------------------------(lctOp1) \u03931|- (f (x-1))*x:int Another (counter) example which shows that we can't type check the following program \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] against the type \\(int\\) . fail, no proof exists ---------------------- \u0393\u2295(x:int)|- x:bool ----------------------------------(lctIf) \u0393|-1:int (lctInt) \u0393\u2295(x:int)|-if x then x else 0:int --------------------------------------------------------(lctLet) \u0393|- let x:int = 1 in (if x then x else 0):int Property 1 - Uniqueness The following property states that if a lambda term is typable, its type must be unique. Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\) . Then \\(T\\) and \\(T'\\) must be the same. Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\) , i.e. all the variables being mapped. Property 2 - Progress The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck. Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) . Property 3 - Preservation The third property states that the type of a lambda term does not change over evaluation. Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\) . Then \\(\\Gamma \\vdash t':T\\) . Issue with let-binding The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term. \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\ \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to f , either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both. To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System. Hindley Milner Type System We define the lambda calculus syntax for Hindley Milner Type System as follows \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\ {\\tt (Type Scheme)} & \\sigma & ::= & \\forall \\alpha. \\sigma \\mid T \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times \\sigma ) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them. We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types. We describe the Hindley Milner Type Checking rules as follows $$ \\begin{array}{rc} {\\tt (hmInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (hmBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rules for constants remain unchanged. \\[ \\begin{array}{rc} {\\tt (hmVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\\\ \\hline \\Gamma \\vdash x : \\sigma \\end{array} \\end{array} \\] The rule for variable is adjusted to use type signatures instead of types. \\[ \\begin{array}{rc} {\\tt (hmLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\\\ \\hline \\Gamma \\vdash \\lambda x.t :T\\rightarrow T' \\end{array} \\\\ \\\\ {\\tt (hmApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\\\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} \\] In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\) . It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\) . The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\) . $$ \\begin{array}{rc} {\\tt (hmFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma \\ \\hline \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha \\end{array} \\end{array} $$ To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\) . $$ \\begin{array}{rc} {\\tt (hmIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : \\sigma \\ \\ \\ \\Gamma \\vdash t_3 : \\sigma \\ \\hline \\Gamma \\vdash if\\ t_1\\ { t_2}\\ else { t_3 }: \\sigma \\end{array} \\ \\ \\end{array} $$ We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\) . $$ \\begin{array}{rc} {\\tt (hmOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in{+,-,*,/} \\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\ \\ {\\tt (hmOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ {\\tt (hmOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ \\end{array} $$ The type checking rules for binary operation remain unchanged. $$ \\begin{array}{rc} {\\tt (hmLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\ \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x = t_1\\ in\\ t_2 :T_2 \\end{array} \\ \\ {\\tt (hmInst)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2 \\ \\hline \\Gamma \\vdash t : \\sigma_2 \\end{array} \\ \\ {\\tt (hmGen)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma) \\ \\hline \\Gamma \\vdash t : \\forall \\alpha.\\sigma \\end{array} \\end{array} $$ In the rule \\({\\tt (hmLet)}\\) , we first type check \\(t_1\\) againt \\(\\sigma_1\\) , which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\) . For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\) . In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\) , provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) . Definition - Type Instances Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\) . In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\) . Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\) , then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\) . The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. \\[ \\begin{array}{rcl} ftv(\\alpha) & = & \\{\\alpha \\} \\\\ ftv(int) & = & \\{ \\} \\\\ ftv(bool) & = & \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) & = & ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) & = & ftv(\\sigma) - \\{ \\alpha \\} \\end{array} \\] \\(ftv()\\) is also overloaded to extra free type variables from a type environment. \\[ \\begin{array}{rcl} ftv(\\Gamma) & = & \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] The application of a type substitution can be defined as \\[ \\begin{array}{rcll} [] \\sigma & = & \\sigma \\\\ [T/\\alpha] int & = & int \\\\ [T/\\alpha] bool & = & bool \\\\ [T/\\alpha] \\alpha & = & T \\\\ [T/\\alpha] \\beta & = & \\beta & \\beta \\neq \\alpha \\\\ [T/\\alpha] T_1 \\rightarrow T_2 & = & ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\ [T/\\alpha] \\forall \\beta. \\sigma & = & \\forall \\beta. ([T/\\alpha]\\sigma) & \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\ (\\Psi_1 \\circ \\Psi_2)\\sigma & = & \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\) . Example Let's consider the type-checking derivation of our running (counter) example. Let \u0393 = {} and \u03931 = {(f,\u2200\u03b1.\u03b1->\u03b1)} . -------------------(hmVar) \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 --------------------(hmLam) \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3->\u03b2 ------------(hmVar) -------------------(hmLam) \u0393\u2295(x,\u03b1)|-x:\u03b1 \u03931|-\u03bbx.\u03bby.x:\u03b2->\u03b3->\u03b2 \u03b3,\u03b2\u2209ftv(\u03931) ------------(hmLam) --------------------------(hmGen) \u0393|-\u03bbx.x:\u03b1->\u03b1 \u03b1\u2209ftv(\u0393) \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 1] -----------------(hmGen) -------------------------------------------(hmLet) \u0393|-\u03bbx.x:\u2200\u03b1.\u03b1->\u03b1 \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int ------------------------------------------------------------------- (hmLet) \u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int Let \u03932 = {(f,\u2200\u03b1.\u03b1->\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)} , we find [subtree 1] is as follows --------------------(hmVar) \u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2291 \u2200\u03b3.int->\u03b3->int ----------------------------------(hmInst) \u03932|-g:\u2200\u03b3.int->\u03b3->int [subtree 3] -----------------------------------------------(hmApp) \u03932|-g (f 1):\u2200\u03b3.\u03b3->int \u2200\u03b3.\u03b3->int \u2291 bool->int -------------------------------------------------(hmInst) \u03932|-g (f 1):bool->int [subtree 2] ---------------------------------------------------------------(hmApp) \u03932|-g (f 1) (f true):int Where [subtree 2] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 bool->bool -------------------(hmInst) ----------------(hmBool) \u03932|-f:bool->bool \u03932|-true:bool ----------------------------------------------------(hmApp) \u03932|-f true:bool Where [subtree 3] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 int->int -------------------(hmInst) ----------------(hmInt) \u03932|-f:int->int \u03932|-1:int ---------------------------------------------------(hmApp) \u03932|-f 1:int As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\) , we are able to give let-bound variables f and g some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule. Property 4 - Uniqueness The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming. Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\) . Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming. For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same. Property 5 - Progress The Progress property is valid for Hindley Milner type checking. Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) . Property 6 - Preservation The Presevation property is also held for Hindley Milner type checking. Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\) . Then \\(\\Gamma \\vdash t':\\sigma\\) . Type Inference for Lambda Calculus To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W . The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\) , which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\) , the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\) . \\[ \\begin{array}{rc} {\\tt (wInt)} & \\begin{array}{c} c\\ {\\tt is\\ an\\ integer} \\\\ \\hline \\Gamma, c \\vDash int, [] \\end{array} \\\\ \\\\ {\\tt (wBool)} & \\begin{array}{c} c\\in \\{true,false \\} \\\\ \\hline \\Gamma, c \\vDash bool, [] \\end{array} \\end{array} \\] The rules for integer and boolean constants are straight forward. We omit the explanation. \\[ \\begin{array}{rc} {\\tt (wVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T \\\\ \\hline \\Gamma, x \\vDash T, [] \\end{array} \\\\ \\\\ {\\tt (wFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T \\\\ \\hline \\Gamma, fix \\vDash T, [] \\end{array} \\end{array} \\] The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\) . Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\) , which serves as the starting input. \\[ \\begin{array}{rc} {\\tt (wLam)} & \\begin{array}{c} \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi \\\\ \\hline \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi \\end{array} \\end{array} \\] The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\) . Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\) . The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\) . The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\) . For instance \\(\\lambda x. x + 1\\) will ground \\(x\\) 's skolem type variable to \\(int\\) . \\[ \\begin{array}{rc} {\\tt (wApp)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3) \\\\ \\hline \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\) . We first apply the inference recursively to \\(t_1\\) , producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\) . Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\) 's type as \\(T_2\\) with a subsitution \\(\\Psi_2\\) . To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\) ), and \\(T_2 \\rightarrow \\alpha_3\\) . If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\) . \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\) . At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution. \\[ \\begin{array}{rc} {\\tt (wLet)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2 \\\\ \\hline \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\) . By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\) . We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\) , and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. $$ \\begin{array}{rc} {\\tt (wOp1)} & \\begin{array}{c} op \\in {+,-,*,/} \\ \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\ \\ {\\tt (wOp2)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} $$ The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\) . In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\) . Finally we need to unify \\(\\Psi_2(T_1)\\) , \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\) . Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\) , i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\) , or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\) . We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. In rule \\({\\tt (wOp2)}\\) , the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\) . \\[ \\begin{array}{rc} {\\tt (wIf)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\ \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\ \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\ \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3)) \\\\ \\hline \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)), \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1' \\end{array} \\end{array} \\] In the rule \\({\\tt (wIf)}\\) , we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\) . Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\) . We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\) . Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\) . Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution. Helper functions We find the list of helper functions defined in Algorithm W. Type Substitution \\[ \\begin{array}{rcl} \\Psi(\\Gamma) &= & \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\] Type Instantiation \\[ \\begin{array}{rcl} inst(T) & = & T \\\\ inst(\\forall \\alpha.\\sigma) & = & \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] The type instantation function instantiate a type scheme. In case of a simple type \\(T\\) , it returns \\(T\\) . In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\) , we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\) . In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification. Type Generalization \\[ \\begin{array}{rcl} gen(\\Gamma, T) & = & \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\) , i.e. skolem variables. Type Unification \\[ \\begin{array}{rcl} mgu(\\alpha, T) & = & [T/\\alpha] \\\\ mgu(T, \\alpha) & = & [T/\\alpha] \\\\ mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) & = & let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\ & & in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ & & \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\) , we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them. Examples Let's consider some examples Example 1 \\(\\lambda x.x\\) Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\) (x,\u03b11)\u2208\u0393\u2295(x,\u03b11) inst(\u03b11)=\u03b11 ----------------------------(wVar) \u03b11=newvar \u0393\u2295(x,\u03b11),x|=\u03b11,[] ------------------------------------------(wLam) \u0393,\u03bbx.x|= \u03b11->\u03b11, [] Example 2 \\(\\lambda x.\\lambda y.x\\) (x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21 --------------------------------(wVar) \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[] --------------------------------------(wLam) \u03b21=newvar \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31->\u03b21,[] -------------------------------------------------(wLam) \u0393,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\) [Example 1] ------------------- \u0393,\u03bbx.x|= \u03b11->\u03b11, [] gen(\u0393,\u03b11->\u03b11)=\u2200\u03b1.\u03b1->\u03b1 [subtree 1] ------------------------------------------------------------------(wLet) \u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41] Let \u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1) , where [subtree 1] is [Example 2] -------------------------- \u03931,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] gen(\u03931,\u03b21->\u03b31->\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 2] -------------------------------------------------------------------------------(wLet) \u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[] Let \u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2) , where [subtree 2] is [subtree 3] [subtree 5] \u03b41=newvar mgu(\u03b32->int,bool->\u03b41)=[bool/\u03b32,int/\u03b41] --------------------------------------------------------------------------(wApp) \u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41] Where [subtree 3] is (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)\u2208\u03932 \u03b51=newvar inst(\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)=\u03b22->\u03b32->\u03b22 mgu(\u03b22->\u03b32->\u03b22,int->\u03b51)=[int/\u03b22,\u03b32->int/\u03b51] --------------------------(wVar) \u03932, g|=\u03b22->\u03b32->\u03b22, [] [subtree 4] ---------------------------------------------------------------------(wApp) \u03932, g (f 1)|= [int/\u03b22,\u03b32->int/\u03b51](\u03b51),[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] Where [subtree 4] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b12->\u03b12 \u03b61=newvar -----------------(wVar) ------------(wInt) \u03932, f|=\u03b12->\u03b12,[] \u03932,1|=int,[] mgu(\u03b12->\u03b12,int->\u03b61)=[int/\u03b61,int/\u03b12] ---------------------------------------------------------------------(wApp) [](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12] Let \u03a83=[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] , note that \u03a83(\u03932) =\u03932 , where [subtree 5] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b13->\u03b13 \u03b71=newvar ----------------(wVar) ----------(wBool) \u03932,f|=\u03b13->\u03b13, [] \u03932,true|=bool,[] mgu(\u03b13->\u03b13,bool->\u03b71)=[bool/\u03b13,bool/\u03b71] -----------------------------------------------------------------------[wApp] \u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71] Property 7: Type Inference Soundness The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\) . Property 8: Principality The following property states that the type generated by Algorithm W is the principal type. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\) .","title":"50.054 Static Semantics for Lambda Calculus"},{"location":"static_semantics_2/#50054-static-semantics-for-lambda-calculus","text":"","title":"50.054 Static Semantics for Lambda Calculus"},{"location":"static_semantics_2/#learning-outcomes","text":"Apply type checking algorithm to type check a simply typed lambda calculus expression. Apply Hindley Milner algorithm to type check lambda calculus expressions. Apply Algorithm W to infer type for lambda calculus.","title":"Learning Outcomes"},{"location":"static_semantics_2/#type-checking-for-lambda-calculus","text":"To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. Recall the lambda calculus syntax, with the following adaptation $$ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times T) \\end{array} $$ The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\) . The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\) . Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus . Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details. We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\) , where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\) , \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (lctBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean. $$ \\begin{array}{cc} {\\tt (lctVar)} & \\begin{array}{c} (x, T) \\in \\Gamma \\ \\hline \\Gamma \\vdash x : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctVar)}\\) , we type check a variable \\(x\\) against a type \\(T\\) , which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\ \\hline \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T' \\end{array} \\end{array} $$ In rule \\({\\tt (lctLam)}\\) , we type check a lambda abstraction against a type \\(T\\rightarrow T'\\) . This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\) . $$ \\begin{array}{cc} {\\tt (lctApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctApp)}\\) , we type check a function application, applying \\(t_1\\) to \\(t_2\\) , against a type \\(T_2\\) . This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\) . $$ \\begin{array}{cc} {\\tt (lctLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\ \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x:T_1 = t_1\\ in\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctLet)}\\) , we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\) . This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment \\(\\Gamma \\oplus (x, T_1)\\) . $$ \\begin{array}{cc} {\\tt (lctIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\ \\hline \\Gamma \\vdash if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctIf)}\\) , we type check a if-then-else expression against type \\(T\\) . This is only valid if \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\) . \\[ \\begin{array}{cc} {\\tt (lctOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\\\ \\\\ {\\tt (lctOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ {\\tt (lctOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ \\end{array} \\] The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\) . \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree. \\[ \\begin{array}{cc} {\\tt (lctFix)} & \\begin{array}{c} \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2 \\\\ \\hline \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2 \\end{array} \\end{array} \\] The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\) . We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\) . For example, we would like to type check the following simply typed lambda term. $$ fix\\ (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if\\ x == 0\\ then\\ 1\\ else\\ (f\\ (x-1))* x))) $$ against the type \\(int \\rightarrow int\\) We added the optional parantheses for readability. We find the the following type checking derivation (proof tree). Let \u0393 be the initial type environment. \u0393\u2295(f:int->int)\u2295(x:int)|- x:int (lctVar) \u0393\u2295(f:int->int)\u2295(x:int)|- 0:int (lctInt) ---------------------------------------(lctOp2) [sub tree 1] [sub tree 2] \u0393\u2295(f:int->int)\u2295(x:int)|- x == 0: bool ------------------------------------------------------------------------------- (lctIf) \u0393\u2295(f:int->int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int --------------------------------------------------------------------(lctLam) \u0393\u2295(f:int->int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int->int --------------------------------------------------------------------------------(lctLam) \u0393 |- \u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int->int)->int->int ---------------------------------------------------------------------------------(lctFix) \u0393 |- fix (\u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int->int Let \u03931=\u0393\u2295(f:int->int)\u2295(x:int) Where [sub tree 1] is \u03931|- 1:int (lctInt) and [sub tree 2] is \u03931|-x:int (lctVar) \u03931|-1:int (lctInt) -----------------(lctOp1) \u03931|- f:int->int (lctVar) \u03931|- x-1:int -------------------------------------------------(lctApp) \u03931|- f (x-1):int \u03931 |- x:int (lctVar) -------------------------------------------------------------------------(lctOp1) \u03931|- (f (x-1))*x:int Another (counter) example which shows that we can't type check the following program \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] against the type \\(int\\) . fail, no proof exists ---------------------- \u0393\u2295(x:int)|- x:bool ----------------------------------(lctIf) \u0393|-1:int (lctInt) \u0393\u2295(x:int)|-if x then x else 0:int --------------------------------------------------------(lctLet) \u0393|- let x:int = 1 in (if x then x else 0):int","title":"Type Checking for Lambda Calculus"},{"location":"static_semantics_2/#property-1-uniqueness","text":"The following property states that if a lambda term is typable, its type must be unique. Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\) . Then \\(T\\) and \\(T'\\) must be the same. Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\) , i.e. all the variables being mapped.","title":"Property 1 - Uniqueness"},{"location":"static_semantics_2/#property-2-progress","text":"The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck. Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) .","title":"Property 2 - Progress"},{"location":"static_semantics_2/#property-3-preservation","text":"The third property states that the type of a lambda term does not change over evaluation. Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\) . Then \\(\\Gamma \\vdash t':T\\) .","title":"Property 3 - Preservation"},{"location":"static_semantics_2/#issue-with-let-binding","text":"The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term. \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\ \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to f , either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both. To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System.","title":"Issue with let-binding"},{"location":"static_semantics_2/#hindley-milner-type-system","text":"We define the lambda calculus syntax for Hindley Milner Type System as follows \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\ {\\tt (Type Scheme)} & \\sigma & ::= & \\forall \\alpha. \\sigma \\mid T \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times \\sigma ) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them. We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types. We describe the Hindley Milner Type Checking rules as follows $$ \\begin{array}{rc} {\\tt (hmInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (hmBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rules for constants remain unchanged. \\[ \\begin{array}{rc} {\\tt (hmVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\\\ \\hline \\Gamma \\vdash x : \\sigma \\end{array} \\end{array} \\] The rule for variable is adjusted to use type signatures instead of types. \\[ \\begin{array}{rc} {\\tt (hmLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\\\ \\hline \\Gamma \\vdash \\lambda x.t :T\\rightarrow T' \\end{array} \\\\ \\\\ {\\tt (hmApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\\\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} \\] In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\) . It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\) . The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\) . $$ \\begin{array}{rc} {\\tt (hmFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma \\ \\hline \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha \\end{array} \\end{array} $$ To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\) . $$ \\begin{array}{rc} {\\tt (hmIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : \\sigma \\ \\ \\ \\Gamma \\vdash t_3 : \\sigma \\ \\hline \\Gamma \\vdash if\\ t_1\\ { t_2}\\ else { t_3 }: \\sigma \\end{array} \\ \\ \\end{array} $$ We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\) . $$ \\begin{array}{rc} {\\tt (hmOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in{+,-,*,/} \\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\ \\ {\\tt (hmOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ {\\tt (hmOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ \\end{array} $$ The type checking rules for binary operation remain unchanged. $$ \\begin{array}{rc} {\\tt (hmLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\ \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x = t_1\\ in\\ t_2 :T_2 \\end{array} \\ \\ {\\tt (hmInst)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2 \\ \\hline \\Gamma \\vdash t : \\sigma_2 \\end{array} \\ \\ {\\tt (hmGen)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma) \\ \\hline \\Gamma \\vdash t : \\forall \\alpha.\\sigma \\end{array} \\end{array} $$ In the rule \\({\\tt (hmLet)}\\) , we first type check \\(t_1\\) againt \\(\\sigma_1\\) , which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\) . For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\) . In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\) , provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) .","title":"Hindley Milner Type System"},{"location":"static_semantics_2/#definition-type-instances","text":"Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\) . In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\) . Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\) , then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\) . The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. \\[ \\begin{array}{rcl} ftv(\\alpha) & = & \\{\\alpha \\} \\\\ ftv(int) & = & \\{ \\} \\\\ ftv(bool) & = & \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) & = & ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) & = & ftv(\\sigma) - \\{ \\alpha \\} \\end{array} \\] \\(ftv()\\) is also overloaded to extra free type variables from a type environment. \\[ \\begin{array}{rcl} ftv(\\Gamma) & = & \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] The application of a type substitution can be defined as \\[ \\begin{array}{rcll} [] \\sigma & = & \\sigma \\\\ [T/\\alpha] int & = & int \\\\ [T/\\alpha] bool & = & bool \\\\ [T/\\alpha] \\alpha & = & T \\\\ [T/\\alpha] \\beta & = & \\beta & \\beta \\neq \\alpha \\\\ [T/\\alpha] T_1 \\rightarrow T_2 & = & ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\ [T/\\alpha] \\forall \\beta. \\sigma & = & \\forall \\beta. ([T/\\alpha]\\sigma) & \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\ (\\Psi_1 \\circ \\Psi_2)\\sigma & = & \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\) .","title":"Definition - Type Instances"},{"location":"static_semantics_2/#example","text":"Let's consider the type-checking derivation of our running (counter) example. Let \u0393 = {} and \u03931 = {(f,\u2200\u03b1.\u03b1->\u03b1)} . -------------------(hmVar) \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 --------------------(hmLam) \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3->\u03b2 ------------(hmVar) -------------------(hmLam) \u0393\u2295(x,\u03b1)|-x:\u03b1 \u03931|-\u03bbx.\u03bby.x:\u03b2->\u03b3->\u03b2 \u03b3,\u03b2\u2209ftv(\u03931) ------------(hmLam) --------------------------(hmGen) \u0393|-\u03bbx.x:\u03b1->\u03b1 \u03b1\u2209ftv(\u0393) \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 1] -----------------(hmGen) -------------------------------------------(hmLet) \u0393|-\u03bbx.x:\u2200\u03b1.\u03b1->\u03b1 \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int ------------------------------------------------------------------- (hmLet) \u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int Let \u03932 = {(f,\u2200\u03b1.\u03b1->\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)} , we find [subtree 1] is as follows --------------------(hmVar) \u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2291 \u2200\u03b3.int->\u03b3->int ----------------------------------(hmInst) \u03932|-g:\u2200\u03b3.int->\u03b3->int [subtree 3] -----------------------------------------------(hmApp) \u03932|-g (f 1):\u2200\u03b3.\u03b3->int \u2200\u03b3.\u03b3->int \u2291 bool->int -------------------------------------------------(hmInst) \u03932|-g (f 1):bool->int [subtree 2] ---------------------------------------------------------------(hmApp) \u03932|-g (f 1) (f true):int Where [subtree 2] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 bool->bool -------------------(hmInst) ----------------(hmBool) \u03932|-f:bool->bool \u03932|-true:bool ----------------------------------------------------(hmApp) \u03932|-f true:bool Where [subtree 3] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 int->int -------------------(hmInst) ----------------(hmInt) \u03932|-f:int->int \u03932|-1:int ---------------------------------------------------(hmApp) \u03932|-f 1:int As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\) , we are able to give let-bound variables f and g some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule.","title":"Example"},{"location":"static_semantics_2/#property-4-uniqueness","text":"The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming. Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\) . Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming. For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same.","title":"Property 4 - Uniqueness"},{"location":"static_semantics_2/#property-5-progress","text":"The Progress property is valid for Hindley Milner type checking. Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) .","title":"Property 5 - Progress"},{"location":"static_semantics_2/#property-6-preservation","text":"The Presevation property is also held for Hindley Milner type checking. Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\) . Then \\(\\Gamma \\vdash t':\\sigma\\) .","title":"Property 6 - Preservation"},{"location":"static_semantics_2/#type-inference-for-lambda-calculus","text":"To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W . The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\) , which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\) , the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\) . \\[ \\begin{array}{rc} {\\tt (wInt)} & \\begin{array}{c} c\\ {\\tt is\\ an\\ integer} \\\\ \\hline \\Gamma, c \\vDash int, [] \\end{array} \\\\ \\\\ {\\tt (wBool)} & \\begin{array}{c} c\\in \\{true,false \\} \\\\ \\hline \\Gamma, c \\vDash bool, [] \\end{array} \\end{array} \\] The rules for integer and boolean constants are straight forward. We omit the explanation. \\[ \\begin{array}{rc} {\\tt (wVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T \\\\ \\hline \\Gamma, x \\vDash T, [] \\end{array} \\\\ \\\\ {\\tt (wFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T \\\\ \\hline \\Gamma, fix \\vDash T, [] \\end{array} \\end{array} \\] The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\) . Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\) , which serves as the starting input. \\[ \\begin{array}{rc} {\\tt (wLam)} & \\begin{array}{c} \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi \\\\ \\hline \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi \\end{array} \\end{array} \\] The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\) . Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\) . The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\) . The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\) . For instance \\(\\lambda x. x + 1\\) will ground \\(x\\) 's skolem type variable to \\(int\\) . \\[ \\begin{array}{rc} {\\tt (wApp)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3) \\\\ \\hline \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\) . We first apply the inference recursively to \\(t_1\\) , producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\) . Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\) 's type as \\(T_2\\) with a subsitution \\(\\Psi_2\\) . To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\) ), and \\(T_2 \\rightarrow \\alpha_3\\) . If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\) . \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\) . At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution. \\[ \\begin{array}{rc} {\\tt (wLet)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2 \\\\ \\hline \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\) . By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\) . We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\) , and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. $$ \\begin{array}{rc} {\\tt (wOp1)} & \\begin{array}{c} op \\in {+,-,*,/} \\ \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\ \\ {\\tt (wOp2)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} $$ The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\) . In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\) . Finally we need to unify \\(\\Psi_2(T_1)\\) , \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\) . Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\) , i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\) , or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\) . We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. In rule \\({\\tt (wOp2)}\\) , the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\) . \\[ \\begin{array}{rc} {\\tt (wIf)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\ \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\ \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\ \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3)) \\\\ \\hline \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)), \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1' \\end{array} \\end{array} \\] In the rule \\({\\tt (wIf)}\\) , we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\) . Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\) . We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\) . Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\) . Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution.","title":"Type Inference for Lambda Calculus"},{"location":"static_semantics_2/#helper-functions","text":"We find the list of helper functions defined in Algorithm W.","title":"Helper functions"},{"location":"static_semantics_2/#type-substitution","text":"\\[ \\begin{array}{rcl} \\Psi(\\Gamma) &= & \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\]","title":"Type Substitution"},{"location":"static_semantics_2/#type-instantiation","text":"\\[ \\begin{array}{rcl} inst(T) & = & T \\\\ inst(\\forall \\alpha.\\sigma) & = & \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] The type instantation function instantiate a type scheme. In case of a simple type \\(T\\) , it returns \\(T\\) . In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\) , we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\) . In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification.","title":"Type Instantiation"},{"location":"static_semantics_2/#type-generalization","text":"\\[ \\begin{array}{rcl} gen(\\Gamma, T) & = & \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\) , i.e. skolem variables.","title":"Type Generalization"},{"location":"static_semantics_2/#type-unification","text":"\\[ \\begin{array}{rcl} mgu(\\alpha, T) & = & [T/\\alpha] \\\\ mgu(T, \\alpha) & = & [T/\\alpha] \\\\ mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) & = & let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\ & & in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ & & \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\) , we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them.","title":"Type Unification"},{"location":"static_semantics_2/#examples","text":"Let's consider some examples","title":"Examples"},{"location":"static_semantics_2/#example-1-lambda-xx","text":"Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\) (x,\u03b11)\u2208\u0393\u2295(x,\u03b11) inst(\u03b11)=\u03b11 ----------------------------(wVar) \u03b11=newvar \u0393\u2295(x,\u03b11),x|=\u03b11,[] ------------------------------------------(wLam) \u0393,\u03bbx.x|= \u03b11->\u03b11, []","title":"Example 1 \\(\\lambda x.x\\)"},{"location":"static_semantics_2/#example-2-lambda-xlambda-yx","text":"(x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21 --------------------------------(wVar) \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[] --------------------------------------(wLam) \u03b21=newvar \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31->\u03b21,[] -------------------------------------------------(wLam) \u0393,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[]","title":"Example 2 \\(\\lambda x.\\lambda y.x\\)"},{"location":"static_semantics_2/#example-3-let-flambda-xx-in-let-glambda-xlambda-yx-in-g-f-1-f-true","text":"[Example 1] ------------------- \u0393,\u03bbx.x|= \u03b11->\u03b11, [] gen(\u0393,\u03b11->\u03b11)=\u2200\u03b1.\u03b1->\u03b1 [subtree 1] ------------------------------------------------------------------(wLet) \u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41] Let \u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1) , where [subtree 1] is [Example 2] -------------------------- \u03931,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] gen(\u03931,\u03b21->\u03b31->\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 2] -------------------------------------------------------------------------------(wLet) \u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[] Let \u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2) , where [subtree 2] is [subtree 3] [subtree 5] \u03b41=newvar mgu(\u03b32->int,bool->\u03b41)=[bool/\u03b32,int/\u03b41] --------------------------------------------------------------------------(wApp) \u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41] Where [subtree 3] is (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)\u2208\u03932 \u03b51=newvar inst(\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)=\u03b22->\u03b32->\u03b22 mgu(\u03b22->\u03b32->\u03b22,int->\u03b51)=[int/\u03b22,\u03b32->int/\u03b51] --------------------------(wVar) \u03932, g|=\u03b22->\u03b32->\u03b22, [] [subtree 4] ---------------------------------------------------------------------(wApp) \u03932, g (f 1)|= [int/\u03b22,\u03b32->int/\u03b51](\u03b51),[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] Where [subtree 4] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b12->\u03b12 \u03b61=newvar -----------------(wVar) ------------(wInt) \u03932, f|=\u03b12->\u03b12,[] \u03932,1|=int,[] mgu(\u03b12->\u03b12,int->\u03b61)=[int/\u03b61,int/\u03b12] ---------------------------------------------------------------------(wApp) [](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12] Let \u03a83=[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] , note that \u03a83(\u03932) =\u03932 , where [subtree 5] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b13->\u03b13 \u03b71=newvar ----------------(wVar) ----------(wBool) \u03932,f|=\u03b13->\u03b13, [] \u03932,true|=bool,[] mgu(\u03b13->\u03b13,bool->\u03b71)=[bool/\u03b13,bool/\u03b71] -----------------------------------------------------------------------[wApp] \u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71]","title":"Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\)"},{"location":"static_semantics_2/#property-7-type-inference-soundness","text":"The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\) .","title":"Property 7: Type Inference Soundness"},{"location":"static_semantics_2/#property-8-principality","text":"The following property states that the type generated by Algorithm W is the principal type. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\) .","title":"Property 8: Principality"},{"location":"syntax_analysis/","text":"50.054 - Syntax Analysis Learning Outcome By the end of this lesson, you should be able to Describe the roles and functionalities of lexers and parsers in a compiler pipeline Describe the difference between top-down parsing and bottom-up parsing Apply left-recursion elimination and left-factoring Construct a LL(1) predictive parsing table Explain first-first conflicts and first-follow conflicts A compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation A parse tree can be considered the first intermediate representation (IR). Language, Grammar and Rules What is a language? A language is a set of strings. What is a grammar? A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words. One common way to define a grammar is by defining a set of production rules. A running example Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis. The grammar rule for JSON is as follows <<Grammar 1>> (JSON) J ::= i | 's' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 's':J In the above, the grammar consists of four production rules. Each production rule is of form (Name) LHS ::= RHS Sometimes, we omit the Name. Terms in upper case, are the non-terminal s, and terms in lower case, and symbol terms are the terminal s. For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by | . Each alternatives consists of terminals, non-terminals and mixture of both. For instance, the production rule (JSON) states that a JSON non-terminal J is either an i (an integer), a 's' (a quoted string), an empty list [] , an non-empty list [IS] and an object {NS} . A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the (JSON) production rule can be rewritten as follows, J ::= i J ::= 's' J ::= [] J ::= [IS] J ::= {NS} For each grammar, we expect the LHS of the first production rule is the starting symbol. Lexing Input: Source file in string Output: A sequence of valid tokens according to the language specification (grammar) The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing . Sometimes, a lexer is omitted, as the token validation task can be handled in the parser. Lexical Tokens The set of tokens of a grammar is basically all the terminals. In this JSON grammar example, {i, s, ', [, ], {, }, :, \\, } and white spaces are the Lexical Tokens of the language. If we are to represent it using Scala data types, we could use the following algebraic data type: enum LToken { // lexical Tokens case IntTok(v:Int) case StrTok(v:String) case SQuote case LBracket case RBracket case LBrace case RBrace case Colon case Comma case WhiteSpace } Note that in the above, we find that IntTok and StrTok have semantic components (i.e. the underlying values.) The rest of the tokens do not. Given the input {'k1':1,'k2':[]} the lexer function lex(s:String):List[LToken] should return List(LBRace,SQuote,StrTok(\"k1\"),SQuote,Colon,IntTok(1),Comma,SQuote, StrTok(\"k2\"), Colon,LBracket, RBracket, RBrace) One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows: <<Grammar 2>> (JSON) J ::= I | 'STR' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 'STR':J (Integer) I ::= dI | d (String) STR ::= aSTR | a where d denotes a single digit and a denotes a single ascii character. For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages. Implementing a Lexer using Regular Expression Perhaps one easier way to implement a lexer is to make use of regular expression. A simple example of using scala.util.matching.Regex We can specify a regex pattern as follows. This example was adopted from ( https://www.scala-lang.org/api/3.0.2/scala/util/matching/Regex.html ) val date = raw\"(\\d{4})-(\\d{2})-(\\d{2})\".r Next we can perform a match against the above regex pattern using the match expression. \"2004-01-20\" match { case date(year, month, day) => s\"$year was a good year for PLs.\" } The above expression is evaluated to 2004 was a good year for PLs. We could develop a simple lexer using the above trick. First we define the pattern for reach token. val integer = raw\"(\\d+)(.*)\".r val string = raw\"([^']*)(.*)\".r val squote = raw\"(')(.*)\".r val lbracket = raw\"(\\[)(.*)\".r val rbracket = raw\"(\\])(.*)\".r val lbrace = raw\"(\\{)(.*)\".r val rbrace = raw\"(\\})(.*)\".r val colon = raw\"(:)(.*)\".r val comma = raw\"(,)(.*)\".r For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration. Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned. import LToken.* type Error = String def lex_one(src:String):Either[String, (LToken, String)] = src match { case integer(s, rest) => Right((IntTok(s.toInt), rest)) case squote(_, rest) => Right((SQuote, rest)) case lbracket(_, rest) => Right((LBracket, rest)) case rbracket(_, rest) => Right((RBracket, rest)) case lbrace(_, rest) => Right((LBracket, rest)) case rbrace(_, rest) => Right((RBracket, rest)) case colon(_, rest) => Right((Colon, rest)) case comma(_, rest) => Right((Comma, rest)) case string(s, rest) => Right((StrTok(s), rest)) case _ => Left(s\"lexer error: unexpected token at ${src}\") } Note that the order of the Scala patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern string and the rest except for squote ). Lastly we define the top level lex function by calling lex_one in a recursive function. def lex(src:String):Either[Error, List[LToken]] = { def go(src:String, acc:List[LToken]):Either[Error, List[LToken]] = { if (src.length == 0) { Right(acc) } else { lex_one(src) match { case Left(error) => Left(error) case Right((ltoken, rest)) => go(rest, acc++List(ltoken)) } } } go(src, List()) } Implementing a Lexer using a Parser In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.) Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems. Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Why tree representation? Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation. Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules. Parsing Derivation Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS. Consider the JSON grammar in its unabridged form, (1) J ::= i (2) J ::= 's' (3) J ::= [] (4) J ::= [IS] (5) J ::= {NS} (6) IS ::= J,IS (7) IS ::= J (8) NS ::= N,NS (9) NS ::= N (10) N ::= 's':J We take the output from our lexer example as the input, with some simplification by removing the Scala constructors { , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , } For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom. Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } ... (for the steps skipped, please refer to syntax_analysis_annex.md) (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS). This algorithm is easy to understand but it has some flaws. It does not terminate when the grammar contains left recursion. It involves some trial-and-error (back-tracking), hence it is not efficient Ambiguous Grammar A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees. Consider the following <<Grammar 3>> E ::= E + E E ::= E * E E ::= i Consider the input 1 + 2 * 3 . Parsing this input with the above grammar produces graph E-->E1[\"E\"] E-->+ E-->E2[\"E\"] E1-->i1[\"i(1)\"] E2-->E3[\"E\"] E2-->* E2-->E4[\"E\"] E3-->i2[\"i(2)\"] E4-->i3[\"i(3)\"]; or graph E-->E1[\"E\"] E-->* E-->E2[\"E\"] E1-->E3[\"E\"] E1-->+ E1-->E4[\"E\"] E2-->i3[\"i(3)\"] E3-->i1[\"i(1)\"] E4-->i2[\"i(2)\"]; To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that * should bind stronger than + . Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset <<Grammar 4>> E::= T + E E::= T T::= T * F T::= F F::= i As a result, the input 1 + 2 * 3 is parsed as graph E-->T1[\"T\"] E-->+ E-->E1[\"E\"] T1-->F1[\"F\"] F1-->i1[\"i(1)\"] E1-->T2[\"T\"] T2-->F2[\"F\"] F2-->i2[\"i(2)\"] T2-->* T2-->F3[\"F\"] F3-->i3[\"i(3)\"] ; Grammar with Left Recursion Let's try to run a top-down recursive parsing algorithm over the following grammar <<Grammar 5>> E ::= E + T E ::= T T ::= i i and + are terminals, and E and T are the non-terminals. i denotes an integer. Consider applying the top-down recursive parsing mentioned above to the input 1 , if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation. Let N be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals) Left recursive grammar rules $$ \\begin{array}{rcl} N & ::= & N\\alpha_1 \\ & ... & \\ N & ::= & N\\alpha_n \\ N & ::= & \\beta_1 \\ & ... & \\ N & ::= & \\beta_m \\end{array} $$ can be transformed into \\[ \\begin{array}{rcl} N & ::= & \\beta_1 N' \\\\ & ... & \\\\ N & ::= & \\beta_m N' \\\\ N' & ::= & \\alpha_1 N' \\\\ & ... & \\\\ N' & ::= & \\alpha_n N' \\\\ N' & ::= & \\epsilon \\end{array} \\] Now apply the above to our running example. \\(N\\) is E and \\(\\alpha_1\\) is + T , T is \\(\\beta_1\\) . <<Grammar 6>> E ::= TE' E' ::= + TE' E' ::= epsilon T ::= i The resulting Grammar 6 is equivalent the original Grammar 5. Note that epsilon ( \\(\\epsilon\\) ) is a special terminal which denotes an empty sequence. There are few points to take note For indirect left recursion, some substitution steps are required before applying the above transformation. For instance <<Grammar 7>> G ::= H + G H ::= G + i H ::= i We need to substitute H into the first production rule. <<Grammar 8>> G ::= G + i + G G ::= i + G Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input 1 + 1 with Grammar 6 yields the following parse tree graph E-->T1[\"T\"] E-->Ep1[E'] T1-->i1[\"i(1)\"] Ep1-->+ Ep1-->T2[T] Ep1-->Ep2[E'] T2-->i2[\"i(1)\"] Ep2-->eps1[\u03b5] which needs to be transformed back to graph E-->E1[\"E\"] E-->+ E-->T1[\"T\"] E1-->T2[\"T\"] T1-->i1[\"i(1)\"] T2-->i2[\"i(1)\"] Predictive Recursive Parsing Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking. In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as LL(k) grammar. Here k refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking. BTW, LL(k) stands for left-to-right, left-most derivation with k tokens look-ahead algorithm. Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\) , \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\) \\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence. $$ \\begin{array}{rcl} null(t,G) & = & false \\ null(\\epsilon,G) & = & true \\ null(N,G) & = & \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\ null(\\sigma_1...\\sigma_n,G) & = & null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} $$ \\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\) . \\[ \\begin{array}{rcl} first(\\epsilon, G) & = & \\{\\} \\\\ first(t,G) & = & \\{t\\} \\\\ first(N,G) & = & \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) & = & \\left [ \\begin{array}{ll} first(\\sigma,G) \\cup first(\\overline{\\sigma},G) & {\\tt if}\\ null(\\sigma,G) \\\\ first(\\sigma,G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] \\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\) . \\[ \\begin{array}{rcl} follow(\\sigma,G) & = & \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G} \\left [ \\begin{array}{ll} first(\\overline{\\gamma}, G) \\cup follow(N,G) & {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\ first(\\overline{\\gamma}, G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] Sometimes, for convenience we omit the second parameter \\(G\\) . For example, let \\(G\\) be Grammar 6, then \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) = \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point . That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\) . We will discuss fix-point in-depth in some lesson later. Given \\(null\\) , \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in LL(k) . For simplicity, we check the case k = 1 , we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal. i + E E' T For each production rule \\(N ::= \\overline{\\sigma}\\) , we put the production rule in cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\) cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\) We fill up the table i + E E ::= TE' E' E' ::= + TE' T T ::= i We conclude that a grammar is in LL(1) if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a LL(1) grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol. In general, there are two kinds of conflicts found in grammar that violates the LL(1) grammar requirements. first-first conflict first-follow conflict First-first Conflict Consider the grammar <<Grammar 9>> S ::= Xb S ::= Yc X ::= a Y ::= a We compute \\(null\\) , \\(first\\) and \\(follow\\) . \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] We fill up the following predictive parsing table a b c S S::=Xb, S::=Yc X X::=a Y Y::=a From the above we find that there are two production rules in the cell (S,a) , namely S::=Xb , and S::=Yc . This is a first-first conflict, since both production rules' first set contains a . This prevents us from constructing a predictable parser by observing the leading symbol from the input. First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion). From our running example, we find that the cell (S,a) has more than one production rule applicable. This is caused by the fact that both X::=a and Y::=a start with the non-terminal a . We could apply substitution to eliminate X and Y . <<Grammar 10>> S ::= ab S ::= ac Then we could introduce a new non-terminal Z which capture the following languages after a . <<Grammar 11>> S ::= aZ Z ::= b Z ::= c First-Follow Conflict Consider the following grammar <<Grammar 12>> S ::= Xd X ::= C X ::= Ba C ::= epsilon B ::= d and the \\(null\\) , \\(first\\) and \\(follow\\) functions \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] We construct the predictive parsing table as follows a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d In the cell of (X,d) we find two production rules X::=Ba and X::=C (S::=Xd) . It is a first-follow conflict, because the first production rule is discovered through the first(X) set and the second one is from the follow(X) set. Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts Substitute B and C <<Grammar 13>> S ::= Xd X ::= epsilon X ::= da Substitute X <<Grammar 14>> S ::= d S ::= dad However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict. To LL(1) or not LL(1) Given a grammar, we could get a LL(1) grammar equivalent in most of the cases. Disambiguate the grammar if it is ambiguous Eliminate the left recursion Apply left-factoring if there exists some first-first conflict Apply substitution if there exists some first-follow conflict repeat 3 if first-first conflict is introduced Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm. Let's consider another example (a subset of Grammar 3). <<Grammar 15>> E ::= E + E E ::= i Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion <<Grammar 16>> E ::= iE' E' ::= + EE' E' ::= epsilon Next we compute the predictive parsing table. \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon As shown from the above, the grammar contains a first-follow conflict, therefore it is not a LL(1) . It is not possible to perform substitution to eliminate the first-follow conflict because it will lead to infinite expansion. A short summary so far for top-down recursive parsing Top-down parsing is simple, however might be inefficient. We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists. We need to eliminate left recursion so that the parsing will terminate. We construct the predictive parsing table to check whether the grammar is in LL(k) . If the grammar is in LL(k) we can always pick the right production rule given the first k leading symbols from the input. For most of the cases, LL(1) is sufficient for practical use. We also can conclude that a LL(k+1) grammar is also a LL(k) grammar, but the other way does not hold. Given a particular k and a grammar G , we can check whether G is LL(k) . However given a grammar G to find a k such that G is LL(k) is undecidable. Summary We have covered The roles and functionalities of lexers and parsers in a compiler pipeline There are two major types of parser, top-down parsing and bottom-up parsing How to eliminate left-recursion from a grammar, How to apply left-factoring How to construct a LL(1) predictive parsing table","title":"50.054 - Syntax Analysis"},{"location":"syntax_analysis/#50054-syntax-analysis","text":"","title":"50.054 - Syntax Analysis"},{"location":"syntax_analysis/#learning-outcome","text":"By the end of this lesson, you should be able to Describe the roles and functionalities of lexers and parsers in a compiler pipeline Describe the difference between top-down parsing and bottom-up parsing Apply left-recursion elimination and left-factoring Construct a LL(1) predictive parsing table Explain first-first conflicts and first-follow conflicts","title":"Learning Outcome"},{"location":"syntax_analysis/#a-compiler-pipeline","text":"graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation A parse tree can be considered the first intermediate representation (IR).","title":"A compiler pipeline"},{"location":"syntax_analysis/#language-grammar-and-rules","text":"","title":"Language, Grammar and Rules"},{"location":"syntax_analysis/#what-is-a-language","text":"A language is a set of strings.","title":"What is a language?"},{"location":"syntax_analysis/#what-is-a-grammar","text":"A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words. One common way to define a grammar is by defining a set of production rules.","title":"What is a grammar?"},{"location":"syntax_analysis/#a-running-example","text":"Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis. The grammar rule for JSON is as follows <<Grammar 1>> (JSON) J ::= i | 's' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 's':J In the above, the grammar consists of four production rules. Each production rule is of form (Name) LHS ::= RHS Sometimes, we omit the Name. Terms in upper case, are the non-terminal s, and terms in lower case, and symbol terms are the terminal s. For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by | . Each alternatives consists of terminals, non-terminals and mixture of both. For instance, the production rule (JSON) states that a JSON non-terminal J is either an i (an integer), a 's' (a quoted string), an empty list [] , an non-empty list [IS] and an object {NS} . A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the (JSON) production rule can be rewritten as follows, J ::= i J ::= 's' J ::= [] J ::= [IS] J ::= {NS} For each grammar, we expect the LHS of the first production rule is the starting symbol.","title":"A running example"},{"location":"syntax_analysis/#lexing","text":"Input: Source file in string Output: A sequence of valid tokens according to the language specification (grammar) The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing . Sometimes, a lexer is omitted, as the token validation task can be handled in the parser.","title":"Lexing"},{"location":"syntax_analysis/#lexical-tokens","text":"The set of tokens of a grammar is basically all the terminals. In this JSON grammar example, {i, s, ', [, ], {, }, :, \\, } and white spaces are the Lexical Tokens of the language. If we are to represent it using Scala data types, we could use the following algebraic data type: enum LToken { // lexical Tokens case IntTok(v:Int) case StrTok(v:String) case SQuote case LBracket case RBracket case LBrace case RBrace case Colon case Comma case WhiteSpace } Note that in the above, we find that IntTok and StrTok have semantic components (i.e. the underlying values.) The rest of the tokens do not. Given the input {'k1':1,'k2':[]} the lexer function lex(s:String):List[LToken] should return List(LBRace,SQuote,StrTok(\"k1\"),SQuote,Colon,IntTok(1),Comma,SQuote, StrTok(\"k2\"), Colon,LBracket, RBracket, RBrace) One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows: <<Grammar 2>> (JSON) J ::= I | 'STR' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 'STR':J (Integer) I ::= dI | d (String) STR ::= aSTR | a where d denotes a single digit and a denotes a single ascii character. For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages.","title":"Lexical Tokens"},{"location":"syntax_analysis/#implementing-a-lexer-using-regular-expression","text":"Perhaps one easier way to implement a lexer is to make use of regular expression.","title":"Implementing a Lexer using Regular Expression"},{"location":"syntax_analysis/#a-simple-example-of-using-scalautilmatchingregex","text":"We can specify a regex pattern as follows. This example was adopted from ( https://www.scala-lang.org/api/3.0.2/scala/util/matching/Regex.html ) val date = raw\"(\\d{4})-(\\d{2})-(\\d{2})\".r Next we can perform a match against the above regex pattern using the match expression. \"2004-01-20\" match { case date(year, month, day) => s\"$year was a good year for PLs.\" } The above expression is evaluated to 2004 was a good year for PLs. We could develop a simple lexer using the above trick. First we define the pattern for reach token. val integer = raw\"(\\d+)(.*)\".r val string = raw\"([^']*)(.*)\".r val squote = raw\"(')(.*)\".r val lbracket = raw\"(\\[)(.*)\".r val rbracket = raw\"(\\])(.*)\".r val lbrace = raw\"(\\{)(.*)\".r val rbrace = raw\"(\\})(.*)\".r val colon = raw\"(:)(.*)\".r val comma = raw\"(,)(.*)\".r For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration. Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned. import LToken.* type Error = String def lex_one(src:String):Either[String, (LToken, String)] = src match { case integer(s, rest) => Right((IntTok(s.toInt), rest)) case squote(_, rest) => Right((SQuote, rest)) case lbracket(_, rest) => Right((LBracket, rest)) case rbracket(_, rest) => Right((RBracket, rest)) case lbrace(_, rest) => Right((LBracket, rest)) case rbrace(_, rest) => Right((RBracket, rest)) case colon(_, rest) => Right((Colon, rest)) case comma(_, rest) => Right((Comma, rest)) case string(s, rest) => Right((StrTok(s), rest)) case _ => Left(s\"lexer error: unexpected token at ${src}\") } Note that the order of the Scala patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern string and the rest except for squote ). Lastly we define the top level lex function by calling lex_one in a recursive function. def lex(src:String):Either[Error, List[LToken]] = { def go(src:String, acc:List[LToken]):Either[Error, List[LToken]] = { if (src.length == 0) { Right(acc) } else { lex_one(src) match { case Left(error) => Left(error) case Right((ltoken, rest)) => go(rest, acc++List(ltoken)) } } } go(src, List()) }","title":"A simple example of using scala.util.matching.Regex"},{"location":"syntax_analysis/#implementing-a-lexer-using-a-parser","text":"In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.) Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems.","title":"Implementing a Lexer using a Parser"},{"location":"syntax_analysis/#parsing","text":"Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Why tree representation? Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation. Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules.","title":"Parsing"},{"location":"syntax_analysis/#parsing-derivation","text":"Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS. Consider the JSON grammar in its unabridged form, (1) J ::= i (2) J ::= 's' (3) J ::= [] (4) J ::= [IS] (5) J ::= {NS} (6) IS ::= J,IS (7) IS ::= J (8) NS ::= N,NS (9) NS ::= N (10) N ::= 's':J We take the output from our lexer example as the input, with some simplification by removing the Scala constructors { , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , } For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom. Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } ... (for the steps skipped, please refer to syntax_analysis_annex.md) (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS). This algorithm is easy to understand but it has some flaws. It does not terminate when the grammar contains left recursion. It involves some trial-and-error (back-tracking), hence it is not efficient","title":"Parsing Derivation"},{"location":"syntax_analysis/#ambiguous-grammar","text":"A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees. Consider the following <<Grammar 3>> E ::= E + E E ::= E * E E ::= i Consider the input 1 + 2 * 3 . Parsing this input with the above grammar produces graph E-->E1[\"E\"] E-->+ E-->E2[\"E\"] E1-->i1[\"i(1)\"] E2-->E3[\"E\"] E2-->* E2-->E4[\"E\"] E3-->i2[\"i(2)\"] E4-->i3[\"i(3)\"]; or graph E-->E1[\"E\"] E-->* E-->E2[\"E\"] E1-->E3[\"E\"] E1-->+ E1-->E4[\"E\"] E2-->i3[\"i(3)\"] E3-->i1[\"i(1)\"] E4-->i2[\"i(2)\"]; To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that * should bind stronger than + . Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset <<Grammar 4>> E::= T + E E::= T T::= T * F T::= F F::= i As a result, the input 1 + 2 * 3 is parsed as graph E-->T1[\"T\"] E-->+ E-->E1[\"E\"] T1-->F1[\"F\"] F1-->i1[\"i(1)\"] E1-->T2[\"T\"] T2-->F2[\"F\"] F2-->i2[\"i(2)\"] T2-->* T2-->F3[\"F\"] F3-->i3[\"i(3)\"] ;","title":"Ambiguous Grammar"},{"location":"syntax_analysis/#grammar-with-left-recursion","text":"Let's try to run a top-down recursive parsing algorithm over the following grammar <<Grammar 5>> E ::= E + T E ::= T T ::= i i and + are terminals, and E and T are the non-terminals. i denotes an integer. Consider applying the top-down recursive parsing mentioned above to the input 1 , if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation. Let N be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals) Left recursive grammar rules $$ \\begin{array}{rcl} N & ::= & N\\alpha_1 \\ & ... & \\ N & ::= & N\\alpha_n \\ N & ::= & \\beta_1 \\ & ... & \\ N & ::= & \\beta_m \\end{array} $$ can be transformed into \\[ \\begin{array}{rcl} N & ::= & \\beta_1 N' \\\\ & ... & \\\\ N & ::= & \\beta_m N' \\\\ N' & ::= & \\alpha_1 N' \\\\ & ... & \\\\ N' & ::= & \\alpha_n N' \\\\ N' & ::= & \\epsilon \\end{array} \\] Now apply the above to our running example. \\(N\\) is E and \\(\\alpha_1\\) is + T , T is \\(\\beta_1\\) . <<Grammar 6>> E ::= TE' E' ::= + TE' E' ::= epsilon T ::= i The resulting Grammar 6 is equivalent the original Grammar 5. Note that epsilon ( \\(\\epsilon\\) ) is a special terminal which denotes an empty sequence. There are few points to take note For indirect left recursion, some substitution steps are required before applying the above transformation. For instance <<Grammar 7>> G ::= H + G H ::= G + i H ::= i We need to substitute H into the first production rule. <<Grammar 8>> G ::= G + i + G G ::= i + G Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input 1 + 1 with Grammar 6 yields the following parse tree graph E-->T1[\"T\"] E-->Ep1[E'] T1-->i1[\"i(1)\"] Ep1-->+ Ep1-->T2[T] Ep1-->Ep2[E'] T2-->i2[\"i(1)\"] Ep2-->eps1[\u03b5] which needs to be transformed back to graph E-->E1[\"E\"] E-->+ E-->T1[\"T\"] E1-->T2[\"T\"] T1-->i1[\"i(1)\"] T2-->i2[\"i(1)\"]","title":"Grammar with Left Recursion"},{"location":"syntax_analysis/#predictive-recursive-parsing","text":"Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking. In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as LL(k) grammar. Here k refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking. BTW, LL(k) stands for left-to-right, left-most derivation with k tokens look-ahead algorithm. Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\) , \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\) \\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence. $$ \\begin{array}{rcl} null(t,G) & = & false \\ null(\\epsilon,G) & = & true \\ null(N,G) & = & \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\ null(\\sigma_1...\\sigma_n,G) & = & null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} $$ \\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\) . \\[ \\begin{array}{rcl} first(\\epsilon, G) & = & \\{\\} \\\\ first(t,G) & = & \\{t\\} \\\\ first(N,G) & = & \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) & = & \\left [ \\begin{array}{ll} first(\\sigma,G) \\cup first(\\overline{\\sigma},G) & {\\tt if}\\ null(\\sigma,G) \\\\ first(\\sigma,G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] \\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\) . \\[ \\begin{array}{rcl} follow(\\sigma,G) & = & \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G} \\left [ \\begin{array}{ll} first(\\overline{\\gamma}, G) \\cup follow(N,G) & {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\ first(\\overline{\\gamma}, G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] Sometimes, for convenience we omit the second parameter \\(G\\) . For example, let \\(G\\) be Grammar 6, then \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) = \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point . That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\) . We will discuss fix-point in-depth in some lesson later. Given \\(null\\) , \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in LL(k) . For simplicity, we check the case k = 1 , we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal. i + E E' T For each production rule \\(N ::= \\overline{\\sigma}\\) , we put the production rule in cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\) cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\) We fill up the table i + E E ::= TE' E' E' ::= + TE' T T ::= i We conclude that a grammar is in LL(1) if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a LL(1) grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol. In general, there are two kinds of conflicts found in grammar that violates the LL(1) grammar requirements. first-first conflict first-follow conflict","title":"Predictive Recursive Parsing"},{"location":"syntax_analysis/#first-first-conflict","text":"Consider the grammar <<Grammar 9>> S ::= Xb S ::= Yc X ::= a Y ::= a We compute \\(null\\) , \\(first\\) and \\(follow\\) . \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] We fill up the following predictive parsing table a b c S S::=Xb, S::=Yc X X::=a Y Y::=a From the above we find that there are two production rules in the cell (S,a) , namely S::=Xb , and S::=Yc . This is a first-first conflict, since both production rules' first set contains a . This prevents us from constructing a predictable parser by observing the leading symbol from the input. First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion). From our running example, we find that the cell (S,a) has more than one production rule applicable. This is caused by the fact that both X::=a and Y::=a start with the non-terminal a . We could apply substitution to eliminate X and Y . <<Grammar 10>> S ::= ab S ::= ac Then we could introduce a new non-terminal Z which capture the following languages after a . <<Grammar 11>> S ::= aZ Z ::= b Z ::= c","title":"First-first Conflict"},{"location":"syntax_analysis/#first-follow-conflict","text":"Consider the following grammar <<Grammar 12>> S ::= Xd X ::= C X ::= Ba C ::= epsilon B ::= d and the \\(null\\) , \\(first\\) and \\(follow\\) functions \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] We construct the predictive parsing table as follows a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d In the cell of (X,d) we find two production rules X::=Ba and X::=C (S::=Xd) . It is a first-follow conflict, because the first production rule is discovered through the first(X) set and the second one is from the follow(X) set. Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts Substitute B and C <<Grammar 13>> S ::= Xd X ::= epsilon X ::= da Substitute X <<Grammar 14>> S ::= d S ::= dad However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict.","title":"First-Follow Conflict"},{"location":"syntax_analysis/#to-ll1-or-not-ll1","text":"Given a grammar, we could get a LL(1) grammar equivalent in most of the cases. Disambiguate the grammar if it is ambiguous Eliminate the left recursion Apply left-factoring if there exists some first-first conflict Apply substitution if there exists some first-follow conflict repeat 3 if first-first conflict is introduced Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm. Let's consider another example (a subset of Grammar 3). <<Grammar 15>> E ::= E + E E ::= i Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion <<Grammar 16>> E ::= iE' E' ::= + EE' E' ::= epsilon Next we compute the predictive parsing table. \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon As shown from the above, the grammar contains a first-follow conflict, therefore it is not a LL(1) . It is not possible to perform substitution to eliminate the first-follow conflict because it will lead to infinite expansion.","title":"To LL(1) or not LL(1)"},{"location":"syntax_analysis/#a-short-summary-so-far-for-top-down-recursive-parsing","text":"Top-down parsing is simple, however might be inefficient. We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists. We need to eliminate left recursion so that the parsing will terminate. We construct the predictive parsing table to check whether the grammar is in LL(k) . If the grammar is in LL(k) we can always pick the right production rule given the first k leading symbols from the input. For most of the cases, LL(1) is sufficient for practical use. We also can conclude that a LL(k+1) grammar is also a LL(k) grammar, but the other way does not hold. Given a particular k and a grammar G , we can check whether G is LL(k) . However given a grammar G to find a k such that G is LL(k) is undecidable.","title":"A short summary so far for top-down recursive parsing"},{"location":"syntax_analysis/#summary","text":"We have covered The roles and functionalities of lexers and parsers in a compiler pipeline There are two major types of parser, top-down parsing and bottom-up parsing How to eliminate left-recursion from a grammar, How to apply left-factoring How to construct a LL(1) predictive parsing table","title":"Summary"},{"location":"syntax_analysis_2/","text":"50.054 - Syntax Analysis 2 Learning Outcome By the end of this lesson, you should be able to Construct a LR(0) parsing table Explain shift-reduce conflict Construct a SLR parsing table Bottom-up parsing An issue with LL(k) parsing is that we always need to make sure that we can pick the correct production rule by examining the first k tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking. What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing. LR(k) stands for left-to-right, right-most derivation with k lookahead tokens. In essence, LR(k) relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser. To understand LR(k) parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.) Let's recall Grammar 6 <<Grammar 6>> 1 S' ::= E$ 2 E ::= TE' 3 E' ::= + TE' 4 E' ::= epsilon 5 T ::= i We added number to each production rule, and we introduce a top level production rule S' ::= E$ where $ denotes the end of input symbol. Let's consider the following parsing table for Grammar 6. + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs, we assume that state IDs are having 2 digits, and state 10 is the starting state. In each cell, we find a set of parsing actions. shift s where s dentes a state ID. Given shift s in a cell ( s' , t ), we change the parser state from s' to s and consume the leading token t from the input and store it in the stack. accept . Given accept found in a cell ( s , $ ), the parsing is completed successfully. goto s where s denotes a state ID. Given goto s in a cell ( s' , t ), we change the parser's state to s . reduce p where p denotes a production rule ID. Given reduce p in a cell ( s , t ), lookup production rule LHS::=RHS from the grammar by p . We pop the items from top of the stack by reversing RHS . Given the state of the current top element of the stack, let's say s' , we lookup the goto action in cell ( s' , LHS ) and push LHS to the stack and perform the goto action. Consider the parsing the input 1+2+3 stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ We start with state (10) in the stack. Given the first token from the input is 1 (i.e. an i token), we look up the parsing table and find the shift 13 action in cell ( 10 , i ). By executing this action, we push i(13) in the stack. The next input is + . Given the current state is (13), we apply the smae strategy to find action reduce 5 in cell ( 13 , + ). Recall that the production rule with id 5 is T::=i , we pop the i(13) from the stack, and check for the correspondent action in cell ( 10 , T ), we find goto 11 . Hence we push T(11) into the stack. We follow the remaining steps to parse the input when we meet the accept action. One interesting observation is that the order of the rules found in the rule column is the reverse order of the list of rules we used in LL(k) parsing. Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for LR(k) grammars. LR(0) Parsing We first consider the simplest parsing table where we ignore the leading token from the input, LR(0) . The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine. From this point onwards, we use pseudo Scala syntax illustrate the algorithm behind the parsing table construction. Let . denote a meta symbol which indicate the current parsing context in a production rule. For instance for production rule 3 E' ::= +TE' , we have four possible contexts E' ::= .+TE' E' ::= +.TE' E' ::= +T.E' E' ::= +TE'. We call each of these possible contexts an Item . We define Items to be a set of Item s, Grammar to be a set of production rules (whose definition is omitted, we use the syntax LHS::=RHS directly in the pseudo-code.) type Items = Set[Item] type Grammar = Set[Prod] We consider the following operations. def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta) <- I (X ::= gamma) <- G } yield ( X::= . gamma ).union( for { (N ::= . epsilon ) <- I } yield ( N::= epsilon .) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta) <- I } yield (N ::= alpha X . beta) closure(J)(G) } Function closure takes an item set I and a grammar then returns the closure of I . For each item of shape N::=alpha . X beta in I , we look for the correspondent production rule X ::= gamma in G if X is a non-terminal, add X::= . gamma to the new item sets if it is not yet included in I . Noe that Scala by default does not support pattern such as (N ::= alpha . X beta) and (X::= gamma) . In this section, let's pretend that these patterns are allowed in Scala so that we can explain the algorithm in Scala style pseudo-code. Function goto takes an item set I and searches for item inside of shape N::= alpha . X beta then add N::=alpha X. beta as the next set J . We compute the closure of J and return it as result. type State = Items type Transition = (State, Symbol, State) case class StateMachine(states:Set[State], transitions:Set[Transition], accepts:Set[State]) def buildSM(init:State)(G:Grammar):StateMachine = { def step1(states:Set[State])(trans:Set[Transition]):(Set[State], Set[Transition]) = { // compute all states and transitions val newStateTrans = for { I <- states (A ::= alpha . X beta) <- I J <- pure(goto(I)(G)(X)) } yield (J, (I,X,J)) if newStateTrans.forall( st => st match { case (new_state, _) => states.contains(new_state) }) { (states, trans) } else { val newStates = newStateTrans.map( x => x._1) val newTrans = newStateTrans.map( x => x._2) step1(states.union(newStates))(trans.union(newTrans)) } } def step2(states:Set[State]):Set[State] = { // compute all final states states.filter( I => I.exists( item => item match { case (N ::= alpha . $) => true case _ => false })) } step1(Set(init))(Set()) match { case (states, trans) => { val finals = step2(states) StateMachine(states, trans, finals) } } } Function buildSM consists of two steps. In step1 we start with the initial state init and compute all possible states and transitions by applying goto . In step2 , we compute all final states. By applying buildSM to Grammar 6 yields the following state diagram. graph State10[\"State10 <br/> S'::=.E$ <br/> E::=.TE' <br/> T::=i \"]--T-->State11[\"State11 <br/> E::=T.E' <br/> E'::=+TE' <br/> E'::= . epsilon <br/> E'::= epsilon.\"] State10--E-->State12[\"State12 <br/> S'::=E.$\"] State10--i-->State13[\"State13 <br/> T::=i.\"] State11--+-->State14[\"State14 <br/> E'::= +.TE' <br/> T::=.i\"] State11--E'-->State17[\"State17 <br/> S'::=E.$\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E' <br/> E'::=.+TE' <br/> E'::=.epsilon <br/> E'::=epsilon . \"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'.\"] def reduce(states:List[State]):List[(Items, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items,Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a.append(List((I, N::=alpha))) case (a, _) => a } } } Function reduce takes a list of states and search for item set that contains an item of shape N::= alpha . . enum Action { case Shift(i:State) case Reduce(p:Prod) case Accept case Goto(i:State) } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, N::=alpha) <- reduce(states) x <- allTerminals(G) } yield (I, x, Reduce(N::=alpha)) val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Function ptable computes the LR(0) parsing table by making use of the functions defined earlier. Applying ptable to Grammar 6 yields + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol ( shift ) and a non-terminal symbol ( goto ). SLR parsing One issue with the above LR(0) parsing table is that we see conflicts in cells with multiple actions, e.g. cell ( 11 , + ). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the ptable function. In the ptable function, we blindly assign reduce actions to current state w.r.t. to all symbols. A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal. def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items, Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a ++ (follow(N).map( s => (I, s N::=alpha))) // fix case (a, _) => a } } } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, x, N::=alpha) <- reduce(states) } yield (I, x, Reduce(N::=alpha)) // fix val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section. LR(1) Parsing (Bonus materials) Besides SLR , LR(1) parsing also eliminates many conflicts found in LR(0) . The idea is to re-define item to include the look ahead token. For instance for production rule 3 E' ::= +TE' , we have 12 possible items ( E' ::= .+TE' , + ) ( E' ::= +.TE' , + ) ( E' ::= +T.E' , + ) ( E' ::= +TE'. , + ) ( E' ::= .+TE' , i ) ( E' ::= +.TE' , i ) ( E' ::= +T.E' , i ) ( E' ::= +TE'. , i ) ( E' ::= .+TE' , $ ) ( E' ::= +.TE' , $ ) ( E' ::= +T.E' , $ ) ( E' ::= +TE'. , $ ) We adjust the definition of closure and goto def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta, t) <- I (X ::= gamma) <- G w <- first(beta t) } yield ( X::= . gamma, w).union( for { (N ::= . epsilon, t) <- I } yield ( N::= epsilon ., t) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta, t) <- I } yield (N ::= alpha X . beta, t) closure(J)(G) } When computing the closure of an item (N ::= alpha . X beta, t) , we look up production rule X ::= gamma , to add X ::= .gamma into the new item set, we need to consider the possible leading terminal tokens coming from beta , and t in case beta accepts epsilon. Applying the adjusted definition, we have the follow state diagram graph State10[\"State10 <br/> S'::=.E$, ? <br/> E::=.TE', $ <br/> T::=i, $ <br/> T::=i, + \"]--T-->State11[\"State11 <br/> E::=T.E', $ <br/> E'::=+TE', $ <br/> E'::= . epsilon, $ <br/> E'::= epsilon., $\"] State10--E-->State12[\"State12 <br/> S'::=E.$, ?\"] State10--i-->State13[\"State13 <br/> T::=i., + <br/> T::=i., $\"] State11--+-->State14[\"State14 <br/> E'::= +.TE', $ <br/> T::=.i, + \"] State11--E'-->State17[\"State17 <br/> S'::=E.$, ?\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E', $ <br/> E'::=.+TE', $ <br/> E'::=.epsilon, $ <br/> E'::=epsilon., $\"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'., $\"] For the top-most production rule, there is no leading token, we put a special symbol ? , which does not affect the parsing. To incorporate item's new definition, we adjust the reduce function as follows def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha ., t)) => a.append(List((I, t, N::=alpha))) case (a, _) => a } } } buildSM and ptable function remain unchanged as per SLR parsing. By applying ptable we obtain the same parsing table as SLR parsing. SLR vs LR(1) LR(1) covers a larger set of grammar than SLR . For example consider the following grammar. <<Grammar 16>> 1 S' ::= S$ 2 S ::= A a 3 S ::= b A c 4 S ::= d c 5 S ::= b d a 6 A ::= d SLR produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$<br/> S::=.Aa <br/> S::= .bAc<br/> S::=.dc <br/> S::=.bda <br/> A::=.d \"]--S-->State11[\"State11 <br/> S'::=S.$\"] State10--A-->State12[\"State12 <br/> S::A.a\"] State10--b-->State13[\"State13 <br/> S::=b.Ac<br/> S::=b.da <br/> A::=.d\"] State10--d-->State14[\"State14 <br/> S::= d.c <br/> A::=d.\"] State12--a-->State15[\"State15 <br/> S::=Aa.\"] State13--A-->State16[\"State16 <br/> S::=bA.c\"] State13--d-->State17[\"State17 <br/> A::=d. <br/> S::=bd.a\"] State14--c-->State18[\"State18 <br/> S::=dc.\"] State16--c-->State19[\"State19 <br/> S::=bAc.\"] State17--a-->State20[\"State20 <br/> S::=bda.\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 There exist shift-reduce conflict. This is because in the closure computation when the item X::= . gamma is added to the closure, we approximate the next leading token by follow(X) . However there might be other alternative production rule for X in the grammar. This introduces extraneous reduce actions. LR(1) produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$, ? <br/> S::=.Aa, $ <br/> S::= .bAc, $ <br/> S::=.dc, $ <br/> S::=.bda, $ <br/> A::=.d, a \"]--S-->State11[\"State11 <br/> S'::=S.$, ?\"] State10--A-->State12[\"State12 <br/> S::A.a, $\"] State10--b-->State13[\"State13 <br/> S::=b.Ac, $<br/> S::=b.da, $ <br/> A::=.d, c\"] State10--d-->State14[\"State14 <br/> S::= d.c, $ <br/> A::=d., a\"] State12--a-->State15[\"State15 <br/> S::=Aa., $\"] State13--A-->State16[\"State16 <br/> S::=bA.c,$\"] State13--d-->State17[\"State17 <br/> A::=d.,c <br/> S::=bd.a, $\"] State14--c-->State18[\"State18 <br/> S::=dc., $\"] State16--c-->State19[\"State19 <br/> S::=bAc., $\"] State17--a-->State20[\"State20 <br/> S::=bda., $\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 In which the shift-reduce conflicts are eliminated because when given an item (N ::= alpha . X beta, t) , we add X ::= . gamma into the closure, by computing first(beta t) . This is only specific to this production rule X::= gamma and not other alternative. LR(1) and left recursion LR(1) can't handle all grammar with left recursion. For example processing Grammar 5 (from the previous unit) with LR(1) will result in some shift-reduce conflict. Summary We have covered How to construct a LR(0) parsing table How to construct a SLR parsing table","title":"50.054 - Syntax Analysis 2"},{"location":"syntax_analysis_2/#50054-syntax-analysis-2","text":"","title":"50.054 - Syntax Analysis 2"},{"location":"syntax_analysis_2/#learning-outcome","text":"By the end of this lesson, you should be able to Construct a LR(0) parsing table Explain shift-reduce conflict Construct a SLR parsing table","title":"Learning Outcome"},{"location":"syntax_analysis_2/#bottom-up-parsing","text":"An issue with LL(k) parsing is that we always need to make sure that we can pick the correct production rule by examining the first k tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking. What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing. LR(k) stands for left-to-right, right-most derivation with k lookahead tokens. In essence, LR(k) relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser. To understand LR(k) parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.) Let's recall Grammar 6 <<Grammar 6>> 1 S' ::= E$ 2 E ::= TE' 3 E' ::= + TE' 4 E' ::= epsilon 5 T ::= i We added number to each production rule, and we introduce a top level production rule S' ::= E$ where $ denotes the end of input symbol. Let's consider the following parsing table for Grammar 6. + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs, we assume that state IDs are having 2 digits, and state 10 is the starting state. In each cell, we find a set of parsing actions. shift s where s dentes a state ID. Given shift s in a cell ( s' , t ), we change the parser state from s' to s and consume the leading token t from the input and store it in the stack. accept . Given accept found in a cell ( s , $ ), the parsing is completed successfully. goto s where s denotes a state ID. Given goto s in a cell ( s' , t ), we change the parser's state to s . reduce p where p denotes a production rule ID. Given reduce p in a cell ( s , t ), lookup production rule LHS::=RHS from the grammar by p . We pop the items from top of the stack by reversing RHS . Given the state of the current top element of the stack, let's say s' , we lookup the goto action in cell ( s' , LHS ) and push LHS to the stack and perform the goto action. Consider the parsing the input 1+2+3 stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ We start with state (10) in the stack. Given the first token from the input is 1 (i.e. an i token), we look up the parsing table and find the shift 13 action in cell ( 10 , i ). By executing this action, we push i(13) in the stack. The next input is + . Given the current state is (13), we apply the smae strategy to find action reduce 5 in cell ( 13 , + ). Recall that the production rule with id 5 is T::=i , we pop the i(13) from the stack, and check for the correspondent action in cell ( 10 , T ), we find goto 11 . Hence we push T(11) into the stack. We follow the remaining steps to parse the input when we meet the accept action. One interesting observation is that the order of the rules found in the rule column is the reverse order of the list of rules we used in LL(k) parsing. Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for LR(k) grammars.","title":"Bottom-up parsing"},{"location":"syntax_analysis_2/#lr0-parsing","text":"We first consider the simplest parsing table where we ignore the leading token from the input, LR(0) . The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine. From this point onwards, we use pseudo Scala syntax illustrate the algorithm behind the parsing table construction. Let . denote a meta symbol which indicate the current parsing context in a production rule. For instance for production rule 3 E' ::= +TE' , we have four possible contexts E' ::= .+TE' E' ::= +.TE' E' ::= +T.E' E' ::= +TE'. We call each of these possible contexts an Item . We define Items to be a set of Item s, Grammar to be a set of production rules (whose definition is omitted, we use the syntax LHS::=RHS directly in the pseudo-code.) type Items = Set[Item] type Grammar = Set[Prod] We consider the following operations. def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta) <- I (X ::= gamma) <- G } yield ( X::= . gamma ).union( for { (N ::= . epsilon ) <- I } yield ( N::= epsilon .) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta) <- I } yield (N ::= alpha X . beta) closure(J)(G) } Function closure takes an item set I and a grammar then returns the closure of I . For each item of shape N::=alpha . X beta in I , we look for the correspondent production rule X ::= gamma in G if X is a non-terminal, add X::= . gamma to the new item sets if it is not yet included in I . Noe that Scala by default does not support pattern such as (N ::= alpha . X beta) and (X::= gamma) . In this section, let's pretend that these patterns are allowed in Scala so that we can explain the algorithm in Scala style pseudo-code. Function goto takes an item set I and searches for item inside of shape N::= alpha . X beta then add N::=alpha X. beta as the next set J . We compute the closure of J and return it as result. type State = Items type Transition = (State, Symbol, State) case class StateMachine(states:Set[State], transitions:Set[Transition], accepts:Set[State]) def buildSM(init:State)(G:Grammar):StateMachine = { def step1(states:Set[State])(trans:Set[Transition]):(Set[State], Set[Transition]) = { // compute all states and transitions val newStateTrans = for { I <- states (A ::= alpha . X beta) <- I J <- pure(goto(I)(G)(X)) } yield (J, (I,X,J)) if newStateTrans.forall( st => st match { case (new_state, _) => states.contains(new_state) }) { (states, trans) } else { val newStates = newStateTrans.map( x => x._1) val newTrans = newStateTrans.map( x => x._2) step1(states.union(newStates))(trans.union(newTrans)) } } def step2(states:Set[State]):Set[State] = { // compute all final states states.filter( I => I.exists( item => item match { case (N ::= alpha . $) => true case _ => false })) } step1(Set(init))(Set()) match { case (states, trans) => { val finals = step2(states) StateMachine(states, trans, finals) } } } Function buildSM consists of two steps. In step1 we start with the initial state init and compute all possible states and transitions by applying goto . In step2 , we compute all final states. By applying buildSM to Grammar 6 yields the following state diagram. graph State10[\"State10 <br/> S'::=.E$ <br/> E::=.TE' <br/> T::=i \"]--T-->State11[\"State11 <br/> E::=T.E' <br/> E'::=+TE' <br/> E'::= . epsilon <br/> E'::= epsilon.\"] State10--E-->State12[\"State12 <br/> S'::=E.$\"] State10--i-->State13[\"State13 <br/> T::=i.\"] State11--+-->State14[\"State14 <br/> E'::= +.TE' <br/> T::=.i\"] State11--E'-->State17[\"State17 <br/> S'::=E.$\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E' <br/> E'::=.+TE' <br/> E'::=.epsilon <br/> E'::=epsilon . \"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'.\"] def reduce(states:List[State]):List[(Items, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items,Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a.append(List((I, N::=alpha))) case (a, _) => a } } } Function reduce takes a list of states and search for item set that contains an item of shape N::= alpha . . enum Action { case Shift(i:State) case Reduce(p:Prod) case Accept case Goto(i:State) } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, N::=alpha) <- reduce(states) x <- allTerminals(G) } yield (I, x, Reduce(N::=alpha)) val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Function ptable computes the LR(0) parsing table by making use of the functions defined earlier. Applying ptable to Grammar 6 yields + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol ( shift ) and a non-terminal symbol ( goto ).","title":"LR(0) Parsing"},{"location":"syntax_analysis_2/#slr-parsing","text":"One issue with the above LR(0) parsing table is that we see conflicts in cells with multiple actions, e.g. cell ( 11 , + ). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the ptable function. In the ptable function, we blindly assign reduce actions to current state w.r.t. to all symbols. A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal. def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items, Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a ++ (follow(N).map( s => (I, s N::=alpha))) // fix case (a, _) => a } } } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, x, N::=alpha) <- reduce(states) } yield (I, x, Reduce(N::=alpha)) // fix val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section.","title":"SLR parsing"},{"location":"syntax_analysis_2/#lr1-parsing-bonus-materials","text":"Besides SLR , LR(1) parsing also eliminates many conflicts found in LR(0) . The idea is to re-define item to include the look ahead token. For instance for production rule 3 E' ::= +TE' , we have 12 possible items ( E' ::= .+TE' , + ) ( E' ::= +.TE' , + ) ( E' ::= +T.E' , + ) ( E' ::= +TE'. , + ) ( E' ::= .+TE' , i ) ( E' ::= +.TE' , i ) ( E' ::= +T.E' , i ) ( E' ::= +TE'. , i ) ( E' ::= .+TE' , $ ) ( E' ::= +.TE' , $ ) ( E' ::= +T.E' , $ ) ( E' ::= +TE'. , $ ) We adjust the definition of closure and goto def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta, t) <- I (X ::= gamma) <- G w <- first(beta t) } yield ( X::= . gamma, w).union( for { (N ::= . epsilon, t) <- I } yield ( N::= epsilon ., t) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta, t) <- I } yield (N ::= alpha X . beta, t) closure(J)(G) } When computing the closure of an item (N ::= alpha . X beta, t) , we look up production rule X ::= gamma , to add X ::= .gamma into the new item set, we need to consider the possible leading terminal tokens coming from beta , and t in case beta accepts epsilon. Applying the adjusted definition, we have the follow state diagram graph State10[\"State10 <br/> S'::=.E$, ? <br/> E::=.TE', $ <br/> T::=i, $ <br/> T::=i, + \"]--T-->State11[\"State11 <br/> E::=T.E', $ <br/> E'::=+TE', $ <br/> E'::= . epsilon, $ <br/> E'::= epsilon., $\"] State10--E-->State12[\"State12 <br/> S'::=E.$, ?\"] State10--i-->State13[\"State13 <br/> T::=i., + <br/> T::=i., $\"] State11--+-->State14[\"State14 <br/> E'::= +.TE', $ <br/> T::=.i, + \"] State11--E'-->State17[\"State17 <br/> S'::=E.$, ?\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E', $ <br/> E'::=.+TE', $ <br/> E'::=.epsilon, $ <br/> E'::=epsilon., $\"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'., $\"] For the top-most production rule, there is no leading token, we put a special symbol ? , which does not affect the parsing. To incorporate item's new definition, we adjust the reduce function as follows def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha ., t)) => a.append(List((I, t, N::=alpha))) case (a, _) => a } } } buildSM and ptable function remain unchanged as per SLR parsing. By applying ptable we obtain the same parsing table as SLR parsing.","title":"LR(1) Parsing (Bonus materials)"},{"location":"syntax_analysis_2/#slr-vs-lr1","text":"LR(1) covers a larger set of grammar than SLR . For example consider the following grammar. <<Grammar 16>> 1 S' ::= S$ 2 S ::= A a 3 S ::= b A c 4 S ::= d c 5 S ::= b d a 6 A ::= d SLR produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$<br/> S::=.Aa <br/> S::= .bAc<br/> S::=.dc <br/> S::=.bda <br/> A::=.d \"]--S-->State11[\"State11 <br/> S'::=S.$\"] State10--A-->State12[\"State12 <br/> S::A.a\"] State10--b-->State13[\"State13 <br/> S::=b.Ac<br/> S::=b.da <br/> A::=.d\"] State10--d-->State14[\"State14 <br/> S::= d.c <br/> A::=d.\"] State12--a-->State15[\"State15 <br/> S::=Aa.\"] State13--A-->State16[\"State16 <br/> S::=bA.c\"] State13--d-->State17[\"State17 <br/> A::=d. <br/> S::=bd.a\"] State14--c-->State18[\"State18 <br/> S::=dc.\"] State16--c-->State19[\"State19 <br/> S::=bAc.\"] State17--a-->State20[\"State20 <br/> S::=bda.\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 There exist shift-reduce conflict. This is because in the closure computation when the item X::= . gamma is added to the closure, we approximate the next leading token by follow(X) . However there might be other alternative production rule for X in the grammar. This introduces extraneous reduce actions. LR(1) produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$, ? <br/> S::=.Aa, $ <br/> S::= .bAc, $ <br/> S::=.dc, $ <br/> S::=.bda, $ <br/> A::=.d, a \"]--S-->State11[\"State11 <br/> S'::=S.$, ?\"] State10--A-->State12[\"State12 <br/> S::A.a, $\"] State10--b-->State13[\"State13 <br/> S::=b.Ac, $<br/> S::=b.da, $ <br/> A::=.d, c\"] State10--d-->State14[\"State14 <br/> S::= d.c, $ <br/> A::=d., a\"] State12--a-->State15[\"State15 <br/> S::=Aa., $\"] State13--A-->State16[\"State16 <br/> S::=bA.c,$\"] State13--d-->State17[\"State17 <br/> A::=d.,c <br/> S::=bd.a, $\"] State14--c-->State18[\"State18 <br/> S::=dc., $\"] State16--c-->State19[\"State19 <br/> S::=bAc., $\"] State17--a-->State20[\"State20 <br/> S::=bda., $\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 In which the shift-reduce conflicts are eliminated because when given an item (N ::= alpha . X beta, t) , we add X ::= . gamma into the closure, by computing first(beta t) . This is only specific to this production rule X::= gamma and not other alternative.","title":"SLR vs LR(1)"},{"location":"syntax_analysis_2/#lr1-and-left-recursion","text":"LR(1) can't handle all grammar with left recursion. For example processing Grammar 5 (from the previous unit) with LR(1) will result in some shift-reduce conflict.","title":"LR(1) and left recursion"},{"location":"syntax_analysis_2/#summary","text":"We have covered How to construct a LR(0) parsing table How to construct a SLR parsing table","title":"Summary"},{"location":"syntax_analysis_annex/","text":"Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (10) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->s N-->RQ['] N-->: N-->J2[J] ' s':J, NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->s N-->RQ['] N-->: N-->J2[J] s ':J, NS } k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] ' :J, NS } ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] : J, NS } : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J]; J, NS } 1 , ' k 2 ' : [ ] } (1) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i; i , NS } 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"]; , NS } , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"]; NS } ' k 2 ' : [ ] } (9) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N]; N } ' k 2 ' : [ ] } (10) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[s] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; ' s':J } ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[s] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; s ':J } k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; ' :J } ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; : J } : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; J } [ ] } (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"];","title":"Syntax analysis annex"},{"location":"notes/","text":"50.054 Compilers and Program Analysis Course Notes This repository contains the course notes for the 2023 Fall Term run of 50.054 Compilers and Program Analysis. The course handout can be found here .","title":"50.054 Compilers and Program Analysis Course Notes"},{"location":"notes/#50054-compilers-and-program-analysis-course-notes","text":"This repository contains the course notes for the 2023 Fall Term run of 50.054 Compilers and Program Analysis. The course handout can be found here .","title":"50.054 Compilers and Program Analysis Course Notes"},{"location":"notes/advanced_static_analysis/","text":"50.054 - Advanced Topics in Static Analysis Learning Outcomes Apply Path Sensitive Analysis to Sign Analysis. Apply Static Analysis to detect software security loopholes. Recall that sign analysis The Sign Analysis that we developed in the previous class has some limitation. // PA1 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ] 3: ifn t goto 6 // s3 = s2 4: x <- x - 1 // s4 = s3[ x -> s3(x) -- + ] 5: goto 2 // s5 = s4 6: y <- Math.sqrt(x) // s6 = s3 7: r_ret <- y 8: ret The monotonic equations in the comments are defined based on the following lattice. graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] And the abstract value operators are defined as -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) By converting the equation system into monotonic function f1((s0, s1, s2, s3, s4, s5, s6)) = ( [ x -> top, t -> top, input -> top ], s0[ x -> s0(input) ], lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ], s2, s3[ x -> s3(x) -- + ], s4, s3 ) when we apply the fixed point algorithm to the f1 and the VarSign lattice, we have the following solution s0 = [ x -> top, t -> top, input -> top ], s1 = [ x -> top, t -> top, input -> top ], s2 = [ x -> top, t -> top, input -> top ], s3 = [ x -> top, t -> top, input -> top ], s4 = [ x -> top, t -> top, input -> top ], s5 = [ x -> top, t -> top, input -> top ], s6 = [ x -> top, t -> top, input -> top ] At label 6, x 's sign is \\(\\top\\) . Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. Could it be due to the problem of how the abstract operators -- and >== are defined? No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as +0 and -0 Let's say we adjust the lattice mermaid graph A[\"\u22a4\"]---A1[+0] A[\"\u22a4\"]---A2[-0] A2---B[-] A2---C[0] A1---C[0] A1---D[+] B---E C---E D---E[\u22a5] * and the abstract operators -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + + \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) It does not help, as it might give t a more precise abtract value but it does not help to improve the result of x The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition x>=0 and the path of going out of the while loop is only valid under the condition x < 0 . Path sensitive analysis via assertion Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of PA1 in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop. // SIMP2 x = input; while x >= 0 { assert x >= 0; x = x - 1; } assert x < 0; y = Math.sqrt(x); return y; As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions // PA2 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ] 3: ifn t goto 7 // s3 = s2 4: assert x >= 0 // s4 = s3[ x -> gte(s3(x), 0) ] 5: x <- x - 1 // s5 = s4[ x -> s4(x) -- + ] 6: goto 2 // s6 = s5 7: assert x < 0 // s7 = s3[ x -> lt(s3(x), 0) ] 8: y <- Math.sqrt(x) // s8 = s7 9: r_ret <- y 10: ret We could add the following monotonic function synthesis case case \\(l: assert\\ t\\ >=\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\) case \\(l: assert\\ t\\ <\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\) Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs. gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) To show that the above definitions of gte and lt are sound. We can consider the range notation of the abstract values. $$ \\begin{array}{rcl} \\top & = & [-\\infty, +\\infty] \\ +0 & = & [0, +\\infty] \\ -0 & = & [-\\infty, 0] \\ \\ + & = & [1, +\\infty] \\ \\ - & = & [-\\infty, -1] \\ 0 & = & [0, 0] \\ \\bot & = & [+\\infty, -\\infty] \\end{array} $$ \\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\) . \\(\\bot\\) is an empty range. We can think of gte as \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] Similiarly we can think of lt as \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] where \\(+\\infty - 1 = +\\infty\\) With the adjusted monotonic equations, we can now define the monotonic function f2 as follows f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( [ x -> top, t -> top, input -> top ] s0[ x -> s0(input) ], lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ], s2, s3[ x -> gte(s3(x), 0) ], s4[ x -> s4(x) -- + ], s5, s3[ x -> lt(s3(x), 0) ], s7 ) By applying the fixed point algorithm to f2 we find the following solution s0 = [ x -> top, t -> top, input -> top ] s1 = [ x -> top, t -> top, input -> top ] s2 = [ x -> top, t -> +0, input -> top ] s3 = [ x -> top, t -> +0, input -> top ] s4 = [ x -> +0, t -> +0, input -> top ] s5 = [ x -> top, t -> +0, input -> top ] s6 = [ x -> -, t -> +0, input -> top] s7 = [ x -> -, t -> +0, input -> top] which detects that the sign of x at instruction 8 is - . Information Flow Analysis One widely applicable static analysis is information flow analysis. The information flow in a program describes how data are evaluated and propogated in the program via variables and operations. The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds. Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection. High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure. Tainted Flow IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id; try { Statement statement = dbconnection.createStatement(); ResultSet res = statement.executeQuery( query ); // access sensitive resource } The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. When the id is \"' OR 'a'='a'; delete from customer_data; --\" , the malicious user gains the login access and deletes all records from the customer_data table. This can be prevented by using a prepared statement. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\"; try { PreparedStatement pstmt = connection.prepareStatement( query ); pstmt.setString(1, id); // pstmt is sanitized before being used. ResultSet results = pstmt.executeQuery(); } One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple Let's recast the above into SIMP, we would have the vulunerable code as id = input(); query = \"select \" + id; exec(query); return id; We assume that we've extended SIMP to support string values and string concatenation. The input is a function that prompts the user for input. The exec function is a database builtin function. The following version fixed the vulnerability, assume the sanitize function, sanitizes the input. id = input(); query = \"select \"; query = sanitize(query, id) exec(query); return id; To increase the level of compexlity, let's add some control flow to the example. id = input(); query = \"select \" + id; while (id == \"\") { id = input(); query = sanitize(\"select \", id) } exec(query); return id; In the above, it is not directly clear that the exec() is given a sanitized query. The manual check becomes exponentially hard as the code size grows. We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. graph tainted --- clean --- bot(\"\u22a5\") We rewrite the above SIMP program into the following PA equivalent. 1: id <- input() 2: query <- \"select \" + id 3: b <- id == \"\" 4: ifn b goto 5: id <- input() 6: query <- sanitize(\"select\", id) 7: goto 3 8: _ <- exec(query) 9: r_ret <- id 10: ret We define the equation generation rules as follows, \\[ join(s_i) = \\bigsqcup pred(s_i) \\] where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\) case \\(l: t \\leftarrow input()\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\) case \\(l: t \\leftarrow sanitize(src_1, src_2)\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & clean \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] We inline the equations as comments in the PA code. // s0 = [id -> bot, query -> bot, b -> bot] 1: id <- input() // s1 = s0[ id -> tainted ] 2: query <- \"select \" + id // s2 = s1[ query -> lub(clean, s1(id)) ] 3: b <- id == \"\" // s3 = lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ] 4: ifn b goto 8 // s4 = s3 5: id <- input() // s5 = s4[ id -> tainted ] 6: query <- sanitize(\"select\", id) // s6 = s5[ query -> clean ] 7: goto 3 // s7 = s6 8: exec(query) // s8 = s4 9: r_ret <- id // s9 = s8 10: ret By applying the above we have the followning monotonic function. f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = ( [id -> bot, query -> bot, b -> bot], s0[ id -> tainted], s1[ query -> lub(clean, s1(id)) ], lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ], s3, s4[ id -> tainted ], s5[ query -> clean ], s6, s4, s8 ) By applying the fixed point algorithm, we find the following solution for the monotonic equations. s0 = [id -> bot, query -> bot, b -> bot] s1 = [id -> tainted, query -> bot, b -> bot] s2 = [id -> tainted, query -> tainted, b -> bot] s3 = [id -> tainted, query -> tainted, b -> tainted ] s4 = [id -> tainted, query -> tainted, b -> tainted ] s5 = [id -> tainted, query -> tainted, b -> tainted ] s6 = [id -> tainted, query -> clean, b -> tainted ] s7 = [id -> tainted, query -> clean, b -> tainted ] s8 = [id -> tainted, query -> tainted, b -> tainted ] s9 = [id -> tainted, query -> tainted, b -> tainted ] which says that the use of variable query at instruction 8 is risky as query may be tainted at this point. Sensitive Information Disclosure In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); pstmt.setString(1, username); pstmt.setString(2, password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table user is having a lower security level, since it is accessed by the database users. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); String hashed_password = hash(password); // the hash serves as a declassification operation. pstmt.setString(1, username); pstmt.setString(2, hashed_password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password. The hash function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except, We will have a different lattice for security level. graph secret --- confidential --- bot(\"\u22a5\") where secret has a higher level of security than confidential . Instead of using sanitize to increase the level of security, we use hash (or declassify ) to lower the level of security.","title":"50.054 - Advanced Topics in Static Analysis"},{"location":"notes/advanced_static_analysis/#50054-advanced-topics-in-static-analysis","text":"","title":"50.054 - Advanced Topics in Static Analysis"},{"location":"notes/advanced_static_analysis/#learning-outcomes","text":"Apply Path Sensitive Analysis to Sign Analysis. Apply Static Analysis to detect software security loopholes.","title":"Learning Outcomes"},{"location":"notes/advanced_static_analysis/#recall-that-sign-analysis","text":"The Sign Analysis that we developed in the previous class has some limitation. // PA1 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ] 3: ifn t goto 6 // s3 = s2 4: x <- x - 1 // s4 = s3[ x -> s3(x) -- + ] 5: goto 2 // s5 = s4 6: y <- Math.sqrt(x) // s6 = s3 7: r_ret <- y 8: ret The monotonic equations in the comments are defined based on the following lattice. graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] And the abstract value operators are defined as -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) By converting the equation system into monotonic function f1((s0, s1, s2, s3, s4, s5, s6)) = ( [ x -> top, t -> top, input -> top ], s0[ x -> s0(input) ], lub(s1,s5) [ t -> lub(s1,s5)(x) >== 0 ], s2, s3[ x -> s3(x) -- + ], s4, s3 ) when we apply the fixed point algorithm to the f1 and the VarSign lattice, we have the following solution s0 = [ x -> top, t -> top, input -> top ], s1 = [ x -> top, t -> top, input -> top ], s2 = [ x -> top, t -> top, input -> top ], s3 = [ x -> top, t -> top, input -> top ], s4 = [ x -> top, t -> top, input -> top ], s5 = [ x -> top, t -> top, input -> top ], s6 = [ x -> top, t -> top, input -> top ] At label 6, x 's sign is \\(\\top\\) . Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. Could it be due to the problem of how the abstract operators -- and >== are defined? No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as +0 and -0 Let's say we adjust the lattice mermaid graph A[\"\u22a4\"]---A1[+0] A[\"\u22a4\"]---A2[-0] A2---B[-] A2---C[0] A1---C[0] A1---D[+] B---E C---E D---E[\u22a5] * and the abstract operators -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + + \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) >== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) It does not help, as it might give t a more precise abtract value but it does not help to improve the result of x The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition x>=0 and the path of going out of the while loop is only valid under the condition x < 0 .","title":"Recall that sign analysis"},{"location":"notes/advanced_static_analysis/#path-sensitive-analysis-via-assertion","text":"Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of PA1 in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop. // SIMP2 x = input; while x >= 0 { assert x >= 0; x = x - 1; } assert x < 0; y = Math.sqrt(x); return y; As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions // PA2 // s0 = [ x -> top, t -> top, input -> top ] 1: x <- input // s1 = s0[ x -> s0(input) ] 2: t <- x >= 0 // s2 = lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ] 3: ifn t goto 7 // s3 = s2 4: assert x >= 0 // s4 = s3[ x -> gte(s3(x), 0) ] 5: x <- x - 1 // s5 = s4[ x -> s4(x) -- + ] 6: goto 2 // s6 = s5 7: assert x < 0 // s7 = s3[ x -> lt(s3(x), 0) ] 8: y <- Math.sqrt(x) // s8 = s7 9: r_ret <- y 10: ret We could add the following monotonic function synthesis case case \\(l: assert\\ t\\ >=\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\) case \\(l: assert\\ t\\ <\\ src\\) , \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\) Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs. gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) To show that the above definitions of gte and lt are sound. We can consider the range notation of the abstract values. $$ \\begin{array}{rcl} \\top & = & [-\\infty, +\\infty] \\ +0 & = & [0, +\\infty] \\ -0 & = & [-\\infty, 0] \\ \\ + & = & [1, +\\infty] \\ \\ - & = & [-\\infty, -1] \\ 0 & = & [0, 0] \\ \\bot & = & [+\\infty, -\\infty] \\end{array} $$ \\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\) . \\(\\bot\\) is an empty range. We can think of gte as \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] Similiarly we can think of lt as \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] where \\(+\\infty - 1 = +\\infty\\) With the adjusted monotonic equations, we can now define the monotonic function f2 as follows f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( [ x -> top, t -> top, input -> top ] s0[ x -> s0(input) ], lub(s1,s6) [ t -> lub(s1,s6)(x) >== 0 ], s2, s3[ x -> gte(s3(x), 0) ], s4[ x -> s4(x) -- + ], s5, s3[ x -> lt(s3(x), 0) ], s7 ) By applying the fixed point algorithm to f2 we find the following solution s0 = [ x -> top, t -> top, input -> top ] s1 = [ x -> top, t -> top, input -> top ] s2 = [ x -> top, t -> +0, input -> top ] s3 = [ x -> top, t -> +0, input -> top ] s4 = [ x -> +0, t -> +0, input -> top ] s5 = [ x -> top, t -> +0, input -> top ] s6 = [ x -> -, t -> +0, input -> top] s7 = [ x -> -, t -> +0, input -> top] which detects that the sign of x at instruction 8 is - .","title":"Path sensitive analysis via assertion"},{"location":"notes/advanced_static_analysis/#information-flow-analysis","text":"One widely applicable static analysis is information flow analysis. The information flow in a program describes how data are evaluated and propogated in the program via variables and operations. The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds. Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection. High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure.","title":"Information Flow Analysis"},{"location":"notes/advanced_static_analysis/#tainted-flow","text":"IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id; try { Statement statement = dbconnection.createStatement(); ResultSet res = statement.executeQuery( query ); // access sensitive resource } The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. When the id is \"' OR 'a'='a'; delete from customer_data; --\" , the malicious user gains the login access and deletes all records from the customer_data table. This can be prevented by using a prepared statement. String id = request.getParameter(\"id\"); // untrusted user input String query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\"; try { PreparedStatement pstmt = connection.prepareStatement( query ); pstmt.setString(1, id); // pstmt is sanitized before being used. ResultSet results = pstmt.executeQuery(); } One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple Let's recast the above into SIMP, we would have the vulunerable code as id = input(); query = \"select \" + id; exec(query); return id; We assume that we've extended SIMP to support string values and string concatenation. The input is a function that prompts the user for input. The exec function is a database builtin function. The following version fixed the vulnerability, assume the sanitize function, sanitizes the input. id = input(); query = \"select \"; query = sanitize(query, id) exec(query); return id; To increase the level of compexlity, let's add some control flow to the example. id = input(); query = \"select \" + id; while (id == \"\") { id = input(); query = sanitize(\"select \", id) } exec(query); return id; In the above, it is not directly clear that the exec() is given a sanitized query. The manual check becomes exponentially hard as the code size grows. We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. graph tainted --- clean --- bot(\"\u22a5\") We rewrite the above SIMP program into the following PA equivalent. 1: id <- input() 2: query <- \"select \" + id 3: b <- id == \"\" 4: ifn b goto 5: id <- input() 6: query <- sanitize(\"select\", id) 7: goto 3 8: _ <- exec(query) 9: r_ret <- id 10: ret We define the equation generation rules as follows, \\[ join(s_i) = \\bigsqcup pred(s_i) \\] where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\) case \\(l: t \\leftarrow input()\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\) case \\(l: t \\leftarrow sanitize(src_1, src_2)\\) : \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & clean \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] We inline the equations as comments in the PA code. // s0 = [id -> bot, query -> bot, b -> bot] 1: id <- input() // s1 = s0[ id -> tainted ] 2: query <- \"select \" + id // s2 = s1[ query -> lub(clean, s1(id)) ] 3: b <- id == \"\" // s3 = lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ] 4: ifn b goto 8 // s4 = s3 5: id <- input() // s5 = s4[ id -> tainted ] 6: query <- sanitize(\"select\", id) // s6 = s5[ query -> clean ] 7: goto 3 // s7 = s6 8: exec(query) // s8 = s4 9: r_ret <- id // s9 = s8 10: ret By applying the above we have the followning monotonic function. f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = ( [id -> bot, query -> bot, b -> bot], s0[ id -> tainted], s1[ query -> lub(clean, s1(id)) ], lub(s2,s7)[ b -> lub( lub(s2,s7)(id), clean) ], s3, s4[ id -> tainted ], s5[ query -> clean ], s6, s4, s8 ) By applying the fixed point algorithm, we find the following solution for the monotonic equations. s0 = [id -> bot, query -> bot, b -> bot] s1 = [id -> tainted, query -> bot, b -> bot] s2 = [id -> tainted, query -> tainted, b -> bot] s3 = [id -> tainted, query -> tainted, b -> tainted ] s4 = [id -> tainted, query -> tainted, b -> tainted ] s5 = [id -> tainted, query -> tainted, b -> tainted ] s6 = [id -> tainted, query -> clean, b -> tainted ] s7 = [id -> tainted, query -> clean, b -> tainted ] s8 = [id -> tainted, query -> tainted, b -> tainted ] s9 = [id -> tainted, query -> tainted, b -> tainted ] which says that the use of variable query at instruction 8 is risky as query may be tainted at this point.","title":"Tainted Flow"},{"location":"notes/advanced_static_analysis/#sensitive-information-disclosure","text":"In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); pstmt.setString(1, username); pstmt.setString(2, password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table user is having a lower security level, since it is accessed by the database users. String username = request.getParameter(\"username\"); String password = request.getParameter(\"password\"); // sensitive user input String cmd = \"INSERT INTO user VALUES (?, ?)\"; try { PreparedStatement pstmt = connection.prepareStatement( cmd ); String hashed_password = hash(password); // the hash serves as a declassification operation. pstmt.setString(1, username); pstmt.setString(2, hashed_password); // user password is saved without hashing? ResultSet results = pstmt.execute(); } In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password. The hash function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except, We will have a different lattice for security level. graph secret --- confidential --- bot(\"\u22a5\") where secret has a higher level of security than confidential . Instead of using sanitize to increase the level of security, we use hash (or declassify ) to lower the level of security.","title":"Sensitive Information Disclosure"},{"location":"notes/code_generation/","text":"50.054 - Code Generation Learning Outcomes Name the difference among the target code platforms Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly Handle register spilling Implement the target code generation to JVM bytecode given a Pseudo Assembly Program Recap Compiler Pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C For Target Code Generation, we consider some IR as input, the target code (executable) as the output. Instruction Selection Instruction selection is a process of choosing the target platform on which the language to be executed. There are mainly 3 kinds of target platforms. 3-address instruction RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly 2-address instruction CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86 1-address instruction Stack machine. E.g. JVM Assembly code vs Machine code Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format. 3-address instruction In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction. For instance in 3 address instruction, we have instructions that look like x <- 1 y <- 2 r <- x + y where r , x and y are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, load x 1 load y 2 add r x y The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.) 2-address instruction In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add x and y and store the result in r , we have to write load x 1 load y 2 add x y in the 3rd instruction we add the values stored in registers x and y . The sum will be stored in x . In the last statement, we move the result from x to r . As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex. 1-address instruction In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. For example for the same program, we need t9o encode it in 1-address instruction as follows push 1 push 2 add store r In the first instruction, we push the constant 1 to the left operand register (or the 1st register). In the second instruction, we push the constant 2 to the right oeprand register (the 2nd register). In the 3rd instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2nd register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable r The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex. From PA to 3-address target platform In this section, we consider generating code for a target platform that using 3-address instruciton. Register Allocation Problem Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register rret . Such an assumption is no longer valid in the code generation phase. We face two major constraints. Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation. The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation. For example, the following PA program // PA1 1: x <- inpput 2: y <- x + 1 3: z <- y + 1 4: w <- y * z 5: rret <- w 6: ret has to be translated into 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r3 <- r1 * r2 5: rret <- r3 6: ret assuming we have 4 other registers r0 , r1 , r2 and r3 , besides rret . We can map the PA variables {x : r0, y : r1, z : r2, w : r3} When we only have 3 other registers excluding rret we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: x <- r0 5: r0 <- r1 * r2 6: rret <- r0 7: ret The above program will work within the hardware constraint (3 extra registers besides rret ). Now the register allocation, {x : r0, y : r1, z : r2} is only valid for instructions 1-4 and the alloction for instructions 5-7 is {w : r0, y : r1, z: r2} . As we can argue, we could avoid the offloading by mapping w to rret since it is the one being retured. 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: rret <- r1 * r2 5: ret However this option is not always possible, as the following the w might not be returned variable in some other examples. We could also avoid the offloading by exploiting the liveness analysis, that x is not live from instruction 3 onwards, hence we should not even save the result of r0 to the temporary variable x . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r0 <- r1 * r2 5: rret <- r0 6: ret However this option is not always possible, as in some other situation x is needed later. The Register Allocation Problem is then define as follows. Given a program \\(p\\) , and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized. Interference Graph To solve the register allocation problem, we define a data structure called the interference graph. Two temporary variables are interferring each other when they are both \"live\" at the same time in a program. In the following we include the liveness analysis result as the comments in the program PA1 . // PA1 1: x <- inpput // {input} 2: y <- x + 1 // {x} 3: z <- y + 1 // {y} 4: w <- y * z // {y,z} 5: rret <- w // {w} 6: ret // {} We conclude that y and z are interfering each other. Hence they should not be sharing the same register. graph TD; input x y --- z w From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the rret register. For example we annotate the graph with the mapped registers r0 and r1 graph TD; input[\"input(r0)\"] x[\"x(r0)\"] y[\"y(r0)\"] --- z[\"z(r1)\"] w[\"w(r0)\"] And we can generate the following output 1: r0 <- inpput 2: r0 <- r0 + 1 3: r1 <- r0 + 1 4: r0 <- r0 * r1 5: rret <- r0 6: ret Graph Coloring Problem From the above example, we find that we can recast the register allocation problem into a graph coloring problem. The graph coloring problem is defined as follows. Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. Unfortunately, this problem is NP-complete in general. No efficient algorithm is known. Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists. Chordal Graph A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n > 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle. For example, the following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 v2 --- v4 is chordal, because of \\((v_2,v_4)\\) . The following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 is not chordal, or chordless . It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time. An Example Consider the following PA program with the variable liveness result as comments // PA2 1: a <- 0 // {} 2: b <- 1 // {a} 3: c <- a + b // {a, b} 4: d <- b + c // {b, c} 5: a <- c + d // {c, d} 6: e <- 2 // {a} 7: d <- a + e // {a, e} 8: r_ret <- e + d // {e, d} 9: ret We observe the interference graph graph TD a --- b --- c --- d a --- e --- d and find that it is chordless. SSA saves the day! With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph. For example, if we apply SSA conversion to PA2 We have the following // PA_SSA2 1: a1 <- 0 // {} 2: b1 <- 1 // {a1} 3: c1 <- a1 + b1 // {a1, b1} 4: d1 <- b1 + c1 // {b1, c1} 5: a2 <- c1 + d1 // {c1, d1} 6: e1 <- 2 // {a2} 7: d2 <- a2 + e1 // {a2, e1} 8: r_ret <- e1 + d2 // {e1, d2} 9: ret The liveness analysis algorithm can be adapted to SSA with the following adjustment. We define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j) \\] where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\) . \\[ \\begin{array}{rcl} \\Theta_{i,j} & = & \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] The monotonic functions can be defined by the following cases. case \\(l: \\overline{\\phi}\\ ret\\) , \\(s_l = \\{\\}\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) Now the interference graph of the PA_SSA2 is as follows graph TD; a1 --- b1 --- c1 --- d1 a2 --- e1 --- d2 which is chordal. Coloring Interference Graph generated from SSA According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order. In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.) Therefore we can color the above graph as follows, graph TD; a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\") a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\") From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form. Given that the program interference graph is chordal, the register allocation can be computed in polymomial type. Instead of using building the interference graph, we consider using the live range table of an SSA program, In the following table (of PA_SSA2 ), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An * in a cell (x, l) represent variable x is live at program location l . var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 At any point, (any column), the number of * denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is 2 (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling. Register Spilling However register spilling is avoidable due to program complexity and limit of hardware. Let's consider another example // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x} 3: z <- x * x // {x,y} 4: w <- y * x // {x,y,z} 5: u <- z + w // {z,w} 6: r_ret <- u // {u} 7: ret // {} The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis. var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * From the live range table able, we find that at peak i.e. instruction 4 , there are 3 live variables currently. We would need three registers for the allocation. What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction 4 , by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here. Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point. Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live. // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x(3)} 3: z <- x * x // {x(3),y(4)} 4: w <- y * x // {x(4),y(4),z(5)} 5: u <- z + w // {z(5),w(5)} 6: r_ret <- u // {u(6)} 7: ret // {} From the above results, we can conclude that at instruction 4 , we should sacrifice the live variable z , because z is marked live at label 5 which is needed in the instruction one-hop away in the CFG, compared to x and y which are marked live at label 4 . In other words, z is not as urgently needed compared to x and y . var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label 3 , variable is z is some register, either r0 or r1 , assuming in the target code operation * can use the same register for both operands and the result. We encounter another problem. To spill z (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 3: ?? <- r0 * r0 // what register should hold the result of x * x, before spilling it to `z`? where the comments indicate what happens after the label instruction is excuted. There are two option here ?? is r1 . It implies that we need to spill r1 to y first after instruction 2 and then spill r1 to z after instruction 3 , and load y back to r1 after instruction 3 before instruction 4. ?? is r0 . It implies that we need to spill r0 to z first after instruction 2 and then spill r0 to z after instruction 3 , and load x back to r0 after instruction 3 before instruction 4. In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling ( z in this example) is not needed until then. Now let's say we pick the first option, the register allocation continues var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 where - indicates taht z is being spilled from r1 before label 4 and it needs to be loaded back to r1 before label 5 . And the complete code of PA3_REG is as follows // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 y <- r1 // temporarily save y 3: r1 <- r0 * r0 // z is r1 z <- r1 // spill to z r1 <- y // y is r1 4: r0 <- r1 * r0 // w is r0 (x,y are dead afterwards) r1 <- z // z is r1 5: r1 <- r1 + r0 // u is r1 (z,w are dead afterwards) 6: r_ret <- r1 7: ret In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case). As an exercise, work out what if we save x temporarily instead of y at label 2 . Register allocation for phi assignments What remains to address is the treatment of the phi assignments. Let's consider a slightly bigger example. // PA4 1: x <- input // {input} 2: s <- 0 // {x} 3: c <- 0 // {s,x} 4: b <- c < x // {c,s,x} 5: ifn b goto 9 // {b,c,s,x} 6: s <- c + s // {c,s,x} 7: c <- c + 1 // {c,s,x} 8: goto 4 // {c,s,x} 9: r_ret <- s // {s} 10: ret // {} In the above we find a sum program with liveness analysis results included as comments. Let's convert it into SSA. // PA_SSA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(4),x1(4)} 4: c2 <- phi(3:c1, 8:c3) s2 <- phi(3:s1, 8:s3) b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(4),x1(4)} 8: goto 4 // {c3(4),s3(4),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} We put the liveness analysis results as comments. There are a few options of handling phi assignments. Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code Ensure the variables in the phi assignments sharing the same registers. Let's consider the first approach Conservative approach When we translate the SSA back to PA // PA_SSA_PA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(3.1),x1(4)} 3.1: c2 <- c1 s2 <- s1 // {s1(3.1),x1(4),c1(3.1)} 4: b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(7.1),x1(4)} 7.1: c2 <- c3 s2 <- s3 // {s3(7.1),x1(4),c3(7.1)} 8: goto 4 // {c2(4),s2(6,9),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers. var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 At the peak of the live variables, i.e. instruction 5 , we realize that x1 is live but not urgently needed until 4 which is 5-hop away from the current location. Hence we spill it from register r1 to the temporary variable to free up r1 . Registers are allocated by the next available in round-robin manner. // PA4_REG1 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r2 <- r0 + r2 // s3 is r2 7: r0 <- r0 + 1 // c3 is r0 // c2 is r0 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- r2 // 10: ret // What if at instruction 7 , we allocate r1 to s3 instead of r2 ? Thanks to some indeterminism, we could have a slightly different register allocation as follows var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 // PA4_REG2 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r1 <- r0 + r2 // s3 is r1 7: r2 <- r0 + 1 // c3 is r2 7.1: r0 <- r2 // c2 is r0 r2 <- r1 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- s2 10: ret In this case we have to introduce some additional register shuffling at 7.1 . Compared to PA4_REG1 , this result is less efficient. Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. What we could construct the live range table as follow. var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 Although from the above we find c1 seems to be always dead, but it is not, because its value is merged into c2 in label 4 . This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level. We also take note we want to c1 and c3 to share the same register, and s1 and s3 to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach PA4_REG1 . Note that this approach is not guanranteed to produce more efficient results than the conversvative approach. Summary so far To sum up the code generation process from PA to 3-address target could be carried out as follows, Convert the PA program into a SSA. Perform Liveness Analysis on the SSA. Generate the live range table based on the liveness analysis results. Allocate registers based on the live range table. Detect potential spilling. Depending on the last approach, either convert SSA back to PA and generate the target code according to the live range table, or generate the target code from SSA with register coalesced for the phi assignment operands. Further Reading for SSA-based Register Allocation https://compilers.cs.uni-saarland.de/papers/ssara.pdf https://dl.acm.org/doi/10.1145/512529.512534 JVM bytecode (reduced set) In this section, we consider the generated JVM codes from PA. \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) & jis & ::= & [] \\mid ji\\ jis\\\\ (\\tt JVM\\ Instruction) & ji & ::= & ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\ & & & \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) & n & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt constant) & c & ::= & -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767 \\end{array} \\] As mentioned, JVM has 3 registers a register for the first operand and result a register for the second operand a register for controlling the state of the stack operation (we can't used.) Technically speaking we only have 2 registers. An Example of JVM byte codes is illustrated as follows Supposed we have a PA program as follows, 1: x <- input 2: s <- 0 3: c <- 0 4: b <- c < x 5: ifn b goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: _ret_r <- s 10: ret For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as input to 1 , x to 2 , s to 3 , c to 4 (and b to 5 , though b is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. 4 to l1 and 9 to l2 . iload 1 // push the content of input to register 0 istore 2 // pop register 0's content to x, sipush 0 // push the value 0 to register 0 istore 3 // pop register 0 to s sipush 0 // push the value 0 to register 0 istore 4 // pop register 0 to c ilabel l1 // mark label l1 iload 4 // push the content of c to register 0 iload 2 // push the content of x to register 1 if_icmpge l2 // if register 0 >= register 1 jump, // regardless of the comparison pop both registers iload 4 // push the content of c to register 0 iload 3 // push the content of s to register 1 iadd // sum up the r0 and r1 and result remains in register 0 istore 3 // pop register 0 to s iload 4 // push the content of c to register 0 sipush 1 // push a constant 1 to register 1 iadd istore 4 // pop register 0 to c igoto l1 ilabel l2 iload 3 // push the content of s to register 0 ireturn JVM bytecode operational semantics To describe the operational semantics of JVM bytecodes, we define the following meta symbols. \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) & J & \\subseteq & jis \\\\ (\\tt JVM\\ Environment) & \\Delta & \\subseteq & n \\times c \\\\ (\\tt JVM\\ Stack) & S & = & \\_,\\_ \\mid c,\\_ \\mid c,c \\end{array} \\] An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom ( \\(r_0\\) ) and the right slot is the top ( \\(r_1\\) ). \\(\\_\\) denotes that a slot is vacant. We can decribe the operational semantics of JVM byte codes using the follow rule form \\[ J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] \\(J\\) is the entire program, it is required when we process jumps and conditional jump, the rule rewrites a configuration \\((L\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\) , where \\(\\Delta\\) and \\(\\Delta'\\) are the local environments, \\(S\\) and \\(S'\\) are the stacks, \\(jis\\) and \\(jis'\\) are the currrent and next set of instructions to be processed. $$ \\begin{array}{rc} (\\tt sjLoad1) & J \\vdash (\\Delta, _, _, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), _, jis) \\ \\ (\\tt sjLoad2) & J \\vdash (\\Delta, c, _, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\ \\ (\\tt sjPush1) & J \\vdash (\\Delta, _, _, sipush\\ c;jis) \\longrightarrow (\\Delta, c, _, jis) \\ \\ (\\tt sjPush2) & J \\vdash (\\Delta, c_0, _, sipush\\ c_2;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} $$ The rules \\((\\tt sjLoad1)\\) and \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers. The rules \\((\\tt sjPush1)\\) and \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. \\[ \\begin{array}{rc} (\\tt sjLabel) & J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation. \\[ \\begin{array}{rc} (\\tt sjStore) & J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} (\\tt sjAdd) & J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\ (\\tt sjSub) & J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\ (\\tt sjMul) & J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis) \\end{array} \\] The rules \\((\\tt sjAdd)\\) , \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty. \\[ \\begin{array}{rc} (\\tt sjGoto) & J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\ (\\tt sjCmpNE1) & \\begin{array}{c} c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) & \\begin{array}{c} c_0 = c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ (\\tt sjCmpGE1) & \\begin{array}{c} c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) & \\begin{array}{c} c_0 \\lt c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ \\end{array} \\] The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\) . Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) & = & error \\\\ codeAfterLabel(ilabel\\ l;jis, l') & = & \\left \\{ \\begin{array}{lc} jis & l == l' \\\\ codeAfterLabel(jis, l') & {\\tt otherwise} \\end{array} \\right . \\\\ codeAfterLabel(ji; jis, l) & = & codeAfterLabel(jis, l) \\end{array} \\] The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\) . The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 \\lt c_1\\) . Conversion from PA to JVM bytecodes A simple conversion from PA to JVM bytecodes can be described using the following deduction system. Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels. We have three types of rules. \\(M, L \\vdash lis \\Rightarrow jis\\) , convert a sequence of PA labeled isntructions to a sequence of JVM bytecode instructions. \\(M \\vdash s \\Rightarrow jis\\) , convert a PA operand into a sequence of JVM bytecode instructions. \\(L \\vdash l \\Rightarrow jis\\) , convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton. Converting PA labeled instructions $$ \\begin{array}{rl} {\\tt (jMove)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\ \\hline M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2 \\end{array} \\ \\end{array} $$ The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection. \\[ \\begin{array}{rl} {\\tt (jEq)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3 \\end{array} \\\\ \\\\ {\\tt (jLThan)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 < s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions. \\[ \\begin{array}{rl} {\\tt (jAdd)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jSub)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jMul)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jAdd)\\) , \\((\\tt jSub)\\) and \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM. \\[ \\begin{array}{rl} {\\tt (jGoto)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M,L \\vdash lis \\Rightarrow jis_1 \\\\ \\hline M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jReturn)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\ \\hline M, L \\vdash l_1:rret \\leftarrow s; l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn] \\end{array} \\\\ \\end{array} \\] The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial. Converting PA Operands \\[ \\begin{array}{rl} {\\tt (jConst)} & M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\ {\\tt (jVar)} & M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\ \\end{array} \\] Converting PA Labels \\[ \\begin{array}{rl} {\\tt (jLabel1)} & \\begin{array}{c} l \\not \\in L \\\\ \\hline L \\vdash l \\Rightarrow [] \\end{array} \\\\ \\\\ {\\tt (jLabel2)} & \\begin{array}{c} l \\in L \\\\ \\hline L \\vdash l \\Rightarrow [ilabel\\ l] \\end{array} \\end{array} \\] Optimizing JVM bytecode Though it is limited, there is room to opimize the JVM bytecode. For example, From the following SIMP program r = (1 + 2) * 3 we generate the following PA code via the Maximal Munch 1: t <- 1 + 2 2: r <- t * 3 In turn if we apply the above PA to JVM bytecode conversion sipush 1 sipush 2 iadd istore 2 // 2 is t iload 2 sipush 3 imul istore 3 // 3 is r As observe, the istore 2 followed by iload 2 are rundandant, because t is not needed later (dead). sipush 1 sipush 2 iadd sipush 3 imul istore 3 // 3 is r This can either be done via Liveness analysis on PA level or Generate JVM byte code directly from SIMP. This requires the expression of SIMP assignment to be left nested. The conversion is beyond the scope of this module. Further Reading for JVM bytecode generation https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf Summary for JVM bytecode generation To generate JVM bytecode w/o optimization can be done via deduction system To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.","title":"50.054 - Code Generation"},{"location":"notes/code_generation/#50054-code-generation","text":"","title":"50.054 - Code Generation"},{"location":"notes/code_generation/#learning-outcomes","text":"Name the difference among the target code platforms Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly Handle register spilling Implement the target code generation to JVM bytecode given a Pseudo Assembly Program","title":"Learning Outcomes"},{"location":"notes/code_generation/#recap-compiler-pipeline","text":"graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C For Target Code Generation, we consider some IR as input, the target code (executable) as the output.","title":"Recap Compiler Pipeline"},{"location":"notes/code_generation/#instruction-selection","text":"Instruction selection is a process of choosing the target platform on which the language to be executed. There are mainly 3 kinds of target platforms. 3-address instruction RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly 2-address instruction CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86 1-address instruction Stack machine. E.g. JVM","title":"Instruction Selection"},{"location":"notes/code_generation/#assembly-code-vs-machine-code","text":"Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format.","title":"Assembly code vs Machine code"},{"location":"notes/code_generation/#3-address-instruction","text":"In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction. For instance in 3 address instruction, we have instructions that look like x <- 1 y <- 2 r <- x + y where r , x and y are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, load x 1 load y 2 add r x y The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.)","title":"3-address instruction"},{"location":"notes/code_generation/#2-address-instruction","text":"In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add x and y and store the result in r , we have to write load x 1 load y 2 add x y in the 3rd instruction we add the values stored in registers x and y . The sum will be stored in x . In the last statement, we move the result from x to r . As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex.","title":"2-address instruction"},{"location":"notes/code_generation/#1-address-instruction","text":"In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. For example for the same program, we need t9o encode it in 1-address instruction as follows push 1 push 2 add store r In the first instruction, we push the constant 1 to the left operand register (or the 1st register). In the second instruction, we push the constant 2 to the right oeprand register (the 2nd register). In the 3rd instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2nd register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable r The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex.","title":"1-address instruction"},{"location":"notes/code_generation/#from-pa-to-3-address-target-platform","text":"In this section, we consider generating code for a target platform that using 3-address instruciton.","title":"From PA to 3-address target platform"},{"location":"notes/code_generation/#register-allocation-problem","text":"Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register rret . Such an assumption is no longer valid in the code generation phase. We face two major constraints. Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation. The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation. For example, the following PA program // PA1 1: x <- inpput 2: y <- x + 1 3: z <- y + 1 4: w <- y * z 5: rret <- w 6: ret has to be translated into 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r3 <- r1 * r2 5: rret <- r3 6: ret assuming we have 4 other registers r0 , r1 , r2 and r3 , besides rret . We can map the PA variables {x : r0, y : r1, z : r2, w : r3} When we only have 3 other registers excluding rret we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: x <- r0 5: r0 <- r1 * r2 6: rret <- r0 7: ret The above program will work within the hardware constraint (3 extra registers besides rret ). Now the register allocation, {x : r0, y : r1, z : r2} is only valid for instructions 1-4 and the alloction for instructions 5-7 is {w : r0, y : r1, z: r2} . As we can argue, we could avoid the offloading by mapping w to rret since it is the one being retured. 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: rret <- r1 * r2 5: ret However this option is not always possible, as the following the w might not be returned variable in some other examples. We could also avoid the offloading by exploiting the liveness analysis, that x is not live from instruction 3 onwards, hence we should not even save the result of r0 to the temporary variable x . 1: r0 <- input 2: r1 <- r0 + 1 3: r2 <- r1 + 1 4: r0 <- r1 * r2 5: rret <- r0 6: ret However this option is not always possible, as in some other situation x is needed later. The Register Allocation Problem is then define as follows. Given a program \\(p\\) , and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized.","title":"Register Allocation Problem"},{"location":"notes/code_generation/#interference-graph","text":"To solve the register allocation problem, we define a data structure called the interference graph. Two temporary variables are interferring each other when they are both \"live\" at the same time in a program. In the following we include the liveness analysis result as the comments in the program PA1 . // PA1 1: x <- inpput // {input} 2: y <- x + 1 // {x} 3: z <- y + 1 // {y} 4: w <- y * z // {y,z} 5: rret <- w // {w} 6: ret // {} We conclude that y and z are interfering each other. Hence they should not be sharing the same register. graph TD; input x y --- z w From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the rret register. For example we annotate the graph with the mapped registers r0 and r1 graph TD; input[\"input(r0)\"] x[\"x(r0)\"] y[\"y(r0)\"] --- z[\"z(r1)\"] w[\"w(r0)\"] And we can generate the following output 1: r0 <- inpput 2: r0 <- r0 + 1 3: r1 <- r0 + 1 4: r0 <- r0 * r1 5: rret <- r0 6: ret","title":"Interference Graph"},{"location":"notes/code_generation/#graph-coloring-problem","text":"From the above example, we find that we can recast the register allocation problem into a graph coloring problem. The graph coloring problem is defined as follows. Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. Unfortunately, this problem is NP-complete in general. No efficient algorithm is known. Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists.","title":"Graph Coloring Problem"},{"location":"notes/code_generation/#chordal-graph","text":"A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n > 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle. For example, the following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 v2 --- v4 is chordal, because of \\((v_2,v_4)\\) . The following graph graph TD v1 --- v2 --- v3 --- v4 --- v1 is not chordal, or chordless . It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time.","title":"Chordal Graph"},{"location":"notes/code_generation/#an-example","text":"Consider the following PA program with the variable liveness result as comments // PA2 1: a <- 0 // {} 2: b <- 1 // {a} 3: c <- a + b // {a, b} 4: d <- b + c // {b, c} 5: a <- c + d // {c, d} 6: e <- 2 // {a} 7: d <- a + e // {a, e} 8: r_ret <- e + d // {e, d} 9: ret We observe the interference graph graph TD a --- b --- c --- d a --- e --- d and find that it is chordless.","title":"An Example"},{"location":"notes/code_generation/#ssa-saves-the-day","text":"With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph. For example, if we apply SSA conversion to PA2 We have the following // PA_SSA2 1: a1 <- 0 // {} 2: b1 <- 1 // {a1} 3: c1 <- a1 + b1 // {a1, b1} 4: d1 <- b1 + c1 // {b1, c1} 5: a2 <- c1 + d1 // {c1, d1} 6: e1 <- 2 // {a2} 7: d2 <- a2 + e1 // {a2, e1} 8: r_ret <- e1 + d2 // {e1, d2} 9: ret The liveness analysis algorithm can be adapted to SSA with the following adjustment. We define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j) \\] where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\) . \\[ \\begin{array}{rcl} \\Theta_{i,j} & = & \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] The monotonic functions can be defined by the following cases. case \\(l: \\overline{\\phi}\\ ret\\) , \\(s_l = \\{\\}\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) Now the interference graph of the PA_SSA2 is as follows graph TD; a1 --- b1 --- c1 --- d1 a2 --- e1 --- d2 which is chordal.","title":"SSA saves the day!"},{"location":"notes/code_generation/#coloring-interference-graph-generated-from-ssa","text":"According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order. In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.) Therefore we can color the above graph as follows, graph TD; a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\") a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\") From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form. Given that the program interference graph is chordal, the register allocation can be computed in polymomial type. Instead of using building the interference graph, we consider using the live range table of an SSA program, In the following table (of PA_SSA2 ), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An * in a cell (x, l) represent variable x is live at program location l . var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 At any point, (any column), the number of * denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is 2 (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling.","title":"Coloring Interference Graph generated from SSA"},{"location":"notes/code_generation/#register-spilling","text":"However register spilling is avoidable due to program complexity and limit of hardware. Let's consider another example // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x} 3: z <- x * x // {x,y} 4: w <- y * x // {x,y,z} 5: u <- z + w // {z,w} 6: r_ret <- u // {u} 7: ret // {} The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis. var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * From the live range table able, we find that at peak i.e. instruction 4 , there are 3 live variables currently. We would need three registers for the allocation. What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction 4 , by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here. Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point. Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live. // PA3 1: x <- 1 // {} 2: y <- x + 1 // {x(3)} 3: z <- x * x // {x(3),y(4)} 4: w <- y * x // {x(4),y(4),z(5)} 5: u <- z + w // {z(5),w(5)} 6: r_ret <- u // {u(6)} 7: ret // {} From the above results, we can conclude that at instruction 4 , we should sacrifice the live variable z , because z is marked live at label 5 which is needed in the instruction one-hop away in the CFG, compared to x and y which are marked live at label 4 . In other words, z is not as urgently needed compared to x and y . var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label 3 , variable is z is some register, either r0 or r1 , assuming in the target code operation * can use the same register for both operands and the result. We encounter another problem. To spill z (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 3: ?? <- r0 * r0 // what register should hold the result of x * x, before spilling it to `z`? where the comments indicate what happens after the label instruction is excuted. There are two option here ?? is r1 . It implies that we need to spill r1 to y first after instruction 2 and then spill r1 to z after instruction 3 , and load y back to r1 after instruction 3 before instruction 4. ?? is r0 . It implies that we need to spill r0 to z first after instruction 2 and then spill r0 to z after instruction 3 , and load x back to r0 after instruction 3 before instruction 4. In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling ( z in this example) is not needed until then. Now let's say we pick the first option, the register allocation continues var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 where - indicates taht z is being spilled from r1 before label 4 and it needs to be loaded back to r1 before label 5 . And the complete code of PA3_REG is as follows // PA3_REG 1: r0 <- 1 // x is r0 2: r1 <- r0 + 1 // y is r1 y <- r1 // temporarily save y 3: r1 <- r0 * r0 // z is r1 z <- r1 // spill to z r1 <- y // y is r1 4: r0 <- r1 * r0 // w is r0 (x,y are dead afterwards) r1 <- z // z is r1 5: r1 <- r1 + r0 // u is r1 (z,w are dead afterwards) 6: r_ret <- r1 7: ret In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case). As an exercise, work out what if we save x temporarily instead of y at label 2 .","title":"Register Spilling"},{"location":"notes/code_generation/#register-allocation-for-phi-assignments","text":"What remains to address is the treatment of the phi assignments. Let's consider a slightly bigger example. // PA4 1: x <- input // {input} 2: s <- 0 // {x} 3: c <- 0 // {s,x} 4: b <- c < x // {c,s,x} 5: ifn b goto 9 // {b,c,s,x} 6: s <- c + s // {c,s,x} 7: c <- c + 1 // {c,s,x} 8: goto 4 // {c,s,x} 9: r_ret <- s // {s} 10: ret // {} In the above we find a sum program with liveness analysis results included as comments. Let's convert it into SSA. // PA_SSA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(4),x1(4)} 4: c2 <- phi(3:c1, 8:c3) s2 <- phi(3:s1, 8:s3) b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(4),x1(4)} 8: goto 4 // {c3(4),s3(4),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} We put the liveness analysis results as comments. There are a few options of handling phi assignments. Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code Ensure the variables in the phi assignments sharing the same registers. Let's consider the first approach","title":"Register allocation for phi assignments"},{"location":"notes/code_generation/#conservative-approach","text":"When we translate the SSA back to PA // PA_SSA_PA4 1: x1 <- input1 // {input1(1)} 2: s1 <- 0 // {x1(4)} 3: c1 <- 0 // {s1(3.1),x1(4)} 3.1: c2 <- c1 s2 <- s1 // {s1(3.1),x1(4),c1(3.1)} 4: b1 <- c2 < x1 // {c2(4),s2(6,9),x1(4)} 5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)} 6: s3 <- c2 + s2 // {c2(6),s2(6),x1(4)} 7: c3 <- c2 + 1 // {c2(7),s3(7.1),x1(4)} 7.1: c2 <- c3 s2 <- s3 // {s3(7.1),x1(4),c3(7.1)} 8: goto 4 // {c2(4),s2(6,9),x1(4)} 9: r_ret <- s2 // {s2(9)} 10: ret // {} It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers. var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 At the peak of the live variables, i.e. instruction 5 , we realize that x1 is live but not urgently needed until 4 which is 5-hop away from the current location. Hence we spill it from register r1 to the temporary variable to free up r1 . Registers are allocated by the next available in round-robin manner. // PA4_REG1 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r2 <- r0 + r2 // s3 is r2 7: r0 <- r0 + 1 // c3 is r0 // c2 is r0 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- r2 // 10: ret // What if at instruction 7 , we allocate r1 to s3 instead of r2 ? Thanks to some indeterminism, we could have a slightly different register allocation as follows var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 // PA4_REG2 1: r0 <- input1 // input is r0 r1 <- r0 // x1 is r1 2: r2 <- 0 // s1 is r2 3: r0 <- 0 // c1 is r0 // c2 is r0 // s2 is r2 // no need to load r1 from x1 // b/c x1 is still active in r1 // from 3 to 4 4: x1 <- r1 // spill r1 to x1 r1 <- r0 < r1 // b1 is r1 5: ifn r1 goto 9 // 6: r1 <- r0 + r2 // s3 is r1 7: r2 <- r0 + 1 // c3 is r2 7.1: r0 <- r2 // c2 is r0 r2 <- r1 // s2 is r2 8: r1 <- x1 // restore r1 from x1 goto 4 // b/c x1 is inactive but needed in 4 9: r_ret <- s2 10: ret In this case we have to introduce some additional register shuffling at 7.1 . Compared to PA4_REG1 , this result is less efficient.","title":"Conservative approach"},{"location":"notes/code_generation/#register-coalesced-approach-ensure-the-variables-in-the-phi-assignments-sharing-the-same-registers","text":"Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. What we could construct the live range table as follow. var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 Although from the above we find c1 seems to be always dead, but it is not, because its value is merged into c2 in label 4 . This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level. We also take note we want to c1 and c3 to share the same register, and s1 and s3 to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach PA4_REG1 . Note that this approach is not guanranteed to produce more efficient results than the conversvative approach.","title":"Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers"},{"location":"notes/code_generation/#summary-so-far","text":"To sum up the code generation process from PA to 3-address target could be carried out as follows, Convert the PA program into a SSA. Perform Liveness Analysis on the SSA. Generate the live range table based on the liveness analysis results. Allocate registers based on the live range table. Detect potential spilling. Depending on the last approach, either convert SSA back to PA and generate the target code according to the live range table, or generate the target code from SSA with register coalesced for the phi assignment operands.","title":"Summary so far"},{"location":"notes/code_generation/#further-reading-for-ssa-based-register-allocation","text":"https://compilers.cs.uni-saarland.de/papers/ssara.pdf https://dl.acm.org/doi/10.1145/512529.512534","title":"Further Reading for SSA-based Register Allocation"},{"location":"notes/code_generation/#jvm-bytecode-reduced-set","text":"In this section, we consider the generated JVM codes from PA. \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) & jis & ::= & [] \\mid ji\\ jis\\\\ (\\tt JVM\\ Instruction) & ji & ::= & ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\ & & & \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) & n & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt constant) & c & ::= & -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767 \\end{array} \\] As mentioned, JVM has 3 registers a register for the first operand and result a register for the second operand a register for controlling the state of the stack operation (we can't used.) Technically speaking we only have 2 registers. An Example of JVM byte codes is illustrated as follows Supposed we have a PA program as follows, 1: x <- input 2: s <- 0 3: c <- 0 4: b <- c < x 5: ifn b goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: _ret_r <- s 10: ret For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as input to 1 , x to 2 , s to 3 , c to 4 (and b to 5 , though b is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. 4 to l1 and 9 to l2 . iload 1 // push the content of input to register 0 istore 2 // pop register 0's content to x, sipush 0 // push the value 0 to register 0 istore 3 // pop register 0 to s sipush 0 // push the value 0 to register 0 istore 4 // pop register 0 to c ilabel l1 // mark label l1 iload 4 // push the content of c to register 0 iload 2 // push the content of x to register 1 if_icmpge l2 // if register 0 >= register 1 jump, // regardless of the comparison pop both registers iload 4 // push the content of c to register 0 iload 3 // push the content of s to register 1 iadd // sum up the r0 and r1 and result remains in register 0 istore 3 // pop register 0 to s iload 4 // push the content of c to register 0 sipush 1 // push a constant 1 to register 1 iadd istore 4 // pop register 0 to c igoto l1 ilabel l2 iload 3 // push the content of s to register 0 ireturn","title":"JVM bytecode (reduced set)"},{"location":"notes/code_generation/#jvm-bytecode-operational-semantics","text":"To describe the operational semantics of JVM bytecodes, we define the following meta symbols. \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) & J & \\subseteq & jis \\\\ (\\tt JVM\\ Environment) & \\Delta & \\subseteq & n \\times c \\\\ (\\tt JVM\\ Stack) & S & = & \\_,\\_ \\mid c,\\_ \\mid c,c \\end{array} \\] An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom ( \\(r_0\\) ) and the right slot is the top ( \\(r_1\\) ). \\(\\_\\) denotes that a slot is vacant. We can decribe the operational semantics of JVM byte codes using the follow rule form \\[ J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] \\(J\\) is the entire program, it is required when we process jumps and conditional jump, the rule rewrites a configuration \\((L\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\) , where \\(\\Delta\\) and \\(\\Delta'\\) are the local environments, \\(S\\) and \\(S'\\) are the stacks, \\(jis\\) and \\(jis'\\) are the currrent and next set of instructions to be processed. $$ \\begin{array}{rc} (\\tt sjLoad1) & J \\vdash (\\Delta, _, _, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), _, jis) \\ \\ (\\tt sjLoad2) & J \\vdash (\\Delta, c, _, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\ \\ (\\tt sjPush1) & J \\vdash (\\Delta, _, _, sipush\\ c;jis) \\longrightarrow (\\Delta, c, _, jis) \\ \\ (\\tt sjPush2) & J \\vdash (\\Delta, c_0, _, sipush\\ c_2;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} $$ The rules \\((\\tt sjLoad1)\\) and \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers. The rules \\((\\tt sjPush1)\\) and \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. \\[ \\begin{array}{rc} (\\tt sjLabel) & J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation. \\[ \\begin{array}{rc} (\\tt sjStore) & J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\ \\end{array} \\] The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} (\\tt sjAdd) & J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\ (\\tt sjSub) & J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\ (\\tt sjMul) & J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis) \\end{array} \\] The rules \\((\\tt sjAdd)\\) , \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty. \\[ \\begin{array}{rc} (\\tt sjGoto) & J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\ (\\tt sjCmpNE1) & \\begin{array}{c} c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) & \\begin{array}{c} c_0 = c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ (\\tt sjCmpGE1) & \\begin{array}{c} c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l') \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis') \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) & \\begin{array}{c} c_0 \\lt c_1 \\\\ \\hline J \\vdash (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis) \\end{array} \\\\ \\\\ \\end{array} \\] The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\) . Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) & = & error \\\\ codeAfterLabel(ilabel\\ l;jis, l') & = & \\left \\{ \\begin{array}{lc} jis & l == l' \\\\ codeAfterLabel(jis, l') & {\\tt otherwise} \\end{array} \\right . \\\\ codeAfterLabel(ji; jis, l) & = & codeAfterLabel(jis, l) \\end{array} \\] The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\) . The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 \\lt c_1\\) .","title":"JVM bytecode operational semantics"},{"location":"notes/code_generation/#conversion-from-pa-to-jvm-bytecodes","text":"A simple conversion from PA to JVM bytecodes can be described using the following deduction system. Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels. We have three types of rules. \\(M, L \\vdash lis \\Rightarrow jis\\) , convert a sequence of PA labeled isntructions to a sequence of JVM bytecode instructions. \\(M \\vdash s \\Rightarrow jis\\) , convert a PA operand into a sequence of JVM bytecode instructions. \\(L \\vdash l \\Rightarrow jis\\) , convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton.","title":"Conversion from PA to JVM bytecodes"},{"location":"notes/code_generation/#converting-pa-labeled-instructions","text":"$$ \\begin{array}{rl} {\\tt (jMove)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\ \\hline M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2 \\end{array} \\ \\end{array} $$ The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection. \\[ \\begin{array}{rl} {\\tt (jEq)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3 \\end{array} \\\\ \\\\ {\\tt (jLThan)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l_1:t \\leftarrow s_1 < s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions. \\[ \\begin{array}{rl} {\\tt (jAdd)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jSub)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jMul)} & \\begin{array}{c} L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\ \\hline M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3 \\end{array} \\\\ \\end{array} \\] The rules \\((\\tt jAdd)\\) , \\((\\tt jSub)\\) and \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM. \\[ \\begin{array}{rl} {\\tt (jGoto)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M,L \\vdash lis \\Rightarrow jis_1 \\\\ \\hline M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1 \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (jReturn)} & \\begin{array}{c} L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\ \\hline M, L \\vdash l_1:rret \\leftarrow s; l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn] \\end{array} \\\\ \\end{array} \\] The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial.","title":"Converting PA labeled instructions"},{"location":"notes/code_generation/#converting-pa-operands","text":"\\[ \\begin{array}{rl} {\\tt (jConst)} & M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\ {\\tt (jVar)} & M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\ \\end{array} \\]","title":"Converting PA Operands"},{"location":"notes/code_generation/#converting-pa-labels","text":"\\[ \\begin{array}{rl} {\\tt (jLabel1)} & \\begin{array}{c} l \\not \\in L \\\\ \\hline L \\vdash l \\Rightarrow [] \\end{array} \\\\ \\\\ {\\tt (jLabel2)} & \\begin{array}{c} l \\in L \\\\ \\hline L \\vdash l \\Rightarrow [ilabel\\ l] \\end{array} \\end{array} \\]","title":"Converting PA Labels"},{"location":"notes/code_generation/#optimizing-jvm-bytecode","text":"Though it is limited, there is room to opimize the JVM bytecode. For example, From the following SIMP program r = (1 + 2) * 3 we generate the following PA code via the Maximal Munch 1: t <- 1 + 2 2: r <- t * 3 In turn if we apply the above PA to JVM bytecode conversion sipush 1 sipush 2 iadd istore 2 // 2 is t iload 2 sipush 3 imul istore 3 // 3 is r As observe, the istore 2 followed by iload 2 are rundandant, because t is not needed later (dead). sipush 1 sipush 2 iadd sipush 3 imul istore 3 // 3 is r This can either be done via Liveness analysis on PA level or Generate JVM byte code directly from SIMP. This requires the expression of SIMP assignment to be left nested. The conversion is beyond the scope of this module.","title":"Optimizing JVM bytecode"},{"location":"notes/code_generation/#further-reading-for-jvm-bytecode-generation","text":"https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf","title":"Further Reading for JVM bytecode generation"},{"location":"notes/code_generation/#summary-for-jvm-bytecode-generation","text":"To generate JVM bytecode w/o optimization can be done via deduction system To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.","title":"Summary for JVM bytecode generation"},{"location":"notes/dynamic_semantics/","text":"50.054 - Dynamic Semantics Learning Outcomes Explain the small step operational semantics of a programming language. Explain the big step operational semantics of a programming language. Formalize the run-time behavior of a programming language using small step operational semantics. Formalize the run-time behavior of a programming language using big step operational semantics. Recall that by formalizing the dynamic semantics of a program we are keen to find out How does the program get executed? What does the program compute / return? Operational Semantics Operational Semantics specifies how a program get executed. For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\) -reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\) . The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule. As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed. To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases. Small-Step Operational Semantics of SIMP Let's try to formalize the Operational Semantics of SIMP language, $$ \\begin{array}{rccl} (\\tt SIMP\\ Environment) & \\Delta & \\subseteq & (X \\times c) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\) , i.e. \\(\\{ X \\mid (X,c) \\in \\Delta \\}\\) . We assume for all \\(X \\in dom(\\Delta)\\) , there exists only one entry of \\((X,c) \\in \\Delta\\) . Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\) , an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\) . We define the operational semantics of SIMP with two sets of rules. The first set of rules deal with expression. Small Step Operational Semantics of SIMP Expression The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\) . \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) $$ The ${\\tt (sVar)}$ rule looks up the value of variable $X$ from the memory environment. If the variable is not found, it gets stuck and an error is returned. $$ \\begin{array}{rc} {\\tt (sOp1)} & \\begin{array}{c} \\Delta \\vdash E_1 \\longrightarrow E_1' \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} & \\begin{array}{c} \\Delta \\vdash E_2 \\longrightarrow E_2' \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} & \\begin{array}{c} C_3 = C_1 \\ OP\\ C_2 \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3 \\end{array} \\end{array} \\] The above three rules handle the binary operation expression. \\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step. \\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step. \\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values. \\[ \\begin{array}{rc} {\\tt (sParen1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline \\Delta \\vdash (E) \\longrightarrow (E') \\end{array} \\\\ \\\\ {\\tt (sParen2)} & \\begin{array}{c} \\Delta \\vdash (c) \\longrightarrow c \\end{array} \\end{array} \\] The rules \\({\\tt (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses. Small Step Operational Semantics of SIMP statement The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . The pair of a environment and a statement is called a program configuration. \\[ \\begin{array}{cc} {\\tt (sAssign1)} & \\begin{array}{c} \\Delta\\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, X = E;) \\longrightarrow (\\Delta, X = E';) \\end{array} \\end{array} \\] $$ \\begin{array}{cc} {\\tt (sAssign2)} & \\begin{array}{c} \\Delta' = \\Delta \\oplus (X, C) \\ \\hline (\\Delta, X = C;) \\longrightarrow (\\Delta', nop) \\end{array} \\end{array} $$ The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements. \\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step. \\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\) . The statement of the resulting configuration a \\(nop\\) . \\[ \\begin{array}{cc} {\\tt (sIf1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} & (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} & (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] The rules \\({\\tt (sIf1)}\\) , \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement. \\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step. \\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\) , it proceeds to evaluate the statements in the then clauses. \\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\) , it proceeds to evaluate the statements in the else clauses. \\[ \\begin{array}{cc} {\\tt (sWhile)} & (\\Delta, while\\ E\\ \\{\\overline{S}\\} ) \\longrightarrow (\\Delta, if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement. In the then branch, we unroll the while loop body once followed by the while loop. In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used. \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} & \\begin{array}{c} S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S') \\\\ \\hline (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S}) \\end{array} \\end{array} \\] The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements. \\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\) . \\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\) . It evalues \\(S\\) by one step. For example, {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # using (sSeq) {(input,1)}, x = input ---> # (sAssign1) {(input,1)}, x = 1 ---> # (sAssign2) {(input, 1), (x,1)}, nop ---> {(input,1), (x,1)}, nop; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sNopSeq) {(input,1), (x,1)}, s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,0)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,0)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,0)}, 0 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,0)}, 0 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,0)}, true ---> {(input,1), (x,1), (s,0), (c,0)}, if true { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf2) {(input,1), (x,1), (s,0), (c,0)}, s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, s = c + s ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + s ---> # (sOp1) 0 + s ---> # (sOp2) 0 + 0 ---> # (sOp3) 0 {(input,1), (x,1), (s,0), (c,0)}, s = 0 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,0)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1 ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + 1 ---> # (sOp1) 0 + 1 ---> # (SOp3) 1 {(input,1), (x,1), (s,0), (c,1)}, c = 1 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,1)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,1)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,1)}, 1 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,1)}, 1 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,1)}, false ---> {(input,1), (x,1), (s,0), (c,1)}, if false { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf3) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)} return s; At last the derivation stop at the return statement. We can return the value 0 as result. Big Step Operational Semantics Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program. Big Step Operational Semantics for SIMP expressions We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\) , which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\) . We consider the following three rules $$ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C $$ In case that the expression is a constant, we return the constant itself. \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] In case that the expression is a variable \\(X\\) , we return the value associated with \\(X\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} {\\tt (bOp)} & \\begin{array}{c} \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~ C_1\\ OP\\ C_2 = C_3 \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3 \\end{array} \\end{array} \\] in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values. \\[ \\begin{array}{rc} {\\tt (bParen)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\\\ \\hline \\Delta \\vdash (E) \\Downarrow C \\end{array} \\end{array} \\] the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses. Big Step Operational Semantics for SIMP statements We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\) , which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\) . Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations. We consider the following rules $$ \\begin{array}{rc} {\\tt (bAssign)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\ \\hline (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C) \\end{array} \\end{array} $$ In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment. \\[ \\begin{array}{rc} {\\tt (bIf1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false ~~~~~~ (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2 \\end{array} \\end{array} \\] In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\) , otherwise evaluate \\(\\overline{S_2}\\) . \\[ \\begin{array}{rc} {\\tt (bWhile1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta \\end{array} \\end{array} \\] In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\) , otherwise, we exit the while loop and return the existing memory environment. \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] In case that the statement is a nop statement, there is no change to the memory environment. \\[ \\begin{array}{rc} {\\tt (bSeq)} & \\begin{array}{c} (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta'' \\\\ \\hline (\\Delta, S \\overline{S}) \\Downarrow \\Delta'' \\end{array} \\end{array} \\] In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements. For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively. {(input,1),(x,1)} |- 0 \u21d3 0 (bConst) ---------------------(bAssign) [sub tree 1] {(input,1), (x,1)}, s = 0; {(input,1)} |- input \u21d3 1 (bVar) \u21d3 {(input,1), (x,1), (s,0)} ---------------- (bAssign) -------------------------------------------(bSeq) {(input,1)}, {(input,1), (x,1)}, x = input; s = 0; \u21d3 {(input,1), (x,1)} c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} ---------------------------------------------------------------------------- (bSeq) {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where sub derivation [sub tree 1] is as follows {(input,1), (x,1), (s,0)} [sub tree 2] -------------------- (bReturn) |- 0 \u21d3 0 (bConst) {(input,1), (x,1), (s,0), (c,1)}, return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} --------------------------(bAssign) -------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, {(input,1), (x,1), (s,0), (c,0)}, c = 0; while c < x {s = c + s; c = c + 1;} \u21d3 return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} {(input,1),(x,1),(s,0),(c,0)} ---------------------------------------------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 2] is {(input,1), (x,1), (s,0), (c,0)} |- c \u21d3 0 (bVar) {(input,1), (x,1), (s,0), (c,0)} |- x \u21d3 1 (bVar) 0 < 1 == true [sub tree 3] [sub tree 4] -------------------------------- (bOp) ---------------------------------- (bSeq) {(input,1), (x,1), (s,0), (c,0)} {(input,1), (x,1), (s,0), (c,0)}, |- c < x \u21d3 true s = c + s; c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} ----------------------------------------------------------------------- (bWhile1) {(input,1), (x,1), (s,0), (c,0)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 3] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- s \u21d3 0 (bVar) c + s == 0 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + s \u21d3 0 ------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, s = c + s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 0)} where [sub tree 4] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- 1 \u21d3 1 (bConst) c + 1 == 1 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + 1 \u21d3 1 -------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} [sub tree 5] --------------------------------------------------------------------- (bSeq) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where [sub tree 5] is {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- c \u21d3 1 {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- x \u21d3 1 1 < 1 == false ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 1)} |- c < x \u21d3 false ---------------------------------------------------- (bWhile2) {(input, 1), (x, 1), (s, 0), (c, 1)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} Quick Summary: Small step vs Big Step operational semantics Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating Formal Results We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\) . Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP) Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\) . Proof of this lemma requires some knowledge which will be discussed in the upcoming classes. Operational Semantics of Pseudo Assembly Next we consider the operational semantics of pseudo assembly. Let's define the environments required for the rules. \\[ \\begin{array}{rccl} (\\tt PA\\ Program) & P & \\subseteq & (l \\times li) \\\\ (\\tt PA\\ Environment) & L & \\subseteq & (t \\times c) \\cup (r \\times c) \\end{array} \\] We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values. Small Step Operational Semantics of Pseudo Assembly The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) , which reads, given a PA program \\(P\\) , the current program context \\((L,li)\\) is evaluated to \\((L', li')\\) . Note that we use a memory environment and program label instruction pair to denote a program context. \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) $$ In the ${\\tt (pConst)}$ rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as $c$ and move on to the next instruction. $$ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1)) \\] $${\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) $$ In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction. \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1)) \\end{array} \\end{array} \\] The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment. \\[ \\begin{array}{rc} {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l')) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1)) \\end{array} \\end{array} \\] The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction. \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction. Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error. For example, let \\(P\\) be 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret and \\(input = 1\\) . We have the following derivation P |- {(input,1)}, 1: x <- input ---> # (pTempVar) P |- {(input,1), (x,1)}, 2: s <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0)}, 3: c <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0), (c,0)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---> # (pIfn0) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s <- c + s ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c <- c + 1 ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---> # (pGoto) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---> # (pIfnNot0) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret <- s ---> # (pTempVar) P |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret Formal Results Definition: Consistency of the memory environments Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\) ), iff \\(\\forall (x,v) \\in \\Delta\\) , \\((x,conv(v)) \\in L\\) , and \\(\\forall (y,u) \\in L\\) , \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\) . Lemma: Correctness of the Maximal Munch Algorithm Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\) . Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\) . Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\) Proof Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time. What about big step operational semantics of Pseudo Assembly? As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se. If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.) https://link.springer.com/article/10.1007/BF00264536 Denotational Semantics (Optional Materials) Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit. In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain. Syntactic Domains In many cases, the syntactic domains are defined by the grammar rules. For SIMP program, we have the following syntactic domains. \\(S\\) denotes the domain of all valid single statement \\(E\\) denotes the domain of all valid expressions \\(\\overline{S}\\) denotes the domain of all valid sequence statements \\(OP\\) denotes the domain of all valid operators. \\(C\\) denotes the domain of all constant values. \\(X\\) denotes the domain of all variables. Semantic Domains \\(Int\\) denotes the set of all integers values \\(Bool\\) denotes the set of \\(\\{true, false\\}\\) Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\) . Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\) . Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\) . Denotational Semantics for SIMP expressions The denotational semantics for the SIMP expression is defined by the following semantic functions. Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\) \\[ \\begin{array}{lll} {\\mathbb E}\\llbracket \\cdot \\rrbracket\\ :\\ E &\\rightarrow&\\ \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}\\llbracket X \\rrbracket & = & \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}\\llbracket c \\rrbracket & = & \\lambda\\sigma. c \\\\ {\\mathbb E}\\llbracket E_1\\ OP\\ E_2 \\rrbracket & = &\\lambda\\sigma. {\\mathbb E}\\llbracket E_1\\rrbracket\\sigma\\ \\llbracket OP \\rrbracket\\ {\\mathbb E}\\llbracket E_2\\rrbracket\\sigma\\\\\\ \\end{array} \\] The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value. Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\(\\llbracket + \\rrbracket\\) gives us the sum operation among two integers. Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}\\llbracket E\\rrbracket\\sigma\\) is the short hand for \\(({\\mathbb E}\\llbracket E\\rrbracket)(\\sigma)\\) As we observe, \\({\\mathbb E}\\llbracket \\cdot \\rrbracket\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\) , returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains. Denotational Semantics for SIMP statements To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs. Let \\(\\bot\\) be a special element, called undefined , that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define $$ \\begin{array}{rcl} f \\circ_\\bot g & = & \\lambda \\sigma. \\left [ \\begin{array}{cc} \\bot & g(\\sigma) = \\bot \\ f(g(\\sigma)) & otherwise \\end{array} \\right . \\end{array} $$ which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements. $$ \\begin{array}{lll} {\\mathbb S}\\llbracket \\cdot \\rrbracket : \\overline{S} & \\rightarrow\\ & \\Sigma \\ \\rightarrow \\ \\Sigma \\cup { \\bot } \\ {\\mathbb S} \\llbracket nop \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket return\\ X \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket X = E \\rrbracket& = & \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}\\llbracket E \\rrbracket\\sigma) \\ {\\mathbb S} \\llbracket S \\overline{S} \\rrbracket& = & {\\mathbb S} \\llbracket \\overline{S} \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket S \\rrbracket\\ {\\mathbb S} \\llbracket if \\ E\\ {\\overline{S_1}} \\ else\\ {\\overline{S_2} } \\rrbracket& = & \\lambda\\sigma. \\left [ \\begin{array}{cc} {\\mathbb S} \\llbracket \\overline{S_1} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ {\\mathbb S} \\llbracket \\overline{S_2} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & fix(F) \\ {\\tt where}\\ & & F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence. In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\) . In case of sequence statements, the semantic function returns a \\(\\bot\\) -function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have $$ \\begin{array}{lll} {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & \\lambda\\sigma. \\left { \\begin{array}{cc} ({\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}}\\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} \\llbracket while \\ E\\ \\{\\overline{S}\\} \\rrbracket\\) For example, let \\(\\sigma = \\{ (input, 1)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket x=input; s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma \\\\ = & ({\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket x=input \\rrbracket) (\\sigma) \\\\ = & {\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_1 \\\\ = & ({\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket s=0 \\rrbracket) (\\sigma_1) \\\\ = & {\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_2 \\\\ = & ({\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket c=0 \\rrbracket) (\\sigma_2) \\\\ = & {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_3 \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket \\circ_\\bot{\\mathbb S} \\llbracket s = c + s; c = c + 1; \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket)(\\sigma_4) \\\\ = & {\\mathbb S} \\llbracket return\\ s; \\rrbracket\\sigma_4 \\\\ = & \\sigma_4 \\end{array} \\] where $$ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = { (input,1), (x,1) } \\ \\sigma_2 = \\sigma_1 \\oplus (s,0) = { (input,1), (x,1), (s,0) } \\ \\sigma_3 = \\sigma_2 \\oplus (c,0) = { (input,1), (x,1), (s,0), (c,0) }\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1) = { (input,1), (x,1), (s,0), (c,1) }\\ \\end{array} $$ Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket while\\ input \\{nop;\\}return\\ input; \\rrbracket \\sigma \\\\ = & fix(F) \\sigma \\\\ = & \\bot \\end{array} \\] where \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = true \\\\ \\sigma & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = false \\\\ \\end{array} \\right . \\\\ \\end{array} \\] Since \\({\\mathbb E}\\llbracket input \\rrbracket\\sigma\\) is always \\(true\\) , \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) \\] With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\) . We won't be able to discuss the proof until we look into lattice theory in the upcoming classes. In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function. Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency. Extra readings for denotational semantics https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf https://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf","title":"50.054 - Dynamic Semantics"},{"location":"notes/dynamic_semantics/#50054-dynamic-semantics","text":"","title":"50.054 - Dynamic Semantics"},{"location":"notes/dynamic_semantics/#learning-outcomes","text":"Explain the small step operational semantics of a programming language. Explain the big step operational semantics of a programming language. Formalize the run-time behavior of a programming language using small step operational semantics. Formalize the run-time behavior of a programming language using big step operational semantics. Recall that by formalizing the dynamic semantics of a program we are keen to find out How does the program get executed? What does the program compute / return?","title":"Learning Outcomes"},{"location":"notes/dynamic_semantics/#operational-semantics","text":"Operational Semantics specifies how a program get executed. For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\) -reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\) . The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule. As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed. To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases.","title":"Operational Semantics"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp","text":"Let's try to formalize the Operational Semantics of SIMP language, $$ \\begin{array}{rccl} (\\tt SIMP\\ Environment) & \\Delta & \\subseteq & (X \\times c) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\) , i.e. \\(\\{ X \\mid (X,c) \\in \\Delta \\}\\) . We assume for all \\(X \\in dom(\\Delta)\\) , there exists only one entry of \\((X,c) \\in \\Delta\\) . Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\) , an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\) . We define the operational semantics of SIMP with two sets of rules. The first set of rules deal with expression.","title":"Small-Step Operational Semantics of SIMP"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp-expression","text":"The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\) . \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) $$ The ${\\tt (sVar)}$ rule looks up the value of variable $X$ from the memory environment. If the variable is not found, it gets stuck and an error is returned. $$ \\begin{array}{rc} {\\tt (sOp1)} & \\begin{array}{c} \\Delta \\vdash E_1 \\longrightarrow E_1' \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} & \\begin{array}{c} \\Delta \\vdash E_2 \\longrightarrow E_2' \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} & \\begin{array}{c} C_3 = C_1 \\ OP\\ C_2 \\\\ \\hline \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3 \\end{array} \\end{array} \\] The above three rules handle the binary operation expression. \\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step. \\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step. \\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values. \\[ \\begin{array}{rc} {\\tt (sParen1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline \\Delta \\vdash (E) \\longrightarrow (E') \\end{array} \\\\ \\\\ {\\tt (sParen2)} & \\begin{array}{c} \\Delta \\vdash (c) \\longrightarrow c \\end{array} \\end{array} \\] The rules \\({\\tt (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses.","title":"Small Step Operational Semantics of SIMP Expression"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-simp-statement","text":"The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . The pair of a environment and a statement is called a program configuration. \\[ \\begin{array}{cc} {\\tt (sAssign1)} & \\begin{array}{c} \\Delta\\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, X = E;) \\longrightarrow (\\Delta, X = E';) \\end{array} \\end{array} \\] $$ \\begin{array}{cc} {\\tt (sAssign2)} & \\begin{array}{c} \\Delta' = \\Delta \\oplus (X, C) \\ \\hline (\\Delta, X = C;) \\longrightarrow (\\Delta', nop) \\end{array} \\end{array} $$ The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements. \\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step. \\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\) . The statement of the resulting configuration a \\(nop\\) . \\[ \\begin{array}{cc} {\\tt (sIf1)} & \\begin{array}{c} \\Delta \\vdash E \\longrightarrow E' \\\\ \\hline (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} & (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} & (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}) \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] The rules \\({\\tt (sIf1)}\\) , \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement. \\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step. \\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\) , it proceeds to evaluate the statements in the then clauses. \\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\) , it proceeds to evaluate the statements in the else clauses. \\[ \\begin{array}{cc} {\\tt (sWhile)} & (\\Delta, while\\ E\\ \\{\\overline{S}\\} ) \\longrightarrow (\\Delta, if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement. In the then branch, we unroll the while loop body once followed by the while loop. In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used. \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} & \\begin{array}{c} S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S') \\\\ \\hline (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S}) \\end{array} \\end{array} \\] The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements. \\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\) . \\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\) . It evalues \\(S\\) by one step. For example, {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # using (sSeq) {(input,1)}, x = input ---> # (sAssign1) {(input,1)}, x = 1 ---> # (sAssign2) {(input, 1), (x,1)}, nop ---> {(input,1), (x,1)}, nop; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sNopSeq) {(input,1), (x,1)}, s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq), (sAssign2), (sNoSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } return s; ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,0)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,0)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,0)}, 0 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,0)}, 0 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,0)}, true ---> {(input,1), (x,1), (s,0), (c,0)}, if true { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf2) {(input,1), (x,1), (s,0), (c,0)}, s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, s = c + s ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + s ---> # (sOp1) 0 + s ---> # (sOp2) 0 + 0 ---> # (sOp3) 0 {(input,1), (x,1), (s,0), (c,0)}, s = 0 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,0)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1; while c < x { s = c + s; c = c + 1; } ---> # (sSeq) {(input,1), (x,1), (s,0), (c,0)}, c = c + 1 ---> # (sAssign1) {(input,1), (x,1), (s,0), (c,0)}, c + 1 ---> # (sOp1) 0 + 1 ---> # (SOp3) 1 {(input,1), (x,1), (s,0), (c,1)}, c = 1 ---> # (sAssign2) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)}, while c < x { s = c + s; c = c + 1; } ---> # (sWhile) {(input,1), (x,1), (s,0), (c,1)}, if (c < x) { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf1) {(input,1), (x,1), (s,0), (c,1)}, c < x ---> # (sOp1) {(input,1), (x,1), (s,0), (c,1)}, 1 < x ---> # (sOp2) {(input,1), (x,1), (s,0), (c,1)}, 1 < 1 ---> # (sOp3) {(input,1), (x,1), (s,0), (c,1)}, false ---> {(input,1), (x,1), (s,0), (c,1)}, if false { s = c + s; c = c + 1; while c < x { s = c + s; c = c + 1; } } else { nop } ---> # (sIf3) {(input,1), (x,1), (s,0), (c,1)}, nop ---> # (sNopSeq) {(input,1), (x,1), (s,0), (c,1)} return s; At last the derivation stop at the return statement. We can return the value 0 as result.","title":"Small Step Operational Semantics of SIMP statement"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics","text":"Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program.","title":"Big Step Operational Semantics"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics-for-simp-expressions","text":"We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\) , which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\) . We consider the following three rules $$ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C $$ In case that the expression is a constant, we return the constant itself. \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] In case that the expression is a variable \\(X\\) , we return the value associated with \\(X\\) in \\(\\Delta\\) . \\[ \\begin{array}{rc} {\\tt (bOp)} & \\begin{array}{c} \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~ C_1\\ OP\\ C_2 = C_3 \\\\ \\hline \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3 \\end{array} \\end{array} \\] in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values. \\[ \\begin{array}{rc} {\\tt (bParen)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\\\ \\hline \\Delta \\vdash (E) \\Downarrow C \\end{array} \\end{array} \\] the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses.","title":"Big Step Operational Semantics for SIMP expressions"},{"location":"notes/dynamic_semantics/#big-step-operational-semantics-for-simp-statements","text":"We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\) , which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\) . Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations. We consider the following rules $$ \\begin{array}{rc} {\\tt (bAssign)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow C \\ \\hline (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C) \\end{array} \\end{array} $$ In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment. \\[ \\begin{array}{rc} {\\tt (bIf1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1 \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false ~~~~~~ (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2 \\\\ \\hline (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2 \\end{array} \\end{array} \\] In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\) , otherwise evaluate \\(\\overline{S_2}\\) . \\[ \\begin{array}{rc} {\\tt (bWhile1)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow true ~~~~~~ (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta' \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} & \\begin{array}{c} \\Delta \\vdash E \\Downarrow false \\\\ \\hline (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta \\end{array} \\end{array} \\] In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\) , otherwise, we exit the while loop and return the existing memory environment. \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] In case that the statement is a nop statement, there is no change to the memory environment. \\[ \\begin{array}{rc} {\\tt (bSeq)} & \\begin{array}{c} (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta'' \\\\ \\hline (\\Delta, S \\overline{S}) \\Downarrow \\Delta'' \\end{array} \\end{array} \\] In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements. For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively. {(input,1),(x,1)} |- 0 \u21d3 0 (bConst) ---------------------(bAssign) [sub tree 1] {(input,1), (x,1)}, s = 0; {(input,1)} |- input \u21d3 1 (bVar) \u21d3 {(input,1), (x,1), (s,0)} ---------------- (bAssign) -------------------------------------------(bSeq) {(input,1)}, {(input,1), (x,1)}, x = input; s = 0; \u21d3 {(input,1), (x,1)} c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} ---------------------------------------------------------------------------- (bSeq) {(input, 1)}, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where sub derivation [sub tree 1] is as follows {(input,1), (x,1), (s,0)} [sub tree 2] -------------------- (bReturn) |- 0 \u21d3 0 (bConst) {(input,1), (x,1), (s,0), (c,1)}, return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} --------------------------(bAssign) -------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, {(input,1), (x,1), (s,0), (c,0)}, c = 0; while c < x {s = c + s; c = c + 1;} \u21d3 return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} {(input,1),(x,1),(s,0),(c,0)} ---------------------------------------------------------------------------- (bSeq) {(input,1), (x,1), (s,0)}, c = 0; while c < x { s = c + s; c = c + 1; } return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 2] is {(input,1), (x,1), (s,0), (c,0)} |- c \u21d3 0 (bVar) {(input,1), (x,1), (s,0), (c,0)} |- x \u21d3 1 (bVar) 0 < 1 == true [sub tree 3] [sub tree 4] -------------------------------- (bOp) ---------------------------------- (bSeq) {(input,1), (x,1), (s,0), (c,0)} {(input,1), (x,1), (s,0), (c,0)}, |- c < x \u21d3 true s = c + s; c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} ----------------------------------------------------------------------- (bWhile1) {(input,1), (x,1), (s,0), (c,0)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)} where [sub tree 3] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- s \u21d3 0 (bVar) c + s == 0 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + s \u21d3 0 ------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, s = c + s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 0)} where [sub tree 4] is {(input, 1), (x, 1), (s, 0), (c, 0)} |- c \u21d3 0 (bVar) {(input, 1), (x, 1), (s, 0), (c, 0)} |- 1 \u21d3 1 (bConst) c + 1 == 1 ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 0)} |- c + 1 \u21d3 1 -------------------------------------- (bAssign) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} [sub tree 5] --------------------------------------------------------------------- (bSeq) {(input, 1), (x, 1), (s, 0), (c, 0)}, c = c + 1; while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)} where [sub tree 5] is {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- c \u21d3 1 {(input, 1), (x, 1), (s, 0), (c, 1)} (bVar) |- x \u21d3 1 1 < 1 == false ------------------------------------ (bOp) {(input, 1), (x, 1), (s, 0), (c, 1)} |- c < x \u21d3 false ---------------------------------------------------- (bWhile2) {(input, 1), (x, 1), (s, 0), (c, 1)}, while c < x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}","title":"Big Step Operational Semantics for SIMP statements"},{"location":"notes/dynamic_semantics/#quick-summary-small-step-vs-big-step-operational-semantics","text":"Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating","title":"Quick Summary: Small step vs Big Step operational semantics"},{"location":"notes/dynamic_semantics/#formal-results","text":"We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\) .","title":"Formal Results"},{"location":"notes/dynamic_semantics/#lemma-1-agreement-of-small-step-and-big-step-operational-semantics-of-simp","text":"Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\) . Proof of this lemma requires some knowledge which will be discussed in the upcoming classes.","title":"Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP)"},{"location":"notes/dynamic_semantics/#operational-semantics-of-pseudo-assembly","text":"Next we consider the operational semantics of pseudo assembly. Let's define the environments required for the rules. \\[ \\begin{array}{rccl} (\\tt PA\\ Program) & P & \\subseteq & (l \\times li) \\\\ (\\tt PA\\ Environment) & L & \\subseteq & (t \\times c) \\cup (r \\times c) \\end{array} \\] We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values.","title":"Operational Semantics of Pseudo Assembly"},{"location":"notes/dynamic_semantics/#small-step-operational-semantics-of-pseudo-assembly","text":"The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) , which reads, given a PA program \\(P\\) , the current program context \\((L,li)\\) is evaluated to \\((L', li')\\) . Note that we use a memory environment and program label instruction pair to denote a program context. \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) $$ In the ${\\tt (pConst)}$ rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as $c$ and move on to the next instruction. $$ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1)) \\] $${\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) $$ In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction. \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1)) \\end{array} \\end{array} \\] The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment. \\[ \\begin{array}{rc} {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l')) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1)) \\end{array} \\end{array} \\] The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction. \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction. Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error. For example, let \\(P\\) be 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret and \\(input = 1\\) . We have the following derivation P |- {(input,1)}, 1: x <- input ---> # (pTempVar) P |- {(input,1), (x,1)}, 2: s <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0)}, 3: c <- 0 ---> # (pConst) P |- {(input,1), (x,1), (s,0), (c,0)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---> # (pIfn0) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s <- c + s ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c <- c + 1 ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---> # (pGoto) P |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t <- c < x ---> # (pOp) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---> # (pIfnNot0) P |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret <- s ---> # (pTempVar) P |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret","title":"Small Step Operational Semantics of Pseudo Assembly"},{"location":"notes/dynamic_semantics/#formal-results_1","text":"","title":"Formal Results"},{"location":"notes/dynamic_semantics/#definition-consistency-of-the-memory-environments","text":"Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\) ), iff \\(\\forall (x,v) \\in \\Delta\\) , \\((x,conv(v)) \\in L\\) , and \\(\\forall (y,u) \\in L\\) , \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\) .","title":"Definition: Consistency of the memory environments"},{"location":"notes/dynamic_semantics/#lemma-correctness-of-the-maximal-munch-algorithm","text":"Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\) . Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\) . Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\) . Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\)","title":"Lemma: Correctness of the Maximal Munch Algorithm"},{"location":"notes/dynamic_semantics/#proof","text":"Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time.","title":"Proof"},{"location":"notes/dynamic_semantics/#what-about-big-step-operational-semantics-of-pseudo-assembly","text":"As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se. If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.) https://link.springer.com/article/10.1007/BF00264536","title":"What about big step operational semantics of Pseudo Assembly?"},{"location":"notes/dynamic_semantics/#denotational-semantics-optional-materials","text":"Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit. In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain.","title":"Denotational Semantics (Optional Materials)"},{"location":"notes/dynamic_semantics/#syntactic-domains","text":"In many cases, the syntactic domains are defined by the grammar rules. For SIMP program, we have the following syntactic domains. \\(S\\) denotes the domain of all valid single statement \\(E\\) denotes the domain of all valid expressions \\(\\overline{S}\\) denotes the domain of all valid sequence statements \\(OP\\) denotes the domain of all valid operators. \\(C\\) denotes the domain of all constant values. \\(X\\) denotes the domain of all variables.","title":"Syntactic Domains"},{"location":"notes/dynamic_semantics/#semantic-domains","text":"\\(Int\\) denotes the set of all integers values \\(Bool\\) denotes the set of \\(\\{true, false\\}\\) Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection. Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\) . Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\) . Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\) .","title":"Semantic Domains"},{"location":"notes/dynamic_semantics/#denotational-semantics-for-simp-expressions","text":"The denotational semantics for the SIMP expression is defined by the following semantic functions. Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\) \\[ \\begin{array}{lll} {\\mathbb E}\\llbracket \\cdot \\rrbracket\\ :\\ E &\\rightarrow&\\ \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}\\llbracket X \\rrbracket & = & \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}\\llbracket c \\rrbracket & = & \\lambda\\sigma. c \\\\ {\\mathbb E}\\llbracket E_1\\ OP\\ E_2 \\rrbracket & = &\\lambda\\sigma. {\\mathbb E}\\llbracket E_1\\rrbracket\\sigma\\ \\llbracket OP \\rrbracket\\ {\\mathbb E}\\llbracket E_2\\rrbracket\\sigma\\\\\\ \\end{array} \\] The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value. Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\(\\llbracket + \\rrbracket\\) gives us the sum operation among two integers. Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}\\llbracket E\\rrbracket\\sigma\\) is the short hand for \\(({\\mathbb E}\\llbracket E\\rrbracket)(\\sigma)\\) As we observe, \\({\\mathbb E}\\llbracket \\cdot \\rrbracket\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\) , returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains.","title":"Denotational Semantics for SIMP expressions"},{"location":"notes/dynamic_semantics/#denotational-semantics-for-simp-statements","text":"To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs. Let \\(\\bot\\) be a special element, called undefined , that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define $$ \\begin{array}{rcl} f \\circ_\\bot g & = & \\lambda \\sigma. \\left [ \\begin{array}{cc} \\bot & g(\\sigma) = \\bot \\ f(g(\\sigma)) & otherwise \\end{array} \\right . \\end{array} $$ which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements. $$ \\begin{array}{lll} {\\mathbb S}\\llbracket \\cdot \\rrbracket : \\overline{S} & \\rightarrow\\ & \\Sigma \\ \\rightarrow \\ \\Sigma \\cup { \\bot } \\ {\\mathbb S} \\llbracket nop \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket return\\ X \\rrbracket& = & \\lambda\\sigma. \\sigma \\ {\\mathbb S} \\llbracket X = E \\rrbracket& = & \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}\\llbracket E \\rrbracket\\sigma) \\ {\\mathbb S} \\llbracket S \\overline{S} \\rrbracket& = & {\\mathbb S} \\llbracket \\overline{S} \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket S \\rrbracket\\ {\\mathbb S} \\llbracket if \\ E\\ {\\overline{S_1}} \\ else\\ {\\overline{S_2} } \\rrbracket& = & \\lambda\\sigma. \\left [ \\begin{array}{cc} {\\mathbb S} \\llbracket \\overline{S_1} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ {\\mathbb S} \\llbracket \\overline{S_2} \\rrbracket\\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & fix(F) \\ {\\tt where}\\ & & F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence. In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\) . In case of sequence statements, the semantic function returns a \\(\\bot\\) -function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have $$ \\begin{array}{lll} {\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}} \\rrbracket& = & \\lambda\\sigma. \\left { \\begin{array}{cc} ({\\mathbb S} \\llbracket while \\ E\\ {\\overline{S}}\\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket \\overline{S} \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = true \\ \\sigma & {\\mathbb E}\\llbracket E \\rrbracket\\sigma = false \\ \\end{array} \\right . \\ \\end{array} $$ which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} \\llbracket while \\ E\\ \\{\\overline{S}\\} \\rrbracket\\) For example, let \\(\\sigma = \\{ (input, 1)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket x=input; s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma \\\\ = & ({\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket x=input \\rrbracket) (\\sigma) \\\\ = & {\\mathbb S} \\llbracket s = 0; c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_1 \\\\ = & ({\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket s=0 \\rrbracket) (\\sigma_1) \\\\ = & {\\mathbb S} \\llbracket c=0; while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_2 \\\\ = & ({\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket c=0 \\rrbracket) (\\sigma_2) \\\\ = & {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\}return\\ s; \\rrbracket \\sigma_3 \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket \\circ_\\bot{\\mathbb S} \\llbracket s = c + s; c = c + 1; \\rrbracket) (\\sigma_3) \\\\ = & ({\\mathbb S} \\llbracket return\\ s; \\rrbracket \\circ_\\bot {\\mathbb S} \\llbracket while\\ c < x \\{s = c + s; c = c + 1;\\} \\rrbracket)(\\sigma_4) \\\\ = & {\\mathbb S} \\llbracket return\\ s; \\rrbracket\\sigma_4 \\\\ = & \\sigma_4 \\end{array} \\] where $$ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = { (input,1), (x,1) } \\ \\sigma_2 = \\sigma_1 \\oplus (s,0) = { (input,1), (x,1), (s,0) } \\ \\sigma_3 = \\sigma_2 \\oplus (c,0) = { (input,1), (x,1), (s,0), (c,0) }\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1) = { (input,1), (x,1), (s,0), (c,1) }\\ \\end{array} $$ Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\) \\[ \\begin{array}{ll} & {\\mathbb S} \\llbracket while\\ input \\{nop;\\}return\\ input; \\rrbracket \\sigma \\\\ = & fix(F) \\sigma \\\\ = & \\bot \\end{array} \\] where \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc} (g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = true \\\\ \\sigma & {\\mathbb E}\\llbracket input \\rrbracket\\sigma = false \\\\ \\end{array} \\right . \\\\ \\end{array} \\] Since \\({\\mathbb E}\\llbracket input \\rrbracket\\sigma\\) is always \\(true\\) , \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} \\llbracket nop \\rrbracket)(\\sigma) \\] With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\) . We won't be able to discuss the proof until we look into lattice theory in the upcoming classes. In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function.","title":"Denotational Semantics for SIMP statements"},{"location":"notes/dynamic_semantics/#denotational-semantics-vs-big-step-operational-semantics-vs-small-step-semantics","text":"support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency.","title":"Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics"},{"location":"notes/dynamic_semantics/#extra-readings-for-denotational-semantics","text":"https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf https://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf","title":"Extra readings for denotational semantics"},{"location":"notes/fp_applicative_monad/","text":"50.054 - Applicative and Monad Learning Outcomes Describe and define derived type class Describe and define Applicative Functors Describe and define Monads Apply Monad to in design and develop highly modular and resusable software. Derived Type Class Recall that in our previous lesson, we talk about the Ordering type class. trait Ordering[A] { def compare(x:A,y:A):Int // less than: -1, equal: 0, greater than 1 } Let's consider a variant called Order (actually it is defined in a popular Scala library named cats ). trait Eq[A] { def eqv(x:A, y:A):Boolean } trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } In the above, the Eq type class is a supertype of the Order type class, because all instances of Order type class should have the method eqv implemented. We also say Order is a derived type class of Eq . Let's consider some instances given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt:Order[Int] = new Order[Int] { def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) An alternative approach trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int // def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt(using eqInt:Eq[Int]):Order[Int] = new Order[Int] { def eqv(x:Int,y:Int):Boolean = eqInt.eqv(x,y) def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) In the above definition, we skip the default implementatoin of eqv in Order and make use of the type class instance context to synthesis the eqv method based on the existing type class instances of Eq . (This approach is closer to the one found in Haskell.) Which one is better? Both have their own pros and cons. In the first approach, we give a default implementation for the eqv overridden method in Order type class, it frees us from re-defining the eqv in every type class instance of Order . In this case, the rule/logic is fixed at the top level. In the second approach, eqv in Order type class is not defined. We are required to define it for every single type class instance of Order , that means more work. The advantage is that we have flexibility to redefine/re-mix definition of eqv coming from other type class instances. Functor (Recap) Recall from the last lesson, we make use of the Functor type class to define generic programming style of data processing. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } enum BTree[+A]{ case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Note that we also swap the first and the second arguments of the map function. Applicative Functor The Applicative Functor is a derived type class of Functor , which is defined as follows trait Applicative[F[_]] extends Functor[F] { def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] def pure[A](a: A): F[A] def map[A, B](fa: F[A])(f: A => B): F[B] = ap(pure(f))(fa) } Note that we \"fix\" the map for Applicative in the type class level in this case. (i.e. we are following the first approach.) given listApplicative:Applicative[List] = new Applicative[List] { def pure[A](a:A):List[A] = List(a) def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.map( f => fa.map(f)).flatten } Recall that flatten flattens a list of lists. Alternatively, we can define the ap method of the Applicative[List] instance flatMap . Given l is a list, l.flatMap(f) is the same as l.map(f).flatten . def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.flatMap( f => fa.map(f)) Recall that Scala compiler desugars expression of shape e1.flatMap( v1 => e2.flatMap( v2 => ... en.map(vn => e ... ))) into for { v1 <- e1 v2 <- e2 ... vn <- en } yield (e) Hence we can rewrite the ap method of the Applicative[List] instance as def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = for { f <- ff a <- fa } yield (f(a)) It is not suprising the following produces the same results as the functor instance. listApplicative.map(l)((x:Int) => x + 1) What about pure and ap ? when can we use them? Let's consider the following contrived example. Suppose we would like to apply two sets of operations to elements from l , each operation will produce its own set of results, and the inputs do not depend on the output of the other set. i.e. If the two set of operations, are (x:Int)=> x+1 and (y:Int)=>y*2 . val intOps= List((x:Int)=>x+1, (y:Int)=>y*2) listApplicative.ap(intOps)(l) we get List(2, 3, 4, 2, 4, 6) as the result. Let's consider another example. Recall that Option[A] algebraic datatype which captures a value of type A could be potentially empty. We define the Applicative[Option] instance as follows given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff match { case None => None case Some(f) => fa match { case None => None case Some(a) => Some(f(a)) } } } In the above Applicative instance, the ap method takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning None . This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value. Recall the builtin Option type is defined as follows, // no need to run this. enum Option[+A] { case None case Some(v) def map[B](f:A=>B):Option[B] = this match { case None => None case Some(v) => Some(f(v)) } def flatMap[B](f:A=>Option[B]):Option[B] = this match { case None => None case Some(v) => f(v) match { case None => None case Some(u) => Some(u) } } } Hence optApplicative can be simplified as given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff.flatMap(f => fa.map(f)) // same as listApplicative } or given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = for { f <- ff a <- fa } yield f(a) // same as listApplicative } Applicative Laws Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable. Identity: ap(pure(x=>x)) \\(\\equiv\\) x=>x Homomorphism: ap(pure(f))(pure(x)) \\(\\equiv\\) pure(f(x)) Interchange: ap(u)(pure(y)) \\(\\equiv\\) ap(pure(f=>f(y)))(u) Composition: ap(ap(ap(pure(f=>f.compose))(u))(v))(w) \\(\\equiv\\) ap(u)(ap(v)(w)) Identity law states that applying a lifted identity function of type A=>A is same as an identity function of type F[A] => F[A] where F is the applicative functor. Homomorphism says that applying a lifted function (which has type A=>A before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result. To understand Interchange law let's consider the following equation $$ u\\ y \\equiv (\\lambda f.(f\\ y))\\ u $$ Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\) . To understand the Composition law, we consider the following equation in lambda calculus \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w & \\longrightarrow_{\\beta} \\\\ (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w & \\longrightarrow_{\\beta} \\\\ (u\\circ v)\\ w & \\longrightarrow_{\\tt composition} \\\\ u\\ (v\\ w) \\end{array} \\] The Composition Law says that the above equation remains valid when \\(u\\) , \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\) . Cohort Exercise show that any applicative functor satisfying the above laws also satisfies the Functor Laws Monad Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor. Let's consider a motivating example. Recall that in the earlier lesson, we came across the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } In which we use Option[A] to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to Either[A,B] if we want to have better error messages. Monad is a good application here. Let's consider the type class definition of Monad[F[_]] . trait Monad[F[_]] extends Applicative[F] { def bind[A,B](fa:F[A])(f:A => F[B]):F[B] def pure[A](v:A):F[A] def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] = bind(ff)((f:A=>B) => bind(fa)((a:A)=> pure(f(a)))) } given optMonad:Monad[Option] = new Monad[Option] { def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } The eval function can be re-expressed using Monad[Option] . def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1+v2)}) }) case MathExp.Minus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1-v2)}) }) case MathExp.Mult(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1*v2)}) }) case MathExp.Div(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => if (v2 == 0) {None} else {m.pure(v1/v2)}}) }) case MathExp.Const(i) => m.pure(i) } It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of for comprehension since Option has the member functions flatMap and map defined. Recall that Scala desugars for {...} yield expression into flatMap and map . Thus the above can be rewritten as def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) if (v2 !=0) } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now the readability is restored. Another advantage of coding with Monad is that its abstraction allows us to switch underlying data structure without major code change. Suppose we would like to use Either[String, A] or some other equivalent as return type of eval function to support better error message. But before that, let's consider some subclasses of the Applicative and the Monad type classes. trait ApplicativeError[F[_], E] extends Applicative[F] { def raiseError[A](e:E):F[A] } trait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] { override def raiseError[A](e:E):F[A] } type ErrMsg = String In the above, we define an extension to the Applicative type class, named ApplicativeError which expects an extra type class parameter E that denotes an error. The raiseError method takes a value of type E and returns the Applicative result. Similarly, we extend Monad type class with MonadError type class. Next we include the following type class instance to include Option as one f the MonadError functor. given optMonadError:MonadError[Option, ErrMsg] = new MonadError[Option, ErrMsg] { def raiseError[A](e:ErrMsg):Option[A] = None def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } Next, we adjust the eval function to takes in a MonadError context instead of a Monad context. In addition, we make the error signal more explicit by calling the raiseError() method from the MonadError type class context. def eval2(e:MathExp)(using m:MonadError[Option, ErrMsg]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now let's try to refactor the code to make use of Either[ErrMsg, A] as the functor instead of Option[A] . enum Either[+A, +B] { case Left(v: A) case Right(v: B) // to support for comprehension def flatMap[C>:A,D](f: B => Either[C,D]):Either[C,D] = this match { case Left(a) => Left(a) case Right(b) => f(b) } def map[D](f:B => D):Either[A,D] = this match { case Right(b) => Right(f(b)) case Left(e) => Left(e) } } In the above, we have to define flatMap and map member functions for Either type so that we could make use of the for comprehension later on. One might argue with the type signature of flatMap should be flatMap[D](f: B => Either[A,D]):Either[A,D] . The issue here is that the type variable A will appear in both co- and contra-variant positions. The top-level annotation +A is no longer true. Hence we \"relax\" the type constraint here by introducing a new type variable C which has a lower bound of A (even though we do not need to upcast the result of the Left alternative.) type EitherErr = [B] =>> Either[ErrMsg,B] In the above we define Either algebraic datatype and the type construcor EitherErr . [B] =>> Either[ErrMsg, B] denotes a type lambda, which means that EitherErr is a type constructor (or type function) that takes a type B and return an Either[ErrMsg, B] type. Next, we define the type class instance for MonadError[EitherErr, ErrMsg] given eitherErrMonad: MonadError[EitherErr, ErrMsg] = new MonadError[EitherErr, ErrMsg] { import Either.* def raiseError[B](e: ErrMsg): EitherErr[B] = Left(e) def bind[A, B]( fa: EitherErr[A] )(f: A => EitherErr[B]): EitherErr[B] = fa match { case Right(b) => f(b) case Left(s) => Left(s) } def pure[B](v: B): EitherErr[B] = Right(v) } And finally, we refactor the eval function by changing its type signature. And its body remains unchanged. def eval3(e:MathExp)(using m:MonadError[EitherErr, ErrMsg]):EitherErr[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Commonly used Monads We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads. List Monad We know that List is a Functor and an Applicative. It is not surprising that List is also a Monad. given listMonad:Monad[List] = new Monad[List] { def pure[A](v:A):List[A] = List(v) def bind[A,B](fa:List[A])(f:A => List[B]):List[B] = fa.flatMap(f) } With the above instance, we can write list processing method in for comprehension which is similar to query languages. import java.util.Date import java.util.Calendar import java.util.GregorianCalendar import java.text.SimpleDateFormat case class Staff(id:Int, dob:Date) def mkStaff(id:Int, dobStr:String):Staff = { val sdf = new SimpleDateFormat(\"yyyy-MM-dd\") val dobDate = sdf.parse(dobStr) Staff(id, dobDate) } val staffData = List( mkStaff(1, \"1076-01-02\"), mkStaff(2, \"1986-07-24\") ) def ageBelow(staff:Staff, age:Int): Boolean = staff match { case Staff(id, dob) => { val today = new Date() val calendar = new GregorianCalendar(); calendar.setTime(today) calendar.add(Calendar.YEAR, -age) val ageYearsAgo = calendar.getTime() dob.after(ageYearsAgo) } } def query(data:List[Staff]):List[Staff] = for { staff <- data // from data if ageBelow(staff, 40) // where staff.age < 40 } yield staff // select * Reader Monad Next we consider the Reader Monad. Reader Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable. For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment. case class Reader[R, A] (run: R=>A) { // we need flatMap and map for for-comprehension def flatMap[B](f:A =>Reader[R,B]):Reader[R,B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) match { case Reader(rb) => rb(r) } ) } def map[B](f:A=>B):Reader[R, B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) ) } } type ReaderM = [R] =>> [A] =>> Reader[R, A] trait ReaderMonad[R] extends Monad[ReaderM[R]] { override def pure[A](v:A):Reader[R, A] = Reader (r => v) override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa match { case Reader(ra) => Reader ( r=> f(ra(r)) match { case Reader(rb) => rb(r) } ) } def ask:Reader[R,R] = Reader( r => r) def local[A](f:R=>R)(r:Reader[R,A]):Reader[R,A] = r match { case Reader(ra) => Reader( r => { val localR = f(r) ra(localR) }) } } In the above Reader[R,A] case class defines the structure of the Reader type, where R denotes the shared information for the computation, (source for reader), A denotes the output of the computation. We would like to define Reader[R,_] as a Monad instance. To do so, we define a type-curry version of Reader , i.e. ReaderM . One crucial observation is that bind method in ReaderMonad is nearly identical to flatMap in Reader , with the arguments swapped. In fact, we can re-express bind for all Monads as the flatMap in their underlying case class. override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa.flatMap(f) The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input https://127.0.0.1/ ). For authentication we need to call the authentication server https://127.0.0.10/ temporarily. case class API(url:String) given APIReader:ReaderMonad[API] = new ReaderMonad[API] {} def get(path:String)(using pr:ReaderMonad[API]):Reader[API,Unit] = for { r <- pr.ask s <- r match { case API(url) => pr.pure(println(s\"${url}${path}\")) } } yield s def authServer(api:API):API = API(\"https://127.0.0.10/\") def test1(using pr:ReaderMonad[API]):Reader[API, Unit] = for { a <- pr.local(authServer)(get(\"auth\")) t <- get(\"time\") j <- get(\"job\") } yield (()) def runtest1():Unit = test1 match { case Reader(run) => run(API(\"https://127.0.0.1/\")) } State Monad We consider the State Monad. A State Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like Scala, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging. The following we define a State case class, which has a member computation run:S => (S,A) . case class State[S,A]( run:S=>(S,A)) { def flatMap[B](f: A => State[S,B]):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1,a) => f(a) match { case State(ssb) => ssb(s1) } } ) } def map[B](f:A => B):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1, a) => (s1, f(a)) } ) } } As suggested by the type, the computationn S=>(S,A) , takes in a state S as input and return a tuple of output, consists a new state and the result of the computation. The State Monad type class is defined as a dervied type class of Monad[StateM[S]] . type StateM = [S] =>> [A] =>> State[S,A] trait StateMonad[S] extends Monad[StateM[S]] { override def pure[A](v:A):State[S,A] = State( s=> (s,v)) override def bind[A,B]( fa:State[S,A] )( ff:A => State[S,B] ):State[S,B] = fa.flatMap(ff) def get:State[S, S] = State(s => (s,s)) def set(v:S):State[S,Unit] = State(s => (v,())) } In the pure method's default implementation, we takes a value v of type A and return a State case class oject by wrapping a lambda which takes a state s and returns back the same state s with the input value v . In the default implementation of the bind method, we take a computation fa of type State[S,A] , i.e. a stateful computation over state type S and return a result of type A . In addition, we take a function that expects input of type A and returns a stateful computation State[S,B] . We apply flatMap of fa to ff ., which can be expanded to fa.flatMap(ff) --> fa match { case State(ssa) => State ( s => { ssa(s) match { case (s1,a) => ff(a) match { case State(ssb) => ssb(s1) } } }) } In essence it \"opens\" the computation in fa to extract the run function ssa which takes a state returns result A with the output state. As the output, we construct stateful computation in which a state s is taken as input, we immediately apply s with ssa (i.e. the computation extracted from fa ) to compute the intermediate state s1 and the output a (of type A ). Next we apply ff to a which returns a Stateful computation State[S,B] . We extract the run function from this stateful copmutation, namley ssb and apply it to s1 to continue with the result of the computation. In otherwords, bind function chains up a stateful computation fa with a lambda expressoin that consumes the result from fa and continue with another stateful copmutation. The get and the set methods give us access to the state environment of type S . For instance, case class Counter(c:Int) given counterStateMonad:StateMonad[Counter] = new StateMonad[Counter] { } def incr(using csm:StateMonad[Counter]):State[Counter,Unit] = for { Counter(c) <- csm.get _ <- csm.set(Counter(c+1)) } yield () def app(using csm:StateMonad[Counter]):State[Counter, Int] = for { _ <- incr _ <- incr Counter(v) <- csm.get } yield v In the above we define the state environment as an integer counter. Monadic function incr increase the counter in the state. Monad Laws Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws. Left Identity: bind(pure(a))(f) \\(\\equiv\\) f(a) Right Identity: bind(m)(pure) \\(\\equiv\\) m Associativity: bind(bind(m)(f))(g) \\(\\equiv\\) bind(m)(x => bind(f(x))(g)) Intutively speaking, a bind operation is to extract results of type A from its first argument with type F[A] and apply f to the extracted results. Left identity law enforces that binding a lifted value to f , is the same as applying f to the unlifted value directly, because the lifting and the extraction of the bind cancel each other. Right identity law enforces that binding a lifted value to pure , is the same as the lifted value, because extracting results from m and pure cancel each other. The Associativity law enforces that binding a lifted value m to f then to g is the same as binding m to a monadic bind composition (x => bind(f(x)(g))) Summary In this lesson we have discussed the following A derived type class is a type class that extends from another one. An Applicative Functor is a sub-class of Functor, with the methods pure and ap . The four laws for Applicative Functor. A Monad Functor is a sub-class of Applicative Functor, with the method bind . The three laws of Monad Functor. A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad. Extra Materials Writer Monad The dual of the Reader Monad is the Writer Monad, which has the following definition. // inspired by https://kseo.github.io/posts/2017-01-21-writer-monad.html trait Monoid[A]{ // We omitted the super class SemiRing[A] def mempty:A def mappend:A => A => A } given listMonoid[A]:Monoid[List[A]] = new Monoid[List[A]] { def mempty:List[A] = Nil def mappend:List[A]=>List[A]=>List[A] = (l1:List[A])=>(l2:List[A]) => l1 ++ l2 } case class Writer[W,A]( run: (W,A))(using mw:Monoid[W]) { def flatMap[B](f:A => Writer[W,B]):Writer[W,B] = this match { case Writer((w,a)) => f(a) match { case Writer((w2,b)) => Writer((mw.mappend(w)(w2), b)) } } def map[B](f:A=>B):Writer[W, B] = this match { case Writer((w,a)) => Writer((w, f(a))) } } Similar to the Reader Monad, in the above we define a case class Writer , which has a member value run that returns a tuple of (W,A) . The subtle difference is that the writer memory W has to be an instance of the Monoid type class, in which mempty and mappend operations are defined. type WriterM = [W] =>> [A] =>> Writer[W,A] trait WriterMonad[W] extends Monad[WriterM[W]] { implicit def W0:Monoid[W] override def pure[A](v: A): Writer[W, A] = Writer((W0.mempty, v)) override def bind[A, B]( fa: Writer[W, A] )(f: A => Writer[W, B]): Writer[W, B] = fa match { case Writer((w, a)) => f(a) match { case Writer((w2, b)) => { Writer((W0.mappend(w)(w2), b)) } } } def tell(w: W): Writer[W, Unit] = Writer((w, ())) def pass[A](ma: Writer[W, (A, W => W)]): Writer[W, A] = ma match { case Writer((w, (a, f))) => Writer((f(w), a)) } } In the above we define WriterMonad to be a derived type class of Monad[WriterM[W]] . For a similar reason, we need to include the type class Monoid[W] to ensure that mempty and mappend are defined on W . Besides the pure and bind members, we introduce tell and pass . tell writes the given argument into the writer's memory. pass execute a given computation which returns a value of type A and a memory update function W=>W , and return a Writer whose memory is updated by applied the update function to the memory. In the following we define a simple application with logging mechanism using the Writer Monad. case class LogEntry(msg:String) given logWriterMonad:WriterMonad[List[LogEntry]] = new WriterMonad[List[LogEntry]] { override def W0:Monoid[List[LogEntry]] = new Monoid[List[LogEntry]] { override def mempty = Nil override def mappend = (x:List[LogEntry]) => (y:List[LogEntry]) => x ++ y } } def logger(m: String)(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Unit] = wm.tell(List(LogEntry(m))) def app(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Int] = for { _ <- logger(\"start\") x <- wm.pure(1 + 1) _ <- logger(s\"result is ${x}\") _ <- logger(\"done\") } yield x def runApp(): Int = app match { case Writer((w, i)) => { println(w) i } } Monad Transformer Is the following class a Monad? case class MyState[S,A]( run:S=>Option[(S,A)]) The difference between this class and the State class we've seen earlier is that the execution method run yields result of type Option[(S,A)] instead of (S,A) which means that it can potentially fail. It is ascertained that MyState is also a Monad, and it is a kind of special State Monad. case class MyState[S, A](run: S => Option[(S, A)]) { def flatMap[B](f: A => MyState[S, B]): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => f(a) match { case MyState(ssb) => ssb(s1) } } ) } def map[B](f: A => B): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => Some((s1, f(a))) } ) } } type MyStateM = [S] =>> [A] =>> MyState[S,A] trait MyStateMonad[S] extends Monad[MyStateM[S]] { override def pure[A](v:A):MyState[S,A] = MyState( s=> Some((s,v))) override def bind[A,B]( fa:MyState[S,A] )( ff:A => MyState[S,B] ):MyState[S,B] = fa.flatMap(ff) def get:MyState[S, S] = MyState(s => Some((s,s))) def set(v:S):MyState[S,Unit] = MyState(s => Some((v,()))) } Besides \"stuffing-in\" an Option type, one could use an Either type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer . We begin by parameterizing the Option functor in MyState case class StateT[S, M[_], A](run: S => M[(S, A)])(using m:Monad[M]) { def flatMap[B](f: A => StateT[S, M, B]): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1,a) => f(a) match { case StateT(ssb) => ssb(s1) } } ) ) } def map[B](f: A => B): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1, a) => m.pure((s1, f(a))) }) ) } } In the above it is largely similar to MyState class, except that we parameterize Option by a type parameter M . M[_] indicates that it is of kind *=>* . (using m:Monad[M]) further contraints M must be an instance of Monad, so that we could make use of the bind and pure from M 's Monad instance. Naturally, we can define a derived type class called StateTMonad . type StateTM = [S] =>> [M[_]] =>> [A] =>> StateT[S, M, A] trait StateTMonad[S,M[_]] extends Monad[StateTM[S][M]] { implicit def M0:Monad[M] override def pure[A](v: A): StateT[S, M, A] = StateT(s => M0.pure((s, v))) override def bind[A, B]( fa: StateT[S, M, A] )( ff: A => StateT[S, M, B] ): StateT[S, M, B] = fa.flatMap(ff) def get: StateT[S, M, S] = StateT(s => M0.pure((s, s))) def set(v: S): StateT[S, M, Unit] = StateT(s => M0.pure(v, ())) } Given that Option is a Monad, we can redefine MyStateMonad in terms of StateTMonad and optMonad . trait StateOptMonad[S] extends StateTMonad[S, Option] { override def M0 = optMonad } What about the original vanilla State Monad? We could introduce an Identity Monad. case class Identity[A](run:A) { def flatMap[B](f:A=>Identity[B]):Identity[B] = this match { case Identity(a) => f(a) } def map[B](f:A=>B):Identity[B] = this match { case Identity(a) => Identity(f(a)) } } given identityMonad:Monad[Identity] = new Monad[Identity] { override def pure[A](v:A):Identity[A] = Identity(v) override def bind[A,B](fa:Identity[A])(f: A => Identity[B]):Identity[B] = fa.flatMap(f) } Then we can re-define the vanilla State Monad as follows, (in fact like many existing Monad libraries out there.) trait StateIdentMonad[S] extends StateTMonad[S, Identity] { // same as StateMonad override def M0 = identityMonad } One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes. Similarly we could generalize the Reader Monad to its transformer variant. case class ReaderT[R, M[_], A](run: R => M[A])(using m:Monad[M]) { def flatMap[B](f: A => ReaderT[R, M, B]):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => f(a) match { case ReaderT(rb) => rb(r) })) } def map[B](f: A => B):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => m.pure(f(a)))) } } type ReaderTM = [R] =>>[M[_]] =>> [A] =>> ReaderT[R, M, A] trait ReaderTMonad[R,M[_]] extends Monad[ReaderTM[R][M]] { implicit def M0:Monad[M] override def pure[A](v: A): ReaderT[R, M, A] = ReaderT(r => M0.pure(v)) override def bind[A, B]( fa: ReaderT[R, M, A] )(f: A => ReaderT[R, M, B]): ReaderT[R, M, B] = fa.flatMap(f) def ask: ReaderT[R, M, R] = ReaderT(r => M0.pure(r)) def local[A](f: R => R)(r: ReaderT[R, M, A]): ReaderT[R, M, A] = r match { case ReaderT(ra) => ReaderT(r => { val localR = f(r) ra(localR) }) } } trait ReaderIdentMonad[R] extends ReaderTMonad[R, Identity] { // same as ReaderMonad override def M0 = identityMonad } Note that the order of how Monad Transfomers being stacked up makes a difference, For instance, can you explain what is the difference between the following two? trait ReaderStateIdentMonad[R, S] extends ReaderTMonad[R, StateTM[S][Identity]] { override def M0:StateIdentMonad[S] = new StateIdentMonad[S]{} } trait StateReaderIdentMonad[S, R] extends StateTMonad[S, ReaderTM[R][Identity]] { override def M0:ReaderIdentMonad[R] = new ReaderIdentMonad[R]{} }","title":"50.054 - Applicative and Monad"},{"location":"notes/fp_applicative_monad/#50054-applicative-and-monad","text":"","title":"50.054 - Applicative and Monad"},{"location":"notes/fp_applicative_monad/#learning-outcomes","text":"Describe and define derived type class Describe and define Applicative Functors Describe and define Monads Apply Monad to in design and develop highly modular and resusable software.","title":"Learning Outcomes"},{"location":"notes/fp_applicative_monad/#derived-type-class","text":"Recall that in our previous lesson, we talk about the Ordering type class. trait Ordering[A] { def compare(x:A,y:A):Int // less than: -1, equal: 0, greater than 1 } Let's consider a variant called Order (actually it is defined in a popular Scala library named cats ). trait Eq[A] { def eqv(x:A, y:A):Boolean } trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } In the above, the Eq type class is a supertype of the Order type class, because all instances of Order type class should have the method eqv implemented. We also say Order is a derived type class of Eq . Let's consider some instances given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt:Order[Int] = new Order[Int] { def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1)","title":"Derived Type Class"},{"location":"notes/fp_applicative_monad/#an-alternative-approach","text":"trait Order[A] extends Eq[A] { def compare(x:A, y:A):Int // def eqv(x:A, y:A):Boolean = compare(x,y) == 0 def gt(x:A, y:A):Boolean = compare(x,y) > 0 def lt(x:A, y:A):Boolean = compare(x,y) < 0 } given eqInt:Eq[Int] = new Eq[Int] { def eqv(x:Int, y:Int):Boolean = x == y } given orderInt(using eqInt:Eq[Int]):Order[Int] = new Order[Int] { def eqv(x:Int,y:Int):Boolean = eqInt.eqv(x,y) def compare(x:Int, y:Int):Int = x - y } eqInt.eqv(1,1) orderInt.eqv(1,1) In the above definition, we skip the default implementatoin of eqv in Order and make use of the type class instance context to synthesis the eqv method based on the existing type class instances of Eq . (This approach is closer to the one found in Haskell.)","title":"An alternative approach"},{"location":"notes/fp_applicative_monad/#which-one-is-better","text":"Both have their own pros and cons. In the first approach, we give a default implementation for the eqv overridden method in Order type class, it frees us from re-defining the eqv in every type class instance of Order . In this case, the rule/logic is fixed at the top level. In the second approach, eqv in Order type class is not defined. We are required to define it for every single type class instance of Order , that means more work. The advantage is that we have flexibility to redefine/re-mix definition of eqv coming from other type class instances.","title":"Which one is better?"},{"location":"notes/fp_applicative_monad/#functor-recap","text":"Recall from the last lesson, we make use of the Functor type class to define generic programming style of data processing. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } enum BTree[+A]{ case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Note that we also swap the first and the second arguments of the map function.","title":"Functor (Recap)"},{"location":"notes/fp_applicative_monad/#applicative-functor","text":"The Applicative Functor is a derived type class of Functor , which is defined as follows trait Applicative[F[_]] extends Functor[F] { def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] def pure[A](a: A): F[A] def map[A, B](fa: F[A])(f: A => B): F[B] = ap(pure(f))(fa) } Note that we \"fix\" the map for Applicative in the type class level in this case. (i.e. we are following the first approach.) given listApplicative:Applicative[List] = new Applicative[List] { def pure[A](a:A):List[A] = List(a) def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.map( f => fa.map(f)).flatten } Recall that flatten flattens a list of lists. Alternatively, we can define the ap method of the Applicative[List] instance flatMap . Given l is a list, l.flatMap(f) is the same as l.map(f).flatten . def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = ff.flatMap( f => fa.map(f)) Recall that Scala compiler desugars expression of shape e1.flatMap( v1 => e2.flatMap( v2 => ... en.map(vn => e ... ))) into for { v1 <- e1 v2 <- e2 ... vn <- en } yield (e) Hence we can rewrite the ap method of the Applicative[List] instance as def ap[A, B](ff: List[A => B])(fa: List[A]):List[B] = for { f <- ff a <- fa } yield (f(a)) It is not suprising the following produces the same results as the functor instance. listApplicative.map(l)((x:Int) => x + 1) What about pure and ap ? when can we use them? Let's consider the following contrived example. Suppose we would like to apply two sets of operations to elements from l , each operation will produce its own set of results, and the inputs do not depend on the output of the other set. i.e. If the two set of operations, are (x:Int)=> x+1 and (y:Int)=>y*2 . val intOps= List((x:Int)=>x+1, (y:Int)=>y*2) listApplicative.ap(intOps)(l) we get List(2, 3, 4, 2, 4, 6) as the result. Let's consider another example. Recall that Option[A] algebraic datatype which captures a value of type A could be potentially empty. We define the Applicative[Option] instance as follows given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff match { case None => None case Some(f) => fa match { case None => None case Some(a) => Some(f(a)) } } } In the above Applicative instance, the ap method takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning None . This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value. Recall the builtin Option type is defined as follows, // no need to run this. enum Option[+A] { case None case Some(v) def map[B](f:A=>B):Option[B] = this match { case None => None case Some(v) => Some(f(v)) } def flatMap[B](f:A=>Option[B]):Option[B] = this match { case None => None case Some(v) => f(v) match { case None => None case Some(u) => Some(u) } } } Hence optApplicative can be simplified as given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = ff.flatMap(f => fa.map(f)) // same as listApplicative } or given optApplicative:Applicative[Option] = new Applicative[Option] { def pure[A](v:A):Option[A] = Some(v) def ap[A,B](ff:Option[A=>B])(fa:Option[A]):Option[B] = for { f <- ff a <- fa } yield f(a) // same as listApplicative }","title":"Applicative Functor"},{"location":"notes/fp_applicative_monad/#applicative-laws","text":"Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable. Identity: ap(pure(x=>x)) \\(\\equiv\\) x=>x Homomorphism: ap(pure(f))(pure(x)) \\(\\equiv\\) pure(f(x)) Interchange: ap(u)(pure(y)) \\(\\equiv\\) ap(pure(f=>f(y)))(u) Composition: ap(ap(ap(pure(f=>f.compose))(u))(v))(w) \\(\\equiv\\) ap(u)(ap(v)(w)) Identity law states that applying a lifted identity function of type A=>A is same as an identity function of type F[A] => F[A] where F is the applicative functor. Homomorphism says that applying a lifted function (which has type A=>A before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result. To understand Interchange law let's consider the following equation $$ u\\ y \\equiv (\\lambda f.(f\\ y))\\ u $$ Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\) . To understand the Composition law, we consider the following equation in lambda calculus \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w & \\longrightarrow_{\\beta} \\\\ (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w & \\longrightarrow_{\\beta} \\\\ (u\\circ v)\\ w & \\longrightarrow_{\\tt composition} \\\\ u\\ (v\\ w) \\end{array} \\] The Composition Law says that the above equation remains valid when \\(u\\) , \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\) .","title":"Applicative Laws"},{"location":"notes/fp_applicative_monad/#cohort-exercise","text":"show that any applicative functor satisfying the above laws also satisfies the Functor Laws","title":"Cohort Exercise"},{"location":"notes/fp_applicative_monad/#monad","text":"Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor. Let's consider a motivating example. Recall that in the earlier lesson, we came across the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } In which we use Option[A] to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to Either[A,B] if we want to have better error messages. Monad is a good application here. Let's consider the type class definition of Monad[F[_]] . trait Monad[F[_]] extends Applicative[F] { def bind[A,B](fa:F[A])(f:A => F[B]):F[B] def pure[A](v:A):F[A] def ap[A, B](ff: F[A => B])(fa: F[A]): F[B] = bind(ff)((f:A=>B) => bind(fa)((a:A)=> pure(f(a)))) } given optMonad:Monad[Option] = new Monad[Option] { def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } The eval function can be re-expressed using Monad[Option] . def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1+v2)}) }) case MathExp.Minus(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1-v2)}) }) case MathExp.Mult(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => m.pure(v1*v2)}) }) case MathExp.Div(e1, e2) => m.bind(eval(e1))( v1 => { m.bind(eval(e2))( {v2 => if (v2 == 0) {None} else {m.pure(v1/v2)}}) }) case MathExp.Const(i) => m.pure(i) } It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of for comprehension since Option has the member functions flatMap and map defined. Recall that Scala desugars for {...} yield expression into flatMap and map . Thus the above can be rewritten as def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval(e1) v2 <- eval(e2) if (v2 !=0) } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now the readability is restored. Another advantage of coding with Monad is that its abstraction allows us to switch underlying data structure without major code change. Suppose we would like to use Either[String, A] or some other equivalent as return type of eval function to support better error message. But before that, let's consider some subclasses of the Applicative and the Monad type classes. trait ApplicativeError[F[_], E] extends Applicative[F] { def raiseError[A](e:E):F[A] } trait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] { override def raiseError[A](e:E):F[A] } type ErrMsg = String In the above, we define an extension to the Applicative type class, named ApplicativeError which expects an extra type class parameter E that denotes an error. The raiseError method takes a value of type E and returns the Applicative result. Similarly, we extend Monad type class with MonadError type class. Next we include the following type class instance to include Option as one f the MonadError functor. given optMonadError:MonadError[Option, ErrMsg] = new MonadError[Option, ErrMsg] { def raiseError[A](e:ErrMsg):Option[A] = None def bind[A,B](fa:Option[A])(f:A=>Option[B]):Option[B] = fa match { case None => None case Some(a) => f(a) } def pure[A](v:A):Option[A] = Some(v) } Next, we adjust the eval function to takes in a MonadError context instead of a Monad context. In addition, we make the error signal more explicit by calling the raiseError() method from the MonadError type class context. def eval2(e:MathExp)(using m:MonadError[Option, ErrMsg]):Option[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval2(e1) v2 <- eval2(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) } Now let's try to refactor the code to make use of Either[ErrMsg, A] as the functor instead of Option[A] . enum Either[+A, +B] { case Left(v: A) case Right(v: B) // to support for comprehension def flatMap[C>:A,D](f: B => Either[C,D]):Either[C,D] = this match { case Left(a) => Left(a) case Right(b) => f(b) } def map[D](f:B => D):Either[A,D] = this match { case Right(b) => Right(f(b)) case Left(e) => Left(e) } } In the above, we have to define flatMap and map member functions for Either type so that we could make use of the for comprehension later on. One might argue with the type signature of flatMap should be flatMap[D](f: B => Either[A,D]):Either[A,D] . The issue here is that the type variable A will appear in both co- and contra-variant positions. The top-level annotation +A is no longer true. Hence we \"relax\" the type constraint here by introducing a new type variable C which has a lower bound of A (even though we do not need to upcast the result of the Left alternative.) type EitherErr = [B] =>> Either[ErrMsg,B] In the above we define Either algebraic datatype and the type construcor EitherErr . [B] =>> Either[ErrMsg, B] denotes a type lambda, which means that EitherErr is a type constructor (or type function) that takes a type B and return an Either[ErrMsg, B] type. Next, we define the type class instance for MonadError[EitherErr, ErrMsg] given eitherErrMonad: MonadError[EitherErr, ErrMsg] = new MonadError[EitherErr, ErrMsg] { import Either.* def raiseError[B](e: ErrMsg): EitherErr[B] = Left(e) def bind[A, B]( fa: EitherErr[A] )(f: A => EitherErr[B]): EitherErr[B] = fa match { case Right(b) => f(b) case Left(s) => Left(s) } def pure[B](v: B): EitherErr[B] = Right(v) } And finally, we refactor the eval function by changing its type signature. And its body remains unchanged. def eval3(e:MathExp)(using m:MonadError[EitherErr, ErrMsg]):EitherErr[Int] = e match { case MathExp.Plus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1+v2) case MathExp.Minus(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1-v2) case MathExp.Mult(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) } yield (v1*v2) case MathExp.Div(e1, e2) => for { v1 <- eval3(e1) v2 <- eval3(e2) _ <- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())} } yield (v1/v2) case MathExp.Const(i) => m.pure(i) }","title":"Monad"},{"location":"notes/fp_applicative_monad/#commonly-used-monads","text":"We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads.","title":"Commonly used Monads"},{"location":"notes/fp_applicative_monad/#list-monad","text":"We know that List is a Functor and an Applicative. It is not surprising that List is also a Monad. given listMonad:Monad[List] = new Monad[List] { def pure[A](v:A):List[A] = List(v) def bind[A,B](fa:List[A])(f:A => List[B]):List[B] = fa.flatMap(f) } With the above instance, we can write list processing method in for comprehension which is similar to query languages. import java.util.Date import java.util.Calendar import java.util.GregorianCalendar import java.text.SimpleDateFormat case class Staff(id:Int, dob:Date) def mkStaff(id:Int, dobStr:String):Staff = { val sdf = new SimpleDateFormat(\"yyyy-MM-dd\") val dobDate = sdf.parse(dobStr) Staff(id, dobDate) } val staffData = List( mkStaff(1, \"1076-01-02\"), mkStaff(2, \"1986-07-24\") ) def ageBelow(staff:Staff, age:Int): Boolean = staff match { case Staff(id, dob) => { val today = new Date() val calendar = new GregorianCalendar(); calendar.setTime(today) calendar.add(Calendar.YEAR, -age) val ageYearsAgo = calendar.getTime() dob.after(ageYearsAgo) } } def query(data:List[Staff]):List[Staff] = for { staff <- data // from data if ageBelow(staff, 40) // where staff.age < 40 } yield staff // select *","title":"List Monad"},{"location":"notes/fp_applicative_monad/#reader-monad","text":"Next we consider the Reader Monad. Reader Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable. For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment. case class Reader[R, A] (run: R=>A) { // we need flatMap and map for for-comprehension def flatMap[B](f:A =>Reader[R,B]):Reader[R,B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) match { case Reader(rb) => rb(r) } ) } def map[B](f:A=>B):Reader[R, B] = this match { case Reader(ra) => Reader ( r => f(ra(r)) ) } } type ReaderM = [R] =>> [A] =>> Reader[R, A] trait ReaderMonad[R] extends Monad[ReaderM[R]] { override def pure[A](v:A):Reader[R, A] = Reader (r => v) override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa match { case Reader(ra) => Reader ( r=> f(ra(r)) match { case Reader(rb) => rb(r) } ) } def ask:Reader[R,R] = Reader( r => r) def local[A](f:R=>R)(r:Reader[R,A]):Reader[R,A] = r match { case Reader(ra) => Reader( r => { val localR = f(r) ra(localR) }) } } In the above Reader[R,A] case class defines the structure of the Reader type, where R denotes the shared information for the computation, (source for reader), A denotes the output of the computation. We would like to define Reader[R,_] as a Monad instance. To do so, we define a type-curry version of Reader , i.e. ReaderM . One crucial observation is that bind method in ReaderMonad is nearly identical to flatMap in Reader , with the arguments swapped. In fact, we can re-express bind for all Monads as the flatMap in their underlying case class. override def bind[A,B](fa:Reader[R, A])(f:A=>Reader[R,B]):Reader[R,B] = fa.flatMap(f) The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input https://127.0.0.1/ ). For authentication we need to call the authentication server https://127.0.0.10/ temporarily. case class API(url:String) given APIReader:ReaderMonad[API] = new ReaderMonad[API] {} def get(path:String)(using pr:ReaderMonad[API]):Reader[API,Unit] = for { r <- pr.ask s <- r match { case API(url) => pr.pure(println(s\"${url}${path}\")) } } yield s def authServer(api:API):API = API(\"https://127.0.0.10/\") def test1(using pr:ReaderMonad[API]):Reader[API, Unit] = for { a <- pr.local(authServer)(get(\"auth\")) t <- get(\"time\") j <- get(\"job\") } yield (()) def runtest1():Unit = test1 match { case Reader(run) => run(API(\"https://127.0.0.1/\")) }","title":"Reader Monad"},{"location":"notes/fp_applicative_monad/#state-monad","text":"We consider the State Monad. A State Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like Scala, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging. The following we define a State case class, which has a member computation run:S => (S,A) . case class State[S,A]( run:S=>(S,A)) { def flatMap[B](f: A => State[S,B]):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1,a) => f(a) match { case State(ssb) => ssb(s1) } } ) } def map[B](f:A => B):State[S,B] = this match { case State(ssa) => State( s=> ssa(s) match { case (s1, a) => (s1, f(a)) } ) } } As suggested by the type, the computationn S=>(S,A) , takes in a state S as input and return a tuple of output, consists a new state and the result of the computation. The State Monad type class is defined as a dervied type class of Monad[StateM[S]] . type StateM = [S] =>> [A] =>> State[S,A] trait StateMonad[S] extends Monad[StateM[S]] { override def pure[A](v:A):State[S,A] = State( s=> (s,v)) override def bind[A,B]( fa:State[S,A] )( ff:A => State[S,B] ):State[S,B] = fa.flatMap(ff) def get:State[S, S] = State(s => (s,s)) def set(v:S):State[S,Unit] = State(s => (v,())) } In the pure method's default implementation, we takes a value v of type A and return a State case class oject by wrapping a lambda which takes a state s and returns back the same state s with the input value v . In the default implementation of the bind method, we take a computation fa of type State[S,A] , i.e. a stateful computation over state type S and return a result of type A . In addition, we take a function that expects input of type A and returns a stateful computation State[S,B] . We apply flatMap of fa to ff ., which can be expanded to fa.flatMap(ff) --> fa match { case State(ssa) => State ( s => { ssa(s) match { case (s1,a) => ff(a) match { case State(ssb) => ssb(s1) } } }) } In essence it \"opens\" the computation in fa to extract the run function ssa which takes a state returns result A with the output state. As the output, we construct stateful computation in which a state s is taken as input, we immediately apply s with ssa (i.e. the computation extracted from fa ) to compute the intermediate state s1 and the output a (of type A ). Next we apply ff to a which returns a Stateful computation State[S,B] . We extract the run function from this stateful copmutation, namley ssb and apply it to s1 to continue with the result of the computation. In otherwords, bind function chains up a stateful computation fa with a lambda expressoin that consumes the result from fa and continue with another stateful copmutation. The get and the set methods give us access to the state environment of type S . For instance, case class Counter(c:Int) given counterStateMonad:StateMonad[Counter] = new StateMonad[Counter] { } def incr(using csm:StateMonad[Counter]):State[Counter,Unit] = for { Counter(c) <- csm.get _ <- csm.set(Counter(c+1)) } yield () def app(using csm:StateMonad[Counter]):State[Counter, Int] = for { _ <- incr _ <- incr Counter(v) <- csm.get } yield v In the above we define the state environment as an integer counter. Monadic function incr increase the counter in the state.","title":"State Monad"},{"location":"notes/fp_applicative_monad/#monad-laws","text":"Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws. Left Identity: bind(pure(a))(f) \\(\\equiv\\) f(a) Right Identity: bind(m)(pure) \\(\\equiv\\) m Associativity: bind(bind(m)(f))(g) \\(\\equiv\\) bind(m)(x => bind(f(x))(g)) Intutively speaking, a bind operation is to extract results of type A from its first argument with type F[A] and apply f to the extracted results. Left identity law enforces that binding a lifted value to f , is the same as applying f to the unlifted value directly, because the lifting and the extraction of the bind cancel each other. Right identity law enforces that binding a lifted value to pure , is the same as the lifted value, because extracting results from m and pure cancel each other. The Associativity law enforces that binding a lifted value m to f then to g is the same as binding m to a monadic bind composition (x => bind(f(x)(g)))","title":"Monad Laws"},{"location":"notes/fp_applicative_monad/#summary","text":"In this lesson we have discussed the following A derived type class is a type class that extends from another one. An Applicative Functor is a sub-class of Functor, with the methods pure and ap . The four laws for Applicative Functor. A Monad Functor is a sub-class of Applicative Functor, with the method bind . The three laws of Monad Functor. A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad.","title":"Summary"},{"location":"notes/fp_applicative_monad/#extra-materials","text":"","title":"Extra Materials"},{"location":"notes/fp_applicative_monad/#writer-monad","text":"The dual of the Reader Monad is the Writer Monad, which has the following definition. // inspired by https://kseo.github.io/posts/2017-01-21-writer-monad.html trait Monoid[A]{ // We omitted the super class SemiRing[A] def mempty:A def mappend:A => A => A } given listMonoid[A]:Monoid[List[A]] = new Monoid[List[A]] { def mempty:List[A] = Nil def mappend:List[A]=>List[A]=>List[A] = (l1:List[A])=>(l2:List[A]) => l1 ++ l2 } case class Writer[W,A]( run: (W,A))(using mw:Monoid[W]) { def flatMap[B](f:A => Writer[W,B]):Writer[W,B] = this match { case Writer((w,a)) => f(a) match { case Writer((w2,b)) => Writer((mw.mappend(w)(w2), b)) } } def map[B](f:A=>B):Writer[W, B] = this match { case Writer((w,a)) => Writer((w, f(a))) } } Similar to the Reader Monad, in the above we define a case class Writer , which has a member value run that returns a tuple of (W,A) . The subtle difference is that the writer memory W has to be an instance of the Monoid type class, in which mempty and mappend operations are defined. type WriterM = [W] =>> [A] =>> Writer[W,A] trait WriterMonad[W] extends Monad[WriterM[W]] { implicit def W0:Monoid[W] override def pure[A](v: A): Writer[W, A] = Writer((W0.mempty, v)) override def bind[A, B]( fa: Writer[W, A] )(f: A => Writer[W, B]): Writer[W, B] = fa match { case Writer((w, a)) => f(a) match { case Writer((w2, b)) => { Writer((W0.mappend(w)(w2), b)) } } } def tell(w: W): Writer[W, Unit] = Writer((w, ())) def pass[A](ma: Writer[W, (A, W => W)]): Writer[W, A] = ma match { case Writer((w, (a, f))) => Writer((f(w), a)) } } In the above we define WriterMonad to be a derived type class of Monad[WriterM[W]] . For a similar reason, we need to include the type class Monoid[W] to ensure that mempty and mappend are defined on W . Besides the pure and bind members, we introduce tell and pass . tell writes the given argument into the writer's memory. pass execute a given computation which returns a value of type A and a memory update function W=>W , and return a Writer whose memory is updated by applied the update function to the memory. In the following we define a simple application with logging mechanism using the Writer Monad. case class LogEntry(msg:String) given logWriterMonad:WriterMonad[List[LogEntry]] = new WriterMonad[List[LogEntry]] { override def W0:Monoid[List[LogEntry]] = new Monoid[List[LogEntry]] { override def mempty = Nil override def mappend = (x:List[LogEntry]) => (y:List[LogEntry]) => x ++ y } } def logger(m: String)(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Unit] = wm.tell(List(LogEntry(m))) def app(using wm: WriterMonad[List[LogEntry]] ): Writer[List[LogEntry], Int] = for { _ <- logger(\"start\") x <- wm.pure(1 + 1) _ <- logger(s\"result is ${x}\") _ <- logger(\"done\") } yield x def runApp(): Int = app match { case Writer((w, i)) => { println(w) i } }","title":"Writer Monad"},{"location":"notes/fp_applicative_monad/#monad-transformer","text":"Is the following class a Monad? case class MyState[S,A]( run:S=>Option[(S,A)]) The difference between this class and the State class we've seen earlier is that the execution method run yields result of type Option[(S,A)] instead of (S,A) which means that it can potentially fail. It is ascertained that MyState is also a Monad, and it is a kind of special State Monad. case class MyState[S, A](run: S => Option[(S, A)]) { def flatMap[B](f: A => MyState[S, B]): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => f(a) match { case MyState(ssb) => ssb(s1) } } ) } def map[B](f: A => B): MyState[S, B] = this match { case MyState(ssa) => MyState(s => ssa(s) match { case None => None case Some((s1, a)) => Some((s1, f(a))) } ) } } type MyStateM = [S] =>> [A] =>> MyState[S,A] trait MyStateMonad[S] extends Monad[MyStateM[S]] { override def pure[A](v:A):MyState[S,A] = MyState( s=> Some((s,v))) override def bind[A,B]( fa:MyState[S,A] )( ff:A => MyState[S,B] ):MyState[S,B] = fa.flatMap(ff) def get:MyState[S, S] = MyState(s => Some((s,s))) def set(v:S):MyState[S,Unit] = MyState(s => Some((v,()))) } Besides \"stuffing-in\" an Option type, one could use an Either type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer . We begin by parameterizing the Option functor in MyState case class StateT[S, M[_], A](run: S => M[(S, A)])(using m:Monad[M]) { def flatMap[B](f: A => StateT[S, M, B]): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1,a) => f(a) match { case StateT(ssb) => ssb(s1) } } ) ) } def map[B](f: A => B): StateT[S, M, B] = this match { case StateT(ssa) => StateT(s => m.bind(ssa(s)) (sa => sa match { case (s1, a) => m.pure((s1, f(a))) }) ) } } In the above it is largely similar to MyState class, except that we parameterize Option by a type parameter M . M[_] indicates that it is of kind *=>* . (using m:Monad[M]) further contraints M must be an instance of Monad, so that we could make use of the bind and pure from M 's Monad instance. Naturally, we can define a derived type class called StateTMonad . type StateTM = [S] =>> [M[_]] =>> [A] =>> StateT[S, M, A] trait StateTMonad[S,M[_]] extends Monad[StateTM[S][M]] { implicit def M0:Monad[M] override def pure[A](v: A): StateT[S, M, A] = StateT(s => M0.pure((s, v))) override def bind[A, B]( fa: StateT[S, M, A] )( ff: A => StateT[S, M, B] ): StateT[S, M, B] = fa.flatMap(ff) def get: StateT[S, M, S] = StateT(s => M0.pure((s, s))) def set(v: S): StateT[S, M, Unit] = StateT(s => M0.pure(v, ())) } Given that Option is a Monad, we can redefine MyStateMonad in terms of StateTMonad and optMonad . trait StateOptMonad[S] extends StateTMonad[S, Option] { override def M0 = optMonad } What about the original vanilla State Monad? We could introduce an Identity Monad. case class Identity[A](run:A) { def flatMap[B](f:A=>Identity[B]):Identity[B] = this match { case Identity(a) => f(a) } def map[B](f:A=>B):Identity[B] = this match { case Identity(a) => Identity(f(a)) } } given identityMonad:Monad[Identity] = new Monad[Identity] { override def pure[A](v:A):Identity[A] = Identity(v) override def bind[A,B](fa:Identity[A])(f: A => Identity[B]):Identity[B] = fa.flatMap(f) } Then we can re-define the vanilla State Monad as follows, (in fact like many existing Monad libraries out there.) trait StateIdentMonad[S] extends StateTMonad[S, Identity] { // same as StateMonad override def M0 = identityMonad } One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes. Similarly we could generalize the Reader Monad to its transformer variant. case class ReaderT[R, M[_], A](run: R => M[A])(using m:Monad[M]) { def flatMap[B](f: A => ReaderT[R, M, B]):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => f(a) match { case ReaderT(rb) => rb(r) })) } def map[B](f: A => B):ReaderT[R, M, B] = this match { case ReaderT(ra) => ReaderT( r => m.bind(ra(r)) ( a => m.pure(f(a)))) } } type ReaderTM = [R] =>>[M[_]] =>> [A] =>> ReaderT[R, M, A] trait ReaderTMonad[R,M[_]] extends Monad[ReaderTM[R][M]] { implicit def M0:Monad[M] override def pure[A](v: A): ReaderT[R, M, A] = ReaderT(r => M0.pure(v)) override def bind[A, B]( fa: ReaderT[R, M, A] )(f: A => ReaderT[R, M, B]): ReaderT[R, M, B] = fa.flatMap(f) def ask: ReaderT[R, M, R] = ReaderT(r => M0.pure(r)) def local[A](f: R => R)(r: ReaderT[R, M, A]): ReaderT[R, M, A] = r match { case ReaderT(ra) => ReaderT(r => { val localR = f(r) ra(localR) }) } } trait ReaderIdentMonad[R] extends ReaderTMonad[R, Identity] { // same as ReaderMonad override def M0 = identityMonad } Note that the order of how Monad Transfomers being stacked up makes a difference, For instance, can you explain what is the difference between the following two? trait ReaderStateIdentMonad[R, S] extends ReaderTMonad[R, StateTM[S][Identity]] { override def M0:StateIdentMonad[S] = new StateIdentMonad[S]{} } trait StateReaderIdentMonad[S, R] extends StateTMonad[S, ReaderTM[R][Identity]] { override def M0:ReaderIdentMonad[R] = new ReaderIdentMonad[R]{} }","title":"Monad Transformer"},{"location":"notes/fp_intro/","text":"50.054 - Introduction to functional programming Learning Outcomes By the end of this lesson, you should be able to: Characterize functional programming Comprehend, evaluate lambda terms Differentiate different evaluation strategies Implement simple algorithms using Lambda Calculus What is Functional programming? Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules. How FP differs from other programming languages? The main differences were listed in the earlier section. However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects. Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm): def isort(vals): for i in range(1, len(vals)): curr = i for j in range(i, 0, -1): # scan backward to insert vals[curr] into the right pos if vals[curr] > vals[j-1]: vals[curr], vals[j-1] = vals[j-1], vals[curr] curr = j-1 return vals def isort2(vals): def insert(x, xs): # invarant: xs is already sorted in descending order if len(xs) > 0: if x > xs[0]: return [x] + xs else: return [xs[0]] + insert(x, xs[1:]) else: return [x] def isort_sub(sorted, to_be_sorted): # invariant sorted is already sorted in descending order if len(to_be_sorted) > 0: val = to_be_sorted[0] to_be_sorted_next = to_be_sorted[1:] sorted_next = insert(val, sorted) return isort_sub(sorted_next, to_be_sorted_next) else: return sorted return isort_sub([], vals) isort is implemented in the imperative style; the way we are familiar with. isort2 is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in isort2 in Python, because: it is lengthy it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations. But why are people are interested in FP? The reason is that the invariant of isort is much harder to derive compared to isort2 in which the nested functions' parameters are the subject of the invariants, and the variables in isort2 are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming. Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in isort2 can be expressed as type constraints, which can be verified by the compiler. What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction. In fact many modern FP languagues are quite fast. For example: https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/ Why FP in Compiler Design? Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles. To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in function programs compared to other programming paradigms. One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions. This implies that loop invariances are not constraints among the input and output of these recurisve function. In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages. Lambda Calculus Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s) . Lambda Expression This comic gives a very easy way to understand Lambda Expressions The valid syntax of lambda expression is described as the following EBNF ( Extended Backus Naur Form ) grammar: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] Where: Each line denotes a grammar rule The left hand side (LHS) of the ::= is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol. The RHS of the ::= is a set of alternatives, separated by | . Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\) , \\(\\lambda x.t\\) or \\(t\\ t\\) . \\(x\\) denotes a variable, \\(\\lambda x.t\\) denotes a lambda abstraction. Within a lambda abstraction, \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body. \\(t\\ t\\) denotes a function application. For example, the following are three instances of \\(t\\) . \\(x\\) \\(\\lambda x.x\\) \\((\\lambda x.x)\\ y\\) Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\) , we could interpret it as either \\((\\lambda x.x)\\ (\\lambda y.y)\\) , or \\(\\lambda x.(x\\ \\lambda y.y)\\) As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can. Evaluation Rules Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term. There are only two rules to consider. Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\) , which reads as \\(t\\) is reduced to \\(t'\\) by a step. Beta Reduction \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] What's new here is the term \\([t_2/x]\\) , which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\) , Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\) . For instance, recall our earlier example: \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) & \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x & \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example: \\[ (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction. To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound. \\[ \\begin{array}{rcl} fv(x) & = & \\{x\\}\\\\ fv(\\lambda x.t) & = & fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\end{array} \\] For instance. \\[ \\begin{array}{rcl} fv(\\lambda x.x) & = & fv(x) - \\{x\\} \\\\ & = & \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) & = & fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\ & = & \\{ y \\} \\end{array} \\] One common error we often encounter is capturing the free variables . Consider: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] Note: \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) = \\{ {\\tt y}, w \\} \\] Thus: \\[ \\begin{array}{rl} (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) & \\longrightarrow \\\\ \\lbrack({\\tt y}\\ w)/x\\rbrack \\lambda y.x\\ y & \\longrightarrow \\\\ \\lambda y. ({\\tt y}\\ w)\\ y \\end{array} \\] Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake. Substitution and Alpha Renaming In the following we consider all the possible cases for subsititution \\[ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack x & = & t_1 \\\\ \\lbrack t_1 / x \\rbrack y & = & y & {\\tt if}\\ x \\neq y \\\\ \\lbrack t_1 / x \\rbrack (t_2\\ t_3) & = & \\lbrack t_1 / x \\rbrack t_2\\ \\lbrack t_1 / x \\rbrack t_3 & \\\\ \\lbrack t_1 / x \\rbrack \\lambda y.t_2 & = & \\lambda y. \\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\end{array} \\] In case \\[ y\\neq x\\ {\\tt and} \\ y \\not \\in fv(t_1) \\] is not satified, we need to rename the lambda bound variables that are clashing. Recall: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] We rename the inner lambda bound variable \\(y\\) to \\(z\\) : \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] to avoid clashing, prior applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming. Evaluation strategies So far we have three rules (roughly) $\\beta $ reduction, substitution, and $\\alpha $ renaming. Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules. We call a (sub-)expression of shape \\(\\lambda x.t_1\\ t_2\\) a redex . The task is to look for redexes in a lambda term and rewrite them by applying $\\beta $ reduction and substitution, and sometimes $\\alpha $ renaming to avoid capturing free variables. But in what order shall we apply these rules? There are two mostly known strategies Inner-most, leftmost - Applicative Order Reduction (AOR) Outer-most, leftmost - Normal Order Reduction (NOR) Consider $(\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y) $, AOR: \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y) & \\longrightarrow_{\\tt (\\beta\\ reduction)} &\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] NOR: \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda x. x)\\ z))\\ (\\lambda y.y)} & \\longrightarrow_{\\tt(\\beta)} \\\\ \\underline{(\\lambda x. x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta)} \\\\ \\lambda y.y \\end{array} \\] Interesting Notes Some connection with real world languages: Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions. Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions. AOR or NOR, which one is better? By Church-Rosser Theorem, if a lambda term can be evaluated in two different ways and both ways terminate, both will yield the same result. Recall our earlier example. So how can it be non-terminating? Consider: \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) & \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ ... \\end{array} \\] NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how \\(((\\lambda x.\\lambda y.x)\\ x)\\ ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\) , but diverges in AOR. NOR can be used to evaluate terms that deals with infinite data. Let Binding Let-binding allows us to introduce local (immutable) variables. Approach 1 - extending the syntax and evaluation rules We extend the syntax with let-binding: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] and the evaluation rule: \\[ \\begin{array}{rl} {\\tt (Let)} & let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] and the substitution rule and the free variable function \\(fv()\\) : \\[ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 & = & let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\\\ \\end{array} $$ $$ \\begin{array}{rcl} fv(let\\ x=t_1\\ in\\ t_2) & = & (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\ \\end{array} \\] Note that the alpha renaming should be applied when name clash arises. Approach 2 - desugaring In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language. We can rewrite: \\[ let\\ x=t_1\\ in\\ t_2 \\] into: \\[ (\\lambda x.t_2)\\ t_1 \\] where \\(x \\not\\in fv(t_1)\\) . What happen if \\(x \\in fv(t_1)\\) ? It forms a recursive definition. We will look into recursion in a later section. Conditional Expression A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . There are at least two different ways of incorporating conditional expression in our lambda term language. Approach 1 - Extending the syntax and the evaluation rules We could extend the grammar \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] and the evaluation rules \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3 \\end{array} \\\\ \\\\ {\\tt (ifT)} & if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} & if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ op\\ t_2\\ \\longrightarrow t_1'\\ op\\ t_2 \\end{array} \\\\ \\\\ {\\tt (OpI2)} & \\begin{array}{c} t_2 \\longrightarrow t_2' \\\\ \\hline c_1\\ op\\ t_2\\ \\longrightarrow c_1\\ op\\ t_2' \\end{array} \\\\ \\\\ {\\tt (OpC)} & \\begin{array}{c} invoke\\ low\\ level\\ call\\ op(c_1, c_2) = c_3 \\\\ \\hline c_1\\ op\\ c_2\\ \\longrightarrow c_3 \\end{array} \\\\ \\\\ ... \\end{array} \\] In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises , and the relation the written below is called the conclusion . The conclusion holds if the premises are valid. The rule ${\\tt (ifI)} $ states that if we can evaluate \\(t_1\\) to $t_1' $, then $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ can be evaluated to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $. In otherwords, for us to reduce $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $, a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\) . The rule ${\\tt (ifT)} $ states that if the conditional expression is \\(true\\) , the entire term is evaluated to the then-branch. The rule ${\\tt (ifF)} $ is similar. Rules \\({\\tt (OpI1)}\\) and ${\\tt (OpI2)} $ are similar to rule \\({\\tt (IfI)}\\) . The rule ${\\tt (OpC)} $ invokes the built-in low level call to apply the binary operation to the two operands $c_1 $ and $c_2 $. The substitution rules and free variable function \\(fv()\\) also extended too $$ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack c & = & c \\ \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 & = & (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\ \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 & = & if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\ \\end{array} $$ $$ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\ fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) & = & fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\ fv(c) & = & {} \\ \\end{array} $$ Let's consider an example: \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\ 0\\ else\\ 10/x)\\ 2 & \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\ 0\\ else\\ 10/x & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\] Approach 2 - Church Encoding Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms. Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus. Let's define: \\(true\\) as \\(\\lambda x.\\lambda y.x\\) \\(false\\) as \\(\\lambda x.\\lambda y.y\\) \\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\) We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv (e_1\\ e_2)\\ e_3\\) . For example, \\[ \\begin{array}{rl} ite\\ true\\ w\\ z & = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z & \\longrightarrow \\\\ true\\ w\\ z & = \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z & \\longrightarrow \\\\ w \\end{array} \\] Recursion To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion. Similar to the conditional expression, there are at least two ways of introducing recursion to our language. Approach 1 - Extending the syntax and the evaluation rules We extend the syntax with a mu-abstraction ( \\(\\mu\\) ): \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & ... \\mid \\mu f.t \\end{array} \\] and the evaluation rules: \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ t_2 \\longrightarrow t_1'\\ t_2 \\end{array} \\\\ {\\tt (unfold)} & \\mu f.t \\longrightarrow [(\\mu f.t)/f] t \\\\ \\end{array} \\] Note that we include the ${\\tt (NOR)} $ rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate. We include the following cases for the free variable function \\(fv()\\) and the substitution $$ \\begin{array}{rcl} fv(\\mu f.t) & = & fv(t) - {f} \\end{array} $$ and $$ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack \\mu f.t_2 & = & \\mu f.\\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ f\\neq x\\ {\\tt and}\\ f \\not \\in fv(t_1) \\end{array} $$ For instance: \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)) & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\) , \\({\\tt (NOR)}\\) , \\({\\tt (unfold)}\\) , \\({\\tt (IfT)}\\) , \\({\\tt (IfF)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (OpC)}\\) , \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try to rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course. Approach 2 - Church Encoding Alternatively, recursion can be encoded using the fix-pointer combinator (AKA $Y $-combinator). Let $Y $ be \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] We find that for any function \\(g\\) , we have \\(Y\\ g = g\\ (Y\\ g)\\) . We will work on the derivation during exercise. Let's try to implement the factorial function over natural numbers: \\[ \\begin{array}{cc} fac(n) = \\left [ \\begin{array}{ll} 1 & {if}~ n = 0 \\\\ n*fac(n-1) & {otherwise} \\end{array} \\right . \\end{array} \\] Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition. Let \\(Fac\\) be \\[ \\begin{array}{c} \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above. Discussion 1 How to define the following? \\(one\\) \\(iszero\\) \\(mul\\) \\(pred\\) Discussion 2 The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)? We will work on the two topics discussed above during the cohort class. Summary We have covered Syntax (lambda terms) and Semantics ( \\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming). Evaluation strategies, their properties and connection to real world programming Extending lambda calculus to support conditional and loop Via language extension (we will use) Via Church encoding (fun but not very pragmatic in our context)","title":"50.054 - Introduction to functional programming"},{"location":"notes/fp_intro/#50054-introduction-to-functional-programming","text":"","title":"50.054 - Introduction to functional programming"},{"location":"notes/fp_intro/#learning-outcomes","text":"By the end of this lesson, you should be able to: Characterize functional programming Comprehend, evaluate lambda terms Differentiate different evaluation strategies Implement simple algorithms using Lambda Calculus","title":"Learning Outcomes"},{"location":"notes/fp_intro/#what-is-functional-programming","text":"Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules.","title":"What is Functional programming?"},{"location":"notes/fp_intro/#how-fp-differs-from-other-programming-languages","text":"The main differences were listed in the earlier section. However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects. Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm): def isort(vals): for i in range(1, len(vals)): curr = i for j in range(i, 0, -1): # scan backward to insert vals[curr] into the right pos if vals[curr] > vals[j-1]: vals[curr], vals[j-1] = vals[j-1], vals[curr] curr = j-1 return vals def isort2(vals): def insert(x, xs): # invarant: xs is already sorted in descending order if len(xs) > 0: if x > xs[0]: return [x] + xs else: return [xs[0]] + insert(x, xs[1:]) else: return [x] def isort_sub(sorted, to_be_sorted): # invariant sorted is already sorted in descending order if len(to_be_sorted) > 0: val = to_be_sorted[0] to_be_sorted_next = to_be_sorted[1:] sorted_next = insert(val, sorted) return isort_sub(sorted_next, to_be_sorted_next) else: return sorted return isort_sub([], vals) isort is implemented in the imperative style; the way we are familiar with. isort2 is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in isort2 in Python, because: it is lengthy it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations. But why are people are interested in FP? The reason is that the invariant of isort is much harder to derive compared to isort2 in which the nested functions' parameters are the subject of the invariants, and the variables in isort2 are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming. Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in isort2 can be expressed as type constraints, which can be verified by the compiler. What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction. In fact many modern FP languagues are quite fast. For example: https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/","title":"How FP differs from other programming languages?"},{"location":"notes/fp_intro/#why-fp-in-compiler-design","text":"Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles. To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in function programs compared to other programming paradigms. One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions. This implies that loop invariances are not constraints among the input and output of these recurisve function. In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages.","title":"Why FP in Compiler Design?"},{"location":"notes/fp_intro/#lambda-calculus","text":"Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s) .","title":"Lambda Calculus"},{"location":"notes/fp_intro/#lambda-expression","text":"This comic gives a very easy way to understand Lambda Expressions The valid syntax of lambda expression is described as the following EBNF ( Extended Backus Naur Form ) grammar: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] Where: Each line denotes a grammar rule The left hand side (LHS) of the ::= is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol. The RHS of the ::= is a set of alternatives, separated by | . Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\) , \\(\\lambda x.t\\) or \\(t\\ t\\) . \\(x\\) denotes a variable, \\(\\lambda x.t\\) denotes a lambda abstraction. Within a lambda abstraction, \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body. \\(t\\ t\\) denotes a function application. For example, the following are three instances of \\(t\\) . \\(x\\) \\(\\lambda x.x\\) \\((\\lambda x.x)\\ y\\) Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\) , we could interpret it as either \\((\\lambda x.x)\\ (\\lambda y.y)\\) , or \\(\\lambda x.(x\\ \\lambda y.y)\\) As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can.","title":"Lambda Expression"},{"location":"notes/fp_intro/#evaluation-rules","text":"Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term. There are only two rules to consider. Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\) , which reads as \\(t\\) is reduced to \\(t'\\) by a step.","title":"Evaluation Rules"},{"location":"notes/fp_intro/#beta-reduction","text":"\\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] What's new here is the term \\([t_2/x]\\) , which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\) , Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\) . For instance, recall our earlier example: \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) & \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x & \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example: \\[ (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction. To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound. \\[ \\begin{array}{rcl} fv(x) & = & \\{x\\}\\\\ fv(\\lambda x.t) & = & fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\end{array} \\] For instance. \\[ \\begin{array}{rcl} fv(\\lambda x.x) & = & fv(x) - \\{x\\} \\\\ & = & \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) & = & fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\ & = & (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\ & = & \\{ y \\} \\end{array} \\] One common error we often encounter is capturing the free variables . Consider: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] Note: \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) = \\{ {\\tt y}, w \\} \\] Thus: \\[ \\begin{array}{rl} (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) & \\longrightarrow \\\\ \\lbrack({\\tt y}\\ w)/x\\rbrack \\lambda y.x\\ y & \\longrightarrow \\\\ \\lambda y. ({\\tt y}\\ w)\\ y \\end{array} \\] Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake.","title":"Beta Reduction"},{"location":"notes/fp_intro/#substitution-and-alpha-renaming","text":"In the following we consider all the possible cases for subsititution \\[ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack x & = & t_1 \\\\ \\lbrack t_1 / x \\rbrack y & = & y & {\\tt if}\\ x \\neq y \\\\ \\lbrack t_1 / x \\rbrack (t_2\\ t_3) & = & \\lbrack t_1 / x \\rbrack t_2\\ \\lbrack t_1 / x \\rbrack t_3 & \\\\ \\lbrack t_1 / x \\rbrack \\lambda y.t_2 & = & \\lambda y. \\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\end{array} \\] In case \\[ y\\neq x\\ {\\tt and} \\ y \\not \\in fv(t_1) \\] is not satified, we need to rename the lambda bound variables that are clashing. Recall: \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] We rename the inner lambda bound variable \\(y\\) to \\(z\\) : \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] to avoid clashing, prior applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming.","title":"Substitution and Alpha Renaming"},{"location":"notes/fp_intro/#evaluation-strategies","text":"So far we have three rules (roughly) $\\beta $ reduction, substitution, and $\\alpha $ renaming. Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules. We call a (sub-)expression of shape \\(\\lambda x.t_1\\ t_2\\) a redex . The task is to look for redexes in a lambda term and rewrite them by applying $\\beta $ reduction and substitution, and sometimes $\\alpha $ renaming to avoid capturing free variables. But in what order shall we apply these rules? There are two mostly known strategies Inner-most, leftmost - Applicative Order Reduction (AOR) Outer-most, leftmost - Normal Order Reduction (NOR) Consider $(\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y) $, AOR: \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y) & \\longrightarrow_{\\tt (\\beta\\ reduction)} &\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] NOR: \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) & \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda x. x)\\ z))\\ (\\lambda y.y)} & \\longrightarrow_{\\tt(\\beta)} \\\\ \\underline{(\\lambda x. x)\\ (\\lambda y.y)} & \\longrightarrow_{\\tt (\\beta)} \\\\ \\lambda y.y \\end{array} \\]","title":"Evaluation strategies"},{"location":"notes/fp_intro/#interesting-notes","text":"Some connection with real world languages: Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions. Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions. AOR or NOR, which one is better? By Church-Rosser Theorem, if a lambda term can be evaluated in two different ways and both ways terminate, both will yield the same result. Recall our earlier example. So how can it be non-terminating? Consider: \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) & \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) & \\longrightarrow \\\\ ... \\end{array} \\] NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how \\(((\\lambda x.\\lambda y.x)\\ x)\\ ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\) , but diverges in AOR. NOR can be used to evaluate terms that deals with infinite data.","title":"Interesting Notes"},{"location":"notes/fp_intro/#let-binding","text":"Let-binding allows us to introduce local (immutable) variables.","title":"Let Binding"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-evaluation-rules","text":"We extend the syntax with let-binding: \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] and the evaluation rule: \\[ \\begin{array}{rl} {\\tt (Let)} & let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] and the substitution rule and the free variable function \\(fv()\\) : \\[ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 & = & let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 & {\\tt if}\\ y\\neq x\\ {\\tt and}\\ y \\not \\in fv(t_1) \\\\ \\end{array} $$ $$ \\begin{array}{rcl} fv(let\\ x=t_1\\ in\\ t_2) & = & (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\ \\end{array} \\] Note that the alpha renaming should be applied when name clash arises.","title":"Approach 1 - extending the syntax and evaluation rules"},{"location":"notes/fp_intro/#approach-2-desugaring","text":"In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language. We can rewrite: \\[ let\\ x=t_1\\ in\\ t_2 \\] into: \\[ (\\lambda x.t_2)\\ t_1 \\] where \\(x \\not\\in fv(t_1)\\) . What happen if \\(x \\in fv(t_1)\\) ? It forms a recursive definition. We will look into recursion in a later section.","title":"Approach 2 - desugaring"},{"location":"notes/fp_intro/#conditional-expression","text":"A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . There are at least two different ways of incorporating conditional expression in our lambda term language.","title":"Conditional Expression"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules","text":"We could extend the grammar \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] and the evaluation rules \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3 \\end{array} \\\\ \\\\ {\\tt (ifT)} & if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} & if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ op\\ t_2\\ \\longrightarrow t_1'\\ op\\ t_2 \\end{array} \\\\ \\\\ {\\tt (OpI2)} & \\begin{array}{c} t_2 \\longrightarrow t_2' \\\\ \\hline c_1\\ op\\ t_2\\ \\longrightarrow c_1\\ op\\ t_2' \\end{array} \\\\ \\\\ {\\tt (OpC)} & \\begin{array}{c} invoke\\ low\\ level\\ call\\ op(c_1, c_2) = c_3 \\\\ \\hline c_1\\ op\\ c_2\\ \\longrightarrow c_3 \\end{array} \\\\ \\\\ ... \\end{array} \\] In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises , and the relation the written below is called the conclusion . The conclusion holds if the premises are valid. The rule ${\\tt (ifI)} $ states that if we can evaluate \\(t_1\\) to $t_1' $, then $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ can be evaluated to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $. In otherwords, for us to reduce $if\\ t_1\\ then\\ t_2\\ else\\ t_3 $ to $if\\ t_1' \\ then\\ t_2\\ else\\ t_3 $, a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\) . The rule ${\\tt (ifT)} $ states that if the conditional expression is \\(true\\) , the entire term is evaluated to the then-branch. The rule ${\\tt (ifF)} $ is similar. Rules \\({\\tt (OpI1)}\\) and ${\\tt (OpI2)} $ are similar to rule \\({\\tt (IfI)}\\) . The rule ${\\tt (OpC)} $ invokes the built-in low level call to apply the binary operation to the two operands $c_1 $ and $c_2 $. The substitution rules and free variable function \\(fv()\\) also extended too $$ \\begin{array}{rcll} \\lbrack t_1 / x \\rbrack c & = & c \\ \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 & = & (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\ \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 & = & if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\ \\end{array} $$ $$ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) & = & fv(t_1) \\cup fv(t_2) \\ fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) & = & fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\ fv(c) & = & {} \\ \\end{array} $$ Let's consider an example: \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\ 0\\ else\\ 10/x)\\ 2 & \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\ 0\\ else\\ 10/x & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\ else\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 & \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\]","title":"Approach 1 - Extending the syntax and the evaluation rules"},{"location":"notes/fp_intro/#approach-2-church-encoding","text":"Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms. Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus. Let's define: \\(true\\) as \\(\\lambda x.\\lambda y.x\\) \\(false\\) as \\(\\lambda x.\\lambda y.y\\) \\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\) We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv (e_1\\ e_2)\\ e_3\\) . For example, \\[ \\begin{array}{rl} ite\\ true\\ w\\ z & = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z & \\longrightarrow \\\\ true\\ w\\ z & = \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z & \\longrightarrow \\\\ w \\end{array} \\]","title":"Approach 2 - Church Encoding"},{"location":"notes/fp_intro/#recursion","text":"To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion. Similar to the conditional expression, there are at least two ways of introducing recursion to our language.","title":"Recursion"},{"location":"notes/fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules_1","text":"We extend the syntax with a mu-abstraction ( \\(\\mu\\) ): \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & ... \\mid \\mu f.t \\end{array} \\] and the evaluation rules: \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} & (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} & \\begin{array}{c} t_1 \\longrightarrow t_1' \\\\ \\hline t_1\\ t_2 \\longrightarrow t_1'\\ t_2 \\end{array} \\\\ {\\tt (unfold)} & \\mu f.t \\longrightarrow [(\\mu f.t)/f] t \\\\ \\end{array} \\] Note that we include the ${\\tt (NOR)} $ rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate. We include the following cases for the free variable function \\(fv()\\) and the substitution $$ \\begin{array}{rcl} fv(\\mu f.t) & = & fv(t) - {f} \\end{array} $$ and $$ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack \\mu f.t_2 & = & \\mu f.\\lbrack t_1 / x \\rbrack t_2 & {\\tt if}\\ f\\neq x\\ {\\tt and}\\ f \\not \\in fv(t_1) \\end{array} $$ For instance: \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)))\\ 3 & \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (x-1)) & \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ f\\ (y-1))\\ (3-1)) & \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\) , \\({\\tt (NOR)}\\) , \\({\\tt (unfold)}\\) , \\({\\tt (IfT)}\\) , \\({\\tt (IfF)}\\) , \\({\\tt (IfI)}\\) , \\({\\tt (OpC)}\\) , \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try to rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course.","title":"Approach 1 - Extending the syntax and the evaluation rules"},{"location":"notes/fp_intro/#approach-2-church-encoding_1","text":"Alternatively, recursion can be encoded using the fix-pointer combinator (AKA $Y $-combinator). Let $Y $ be \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] We find that for any function \\(g\\) , we have \\(Y\\ g = g\\ (Y\\ g)\\) . We will work on the derivation during exercise. Let's try to implement the factorial function over natural numbers: \\[ \\begin{array}{cc} fac(n) = \\left [ \\begin{array}{ll} 1 & {if}~ n = 0 \\\\ n*fac(n-1) & {otherwise} \\end{array} \\right . \\end{array} \\] Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition. Let \\(Fac\\) be \\[ \\begin{array}{c} \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above.","title":"Approach 2 - Church Encoding"},{"location":"notes/fp_intro/#discussion-1","text":"How to define the following? \\(one\\) \\(iszero\\) \\(mul\\) \\(pred\\)","title":"Discussion 1"},{"location":"notes/fp_intro/#discussion-2","text":"The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)? We will work on the two topics discussed above during the cohort class.","title":"Discussion 2"},{"location":"notes/fp_intro/#summary","text":"We have covered Syntax (lambda terms) and Semantics ( \\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming). Evaluation strategies, their properties and connection to real world programming Extending lambda calculus to support conditional and loop Via language extension (we will use) Via Church encoding (fun but not very pragmatic in our context)","title":"Summary"},{"location":"notes/fp_scala/","text":"50.054 - Instroduction to Scala Learning Outcomes By the end of this class, you should be able to Develop simple implementation in Scala using List, Conditional, and Recursion Model problems and design solutions using Algebraic Datatype and Pattern Matching Compile and execute simple Scala programs What is Scala? Scala is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Scala has many backends, including JVM, node.js and native. Scala is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Scala, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Scala is used in the real-world business, you may refer to the following for further readings. Scala at Scale at Databricks Why Scala is seeing a renewed interest for developing enterprise software Who is using Scala, Akka and Play framework Type-safe Tensor Scala Hello World Let's say we have a Scala file named HelloWorld.scala println(\"hello world\") We can execute it via either scala HelloWorld.scala or to compile it then run scalac HelloWorld.scala && scala HelloWorld Although in the cohort problems, we are going to rely on a Scala project manager called sbt to build, execute and test our codes. Scala OOP vs Java OOP If you know Object Oriented Programming, you already know 70% of Scala. Consider the following Java code snippet interface FlyBehavior { void fly(); } abstract class Bird { private String species; private FlyBehavior fb; public Bird(String species, FlyBehavior fb) { this.species = species; this.fb = fb; } public String getSpecies() { return this.species; } public void fly() { return this.fb.fly(); } } class Duck extends Bird { public Duck() { super(\"Duck\", new FlyBehavior() { @override void fly() { System.out.println(\"I can't fly\"); } }) } } class BlueJay extends Bird { public BlueJay() { super(\"BlueJay\", new FlyBehavior() { @override void fly() { System.out.println(\"Swwooshh!\"); } }) } } We define an abstract class Bird which has two member attributes, species and fb . We adopt the Strategy design pattern to delegate the fly behavior of the bird through an interface FlyBehavior . Scala has the equivalence of language features as Java. The language has much concise syntax. In the following we implement the same logic in Scala. trait FlyBehavior { def fly() } abstract class Bird(species:String, fb:FlyBehavior) { def getSpecies():String = this.species def fly():Unit = this.fb.fly() } class Duck extends Bird(\"Duck\", new FlyBehavior() { override def fly() = println(\"I can't fly\") }) class BlueJay extends Bird(\"BlueJay\", new FlyBehavior() { override def fly() = println(\"Swwooshh!\") }) In Scala, we prefer inline constructors. A trait is the Scala equivalent of Java's interface. Similar to Python, methods start with def . A method's return type comes after the method name declaration. Type annotations follow their arguments instead of preceding them. Method bodies are defined after an equality sign. The return keyword is optional; the last expression will be returned as the result. The Java style of method body definition is also supported, i.e. the getSpecies() method can be defined as follows: def getSpecies():String { return this.species } Being a JVM language, Scala allows us to import and invoke Java libraries in Scala code. import java.util.LinkedList val l = new java.util.LinkedList[String]() Keyword val defines an immutable variable, and var defines a mutable variable. Functional Programming in Scala at a glance In this module, we focus and utilise mostly the functional programming feature of Scala. Lambda Calculus Scala Variable \\(x\\) x Constant \\(c\\) 1 , 2 , true , false Lambda abstraction \\(\\lambda x.t\\) (x:T) => e Function application \\(t_1\\ t_2\\) e1(e2) Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) if (e1) { e2 } else { e3 } Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) val x = e1 ; e2 Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) def f(x:Int):Int = f(x); f(1); where T denotes a type and :T denotes a type annotation. e , e1 , e2 and e3 denote expressions. Similar to other mainstream languages, defining recursion in Scala is straight-forward, we just make reference to the recursive function name in its body. def fac(x:Int):Int = { if (x == 0) { 1 } else { x*fac(x-1) } } val result = fac(10) Scala Strict and Lazy Evaluation Let f be a non-terminating function def f(x:Int):Int = f(x) The following shows that the function application in Scala is using strict evaluation. def g(x:Int):Int = 1 g(f(1)) // it does not terminate On the other hand, the following code is terminating. def h(x: => Int):Int = 1 h(f(1)) // it terminates! The type annotation : => Int after x states that the argument x is passed in by name (lazy evaluation), not by value (strict evaluation). List Data type We consider a commonly used builtin data type in Scala, the list data type. In Scala, the following define some list values. Nil - an empty list. List() - an empty list. List(1,2) - an integer list contains two values. List(\"a\") - an string list contains one value. 1::List(2,3) - prepends a value 1 to a list containing 2 and 3 . List(\"hello\") ++ List(\"world\") - concatenating two string lists. To iterate through the items in a list, we can use a for-loop: def sum(l:List[Int]):Int = { var s = 0 for (i <- l) { s = s+i } s } which is very similar to what we could implement in Java or Python. However, we are more interested in using the functional programming features in Scala: def sum(l:List[Int]):Int = { l match { case Nil => 0 case (hd::tl) => hd + sum(tl) } } in which l match {case Nil => 0; case (hd::tl) => hd+sum(tl) } denotes a pattern-matching expression in Scala. It is similar to the switch statement found in other main stream languages, except that it has more perks . In this expression, we pattern match the input list l against two list patterns, namely: Nil the empty list, and (hd::tl) the non-empty list Note that here Nil and hd::tl are not list values, because they are appearing after a case keyword and on the left of a thick arrow => . Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list l is an empty list. If it is empty, the sum of an empty list must be 0 . If the input list l is not an empty list, it must have at least one element. The pattern (hd::tl) extracts the first element of the list and binds it to a local variable hd and the remainder (which is the sub list formed by taking away the first element from l ) is bound to hd . We often call hd as the head of the list and tl as the tail. We would like to remind that hd is storing a single integer in this case, and tl is capturing a list of integers. One advantage of implementing the sum function in FP style is that it is much closer to its math specification. \\[ \\begin{array}{rl} sum(l) = & \\left [ \\begin{array}{ll} 0 & {l\\ is\\ empty} \\\\ head(l)+sum(tail(l)) & {otherwise} \\end{array} \\right . \\end{array} \\] Let's consider another example. def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The function reverse takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the match expression. When the input list l is an empty list, we return an empty list. The reverse of an empty list is an empty list When the input l is not empty, we make use of the pattern (hd::tl) to extract the head and the tail of the list We apply reverse recursively to the tail and then concatenate it with a list containing the head. You may notice that the same reverse function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the reverse function into a generic version as follows: def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Note that the first [A] denotes a type argument, with which we specify that the element type of the list is A (any possible type). The type argument is resolved when we apply reverse to a actual argument. For instance in reverse(List(1,2,3)) the Scala compiler will resolve A=Int and in reverse(List(\"a\",\"b\")) it will resolve A=String . A Note on Recursion Note that recursive calls to reverse will incur additional memory space in the machine in form of additional function call frames on the call stack. A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error. While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion. A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. For instance, the reverse() function presented earlier is not. The following variant is a tail recursion def reverse[A](l:List[A]):List[A] = { def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } In the above definition, we rely on a inner function go which is a recursive function. In go , the recursion take places at the last instruction in the (x::xs) case. The trick is to pass around an accumulated output o in each recursive call. Some compilers such as GHC can detect a tail recursive function, but it will not rewrite into a form which no stack is required. As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. However Scala does not automatically re-write a non-tail recursion into a tail recursion. Instead it offers a check: import scala.annotation.tailrec def reverse[A](l:List[A]):List[A] = { @tailrec def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } The annotation tailrec is to hint to the Scala compiler that go should be compiled in a way that no stack frame should be created. If the compiler fails to do that, it will complain. In the absence of the tailrec annotation, the compiler will still try to optimize the tail recursion. If we apply the tailrec annotation to a non-tail recursive function, Scala will complain. @tailrec def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The following error is reported: -- Error: ---------------------------------------------------------------------- 4 | case (hd::tl) => reverse(tl) ++ List(hd) | ^^^^^^^^^^^ | Cannot rewrite recursive call: it is not in tail position 1 error found Map, Fold and Filter Consider the following function def addToEach(x:Int, l:List[Int]):List[Int] = l match { case Nil => Nil case (y::ys) => { val yx = y+x yx::addToEach(x,ys) } } It takes two inputs, an integer x and an integer list l , and adds x to every element in l and put the results in the output list. For instance addToEach(1, List(1,2,3)) yields List(2,3,4) . The above can rewritten by using a generic library method shipped with Scala. def addToEach(x:Int, l:List[Int]):List[Int] = l.map(y=>y+x) The method map is a method of the list class that takes an function as input argument and applies it to all elements in the list object. Note that the above is same as def addToEach(x:Int, l:List[Int]):List[Int] = { def addX(y:Int):Int = y+x l.map(addX) } We can observe that the input list and the output list of the map method must be of the same type and have the same length. Recall in the sum function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function. def sum(l:List[Int]):Int = l.foldLeft(0)((acc,x)=> acc+x) The foldLeft method takes a base accumulator, and a binary function as inputs, and aggregates the elements from the list using the binary function. In particular, the binary aggreation function assumes the first argument is the accumulator. Besides foldLeft , there exists a foldRight method, in which the binary aggregation function expects the second argument is the accumulator. def sum(l:List[Int]):Int = l.foldRight(0)((x,acc)=> x+acc) So what is the difference between foldLeft and foldRight ? What happen if you run the following? Can you explain the difference? val l = List(\"a\",\"better\",\"world\", \"by\", \"design\") l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) Note that + is an overloaded operator. In the above it concatenates two string values. Intuitively, l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) aggregates the list of words using the aggregation function by nesting the recursive calls to the left. ((((\"\"+\" \"+\"a\")+\" \"+\"better\")+\" \"+\"world\")+\" \"+\"by\")+\" \"+\"design\" where l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) aggregates the list of words by nesting the recursive calls to the right. \"a\"+\" \"+(\"better\"+\" \"+(\"world\"+\" \"+(\"by\"+\" \"+(\"design\"+\" \"+\"\")))) The method filter takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false. val l = List(1,2,3,4) def even(x:Int):Boolean = x%2==0 l.filter(even) returns List(2,4) . val l = List('a','1','0','d') l.filter((c:Char) => c.isDigit) returns List('1','0') . With map , foldLeft and filter , we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm: def qsort(l:List[Int]):List[Int] = l match { case Nil => Nil case List(x) => List(x) case (p::rest) => { val ltp = rest.filter( x => x < p) val gep = rest.filter( x => !(x < p)) qsort(ltp) ++ List(p) ++ qsort(gep) } } which resembles the math specification \\[ \\begin{array}{cc} qsort(l) = & \\left[ \\begin{array}{ll} l & |l| < 2 \\\\ qsort(\\{x|x \\in l \\wedge x < head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x < head(l)) \\}) & otherwise \\end{array} \\right . \\end{array} \\] where \\(\\uplus\\) unions two bags and maintains the order. flatMap and for-comprehension There is a variant of map method, consider val l = (1 to 5).toList l.map( i => if (i%2 ==0) { List(i) } else { Nil }) would yield List(List(), List(2), List(), List(4), List()) We would like to get rid of the nested lists and flatten the outer list. One possibility is to: l.flatMap( i => if (i%2 ==0) { List(i) } else { Nil }) Like map , flatMap applies its parameter function to every element in the list. Unlike map , flatMap expects the parameter function produces a list, thus it will join all the sub-lists into one list. With map and flatMap , we can define complex list transformation operations like the following: def listProd[A,B](la:List[A], lb:List[B]):List[(A,B)] = la.flatMap( a => lb.map(b => (a,b))) val l2 = List('a', 'b', 'c') listProd(l, l2) which produces: List((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (4,a), (4,b), (4,c), (5,a), (5,b), (5,c)) Note that Scala supports list comprehension via the for ... yield construct. We could re-express listProd as follows: def listProd2[A,B](la:List[A], lb:List[B]):List[(A,B)] = for { a <- la b <- lb } yield (a,b) The Scala compiler desugars: for { x1 <- e1; x2 <- e2; ...; xn <- en } yield e ```` into: ```scala e1.flatMap( x1 => e2.flatMap(x2 => .... en.map( xn => e) ...)) The above syntactic sugar not only works for the list data type but any data type with flatMap and map defined (as we will see in the upcoming lessons). In its general form, we refer to it as for-comprehension . One extra note to take is that the for-comprehension should not be confused with the for-loop statement exists in the imperative style programming in Scala. var sum = 0 for (i <- 1 to 10) {sum = sum + i} println(sum) The Algebraic Datatype Like many other languages, Scala supports user defined data type. From an earlier section, we have discussed how to use classes and traits in Scala to define data types, making using of the OOP concepts that we have learned. This style of defining data types using abstraction and encapsulation is also known as the abstract datatype. In this section, we consider an alternative, the Algebraic Datatype. Consider the following EBNF of a math expression. \\[ \\begin{array}{rccl} {\\tt (Math Exp)} & e & ::= & e + e \\mid e - e \\mid e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} & c & ::= & ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] And we would like to implement a function eval() which evaluates a \\({\\tt (Math Exp)}\\) to a value. If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\) , and use if-else statements with instanceof to check for a specific subclass instance. Alternative, we can also rely on visitor pattern or delegation. It turns out that using Abstract Datatypes to model the above result in some engineering overhead. Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms) Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values) For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\) The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums. In Scala 3, it is recommended to use enum for Algebraic datatypes. enum MathExp: case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) end MathExp In the above the MathExp ( enum ) datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance Plus(e1:MathExp, e2:MathExp) , which states that a plus expression has two operands, both of which are of type MathExp . Note that the end MathExp is optional, as long as there is an extra line. Alternatively, we can use { } . enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } We can represent the math expression (1+2) * 3 as MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3)) . Note that we call Plus(_,_) , Minus(_,_) , Mult(_,_) , Div(_,_) and Const(_) \"data constructors\", as we use them to construct values of the enum algebraic datatype MathExp . Next let's implement an evaluation function based the specification: \\[ eval(e) = \\left [ \\begin{array}{cl} eval(e_1) + eval(e_2) & if\\ e = e_1+e_2 \\\\ eval(e_1) - eval(e_2) & if\\ e = e_1-e_2 \\\\ eval(e_1) * eval(e_2) & if\\ e = e_1*e_2 \\\\ eval(e_1) / eval(e_2) & if\\ e = e_1/e_2 \\\\ c & if\\ e = c \\end{array} \\right. \\] def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } In Scala, the `enum`` Algebraic datatype can be accessed (destructured) via pattern matching. If we run: eval(MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3))) we get 9 as result. Let's consider another example where we can implement some real-world data structures using the algebraic datatype. Suppose for experimental purposes, we would like to re-implement the list datatype in Scala (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. We will look into the generic version in the next lesson In the following we consider the specification of the MyList data type in EBNF: \\[ \\begin{array}{rccl} {\\tt (MyList)} & l & ::= & Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} & i & ::= & 1 \\mid 2 \\mid ... \\end{array} \\] And we implement using enum in Scala: enum MyList { case Nil case Cons(x:Int, xs:MyList) } Next we implement the map function based on the following specification \\[ map(f, l) = \\left [ \\begin{array}{ll} Nil & if\\ l = Nil\\\\ Cons(f(hd), map(f, tl)) & if\\ l = Cons(hd, tl) \\end{array} \\right . \\] Then we could implement the map function def mapML(f:Int=>Int, l:MyList):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(f,tl)) } Running mapML(x => x+1, MyList.Cons(1,MyList.Nil)) yields MyList.Cons(2,MyList.Nil) . But hang on a second! The map method from the Scala built-in list is a method of a list object, not a stand-alone function. In Scala 3, enum allows us to package the method inside enum values. enum MyList { case Nil case Cons(x:Int, xs:MyList) def mapML(f:Int=>Int):MyList = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Running: val l = MyList.Cons(1, MyList.Nil) l.mapML(x=> x+1) yields the same output as above. Summary In this lesson, we have discussed Scala's OOP vs Java's OOP Scala's FP vs Lambda Calculus How to use the List datatype to model and manipulate collections of multiple values. How to use the Algebraic data type to define user customized data type to solve complex problems.","title":"50.054 - Instroduction to Scala"},{"location":"notes/fp_scala/#50054-instroduction-to-scala","text":"","title":"50.054 - Instroduction to Scala"},{"location":"notes/fp_scala/#learning-outcomes","text":"By the end of this class, you should be able to Develop simple implementation in Scala using List, Conditional, and Recursion Model problems and design solutions using Algebraic Datatype and Pattern Matching Compile and execute simple Scala programs","title":"Learning Outcomes"},{"location":"notes/fp_scala/#what-is-scala","text":"Scala is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Scala has many backends, including JVM, node.js and native. Scala is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Scala, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Scala is used in the real-world business, you may refer to the following for further readings. Scala at Scale at Databricks Why Scala is seeing a renewed interest for developing enterprise software Who is using Scala, Akka and Play framework Type-safe Tensor","title":"What is Scala?"},{"location":"notes/fp_scala/#scala-hello-world","text":"Let's say we have a Scala file named HelloWorld.scala println(\"hello world\") We can execute it via either scala HelloWorld.scala or to compile it then run scalac HelloWorld.scala && scala HelloWorld Although in the cohort problems, we are going to rely on a Scala project manager called sbt to build, execute and test our codes.","title":"Scala Hello World"},{"location":"notes/fp_scala/#scala-oop-vs-java-oop","text":"If you know Object Oriented Programming, you already know 70% of Scala. Consider the following Java code snippet interface FlyBehavior { void fly(); } abstract class Bird { private String species; private FlyBehavior fb; public Bird(String species, FlyBehavior fb) { this.species = species; this.fb = fb; } public String getSpecies() { return this.species; } public void fly() { return this.fb.fly(); } } class Duck extends Bird { public Duck() { super(\"Duck\", new FlyBehavior() { @override void fly() { System.out.println(\"I can't fly\"); } }) } } class BlueJay extends Bird { public BlueJay() { super(\"BlueJay\", new FlyBehavior() { @override void fly() { System.out.println(\"Swwooshh!\"); } }) } } We define an abstract class Bird which has two member attributes, species and fb . We adopt the Strategy design pattern to delegate the fly behavior of the bird through an interface FlyBehavior . Scala has the equivalence of language features as Java. The language has much concise syntax. In the following we implement the same logic in Scala. trait FlyBehavior { def fly() } abstract class Bird(species:String, fb:FlyBehavior) { def getSpecies():String = this.species def fly():Unit = this.fb.fly() } class Duck extends Bird(\"Duck\", new FlyBehavior() { override def fly() = println(\"I can't fly\") }) class BlueJay extends Bird(\"BlueJay\", new FlyBehavior() { override def fly() = println(\"Swwooshh!\") }) In Scala, we prefer inline constructors. A trait is the Scala equivalent of Java's interface. Similar to Python, methods start with def . A method's return type comes after the method name declaration. Type annotations follow their arguments instead of preceding them. Method bodies are defined after an equality sign. The return keyword is optional; the last expression will be returned as the result. The Java style of method body definition is also supported, i.e. the getSpecies() method can be defined as follows: def getSpecies():String { return this.species } Being a JVM language, Scala allows us to import and invoke Java libraries in Scala code. import java.util.LinkedList val l = new java.util.LinkedList[String]() Keyword val defines an immutable variable, and var defines a mutable variable.","title":"Scala OOP vs Java OOP"},{"location":"notes/fp_scala/#functional-programming-in-scala-at-a-glance","text":"In this module, we focus and utilise mostly the functional programming feature of Scala. Lambda Calculus Scala Variable \\(x\\) x Constant \\(c\\) 1 , 2 , true , false Lambda abstraction \\(\\lambda x.t\\) (x:T) => e Function application \\(t_1\\ t_2\\) e1(e2) Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) if (e1) { e2 } else { e3 } Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) val x = e1 ; e2 Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) def f(x:Int):Int = f(x); f(1); where T denotes a type and :T denotes a type annotation. e , e1 , e2 and e3 denote expressions. Similar to other mainstream languages, defining recursion in Scala is straight-forward, we just make reference to the recursive function name in its body. def fac(x:Int):Int = { if (x == 0) { 1 } else { x*fac(x-1) } } val result = fac(10)","title":"Functional Programming in Scala at a glance"},{"location":"notes/fp_scala/#scala-strict-and-lazy-evaluation","text":"Let f be a non-terminating function def f(x:Int):Int = f(x) The following shows that the function application in Scala is using strict evaluation. def g(x:Int):Int = 1 g(f(1)) // it does not terminate On the other hand, the following code is terminating. def h(x: => Int):Int = 1 h(f(1)) // it terminates! The type annotation : => Int after x states that the argument x is passed in by name (lazy evaluation), not by value (strict evaluation).","title":"Scala Strict and Lazy Evaluation"},{"location":"notes/fp_scala/#list-data-type","text":"We consider a commonly used builtin data type in Scala, the list data type. In Scala, the following define some list values. Nil - an empty list. List() - an empty list. List(1,2) - an integer list contains two values. List(\"a\") - an string list contains one value. 1::List(2,3) - prepends a value 1 to a list containing 2 and 3 . List(\"hello\") ++ List(\"world\") - concatenating two string lists. To iterate through the items in a list, we can use a for-loop: def sum(l:List[Int]):Int = { var s = 0 for (i <- l) { s = s+i } s } which is very similar to what we could implement in Java or Python. However, we are more interested in using the functional programming features in Scala: def sum(l:List[Int]):Int = { l match { case Nil => 0 case (hd::tl) => hd + sum(tl) } } in which l match {case Nil => 0; case (hd::tl) => hd+sum(tl) } denotes a pattern-matching expression in Scala. It is similar to the switch statement found in other main stream languages, except that it has more perks . In this expression, we pattern match the input list l against two list patterns, namely: Nil the empty list, and (hd::tl) the non-empty list Note that here Nil and hd::tl are not list values, because they are appearing after a case keyword and on the left of a thick arrow => . Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list l is an empty list. If it is empty, the sum of an empty list must be 0 . If the input list l is not an empty list, it must have at least one element. The pattern (hd::tl) extracts the first element of the list and binds it to a local variable hd and the remainder (which is the sub list formed by taking away the first element from l ) is bound to hd . We often call hd as the head of the list and tl as the tail. We would like to remind that hd is storing a single integer in this case, and tl is capturing a list of integers. One advantage of implementing the sum function in FP style is that it is much closer to its math specification. \\[ \\begin{array}{rl} sum(l) = & \\left [ \\begin{array}{ll} 0 & {l\\ is\\ empty} \\\\ head(l)+sum(tail(l)) & {otherwise} \\end{array} \\right . \\end{array} \\] Let's consider another example. def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The function reverse takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the match expression. When the input list l is an empty list, we return an empty list. The reverse of an empty list is an empty list When the input l is not empty, we make use of the pattern (hd::tl) to extract the head and the tail of the list We apply reverse recursively to the tail and then concatenate it with a list containing the head. You may notice that the same reverse function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the reverse function into a generic version as follows: def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Note that the first [A] denotes a type argument, with which we specify that the element type of the list is A (any possible type). The type argument is resolved when we apply reverse to a actual argument. For instance in reverse(List(1,2,3)) the Scala compiler will resolve A=Int and in reverse(List(\"a\",\"b\")) it will resolve A=String .","title":"List Data type"},{"location":"notes/fp_scala/#a-note-on-recursion","text":"Note that recursive calls to reverse will incur additional memory space in the machine in form of additional function call frames on the call stack. A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error. While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion. A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. For instance, the reverse() function presented earlier is not. The following variant is a tail recursion def reverse[A](l:List[A]):List[A] = { def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } In the above definition, we rely on a inner function go which is a recursive function. In go , the recursion take places at the last instruction in the (x::xs) case. The trick is to pass around an accumulated output o in each recursive call. Some compilers such as GHC can detect a tail recursive function, but it will not rewrite into a form which no stack is required. As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. However Scala does not automatically re-write a non-tail recursion into a tail recursion. Instead it offers a check: import scala.annotation.tailrec def reverse[A](l:List[A]):List[A] = { @tailrec def go(i:List[A], o:List[A]) : List[A] = i match { case Nil => o case (x::xs) => go(xs, x::o) } go(l,Nil) } The annotation tailrec is to hint to the Scala compiler that go should be compiled in a way that no stack frame should be created. If the compiler fails to do that, it will complain. In the absence of the tailrec annotation, the compiler will still try to optimize the tail recursion. If we apply the tailrec annotation to a non-tail recursive function, Scala will complain. @tailrec def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } The following error is reported: -- Error: ---------------------------------------------------------------------- 4 | case (hd::tl) => reverse(tl) ++ List(hd) | ^^^^^^^^^^^ | Cannot rewrite recursive call: it is not in tail position 1 error found","title":"A Note on Recursion"},{"location":"notes/fp_scala/#map-fold-and-filter","text":"Consider the following function def addToEach(x:Int, l:List[Int]):List[Int] = l match { case Nil => Nil case (y::ys) => { val yx = y+x yx::addToEach(x,ys) } } It takes two inputs, an integer x and an integer list l , and adds x to every element in l and put the results in the output list. For instance addToEach(1, List(1,2,3)) yields List(2,3,4) . The above can rewritten by using a generic library method shipped with Scala. def addToEach(x:Int, l:List[Int]):List[Int] = l.map(y=>y+x) The method map is a method of the list class that takes an function as input argument and applies it to all elements in the list object. Note that the above is same as def addToEach(x:Int, l:List[Int]):List[Int] = { def addX(y:Int):Int = y+x l.map(addX) } We can observe that the input list and the output list of the map method must be of the same type and have the same length. Recall in the sum function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function. def sum(l:List[Int]):Int = l.foldLeft(0)((acc,x)=> acc+x) The foldLeft method takes a base accumulator, and a binary function as inputs, and aggregates the elements from the list using the binary function. In particular, the binary aggreation function assumes the first argument is the accumulator. Besides foldLeft , there exists a foldRight method, in which the binary aggregation function expects the second argument is the accumulator. def sum(l:List[Int]):Int = l.foldRight(0)((x,acc)=> x+acc) So what is the difference between foldLeft and foldRight ? What happen if you run the following? Can you explain the difference? val l = List(\"a\",\"better\",\"world\", \"by\", \"design\") l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) Note that + is an overloaded operator. In the above it concatenates two string values. Intuitively, l.foldLeft(\"\")((acc,x) => (acc+\" \"+x)) aggregates the list of words using the aggregation function by nesting the recursive calls to the left. ((((\"\"+\" \"+\"a\")+\" \"+\"better\")+\" \"+\"world\")+\" \"+\"by\")+\" \"+\"design\" where l.foldRight(\"\")((x,acc) => (x+\" \"+acc)) aggregates the list of words by nesting the recursive calls to the right. \"a\"+\" \"+(\"better\"+\" \"+(\"world\"+\" \"+(\"by\"+\" \"+(\"design\"+\" \"+\"\")))) The method filter takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false. val l = List(1,2,3,4) def even(x:Int):Boolean = x%2==0 l.filter(even) returns List(2,4) . val l = List('a','1','0','d') l.filter((c:Char) => c.isDigit) returns List('1','0') . With map , foldLeft and filter , we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm: def qsort(l:List[Int]):List[Int] = l match { case Nil => Nil case List(x) => List(x) case (p::rest) => { val ltp = rest.filter( x => x < p) val gep = rest.filter( x => !(x < p)) qsort(ltp) ++ List(p) ++ qsort(gep) } } which resembles the math specification \\[ \\begin{array}{cc} qsort(l) = & \\left[ \\begin{array}{ll} l & |l| < 2 \\\\ qsort(\\{x|x \\in l \\wedge x < head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x < head(l)) \\}) & otherwise \\end{array} \\right . \\end{array} \\] where \\(\\uplus\\) unions two bags and maintains the order.","title":"Map, Fold and Filter"},{"location":"notes/fp_scala/#flatmap-and-for-comprehension","text":"There is a variant of map method, consider val l = (1 to 5).toList l.map( i => if (i%2 ==0) { List(i) } else { Nil }) would yield List(List(), List(2), List(), List(4), List()) We would like to get rid of the nested lists and flatten the outer list. One possibility is to: l.flatMap( i => if (i%2 ==0) { List(i) } else { Nil }) Like map , flatMap applies its parameter function to every element in the list. Unlike map , flatMap expects the parameter function produces a list, thus it will join all the sub-lists into one list. With map and flatMap , we can define complex list transformation operations like the following: def listProd[A,B](la:List[A], lb:List[B]):List[(A,B)] = la.flatMap( a => lb.map(b => (a,b))) val l2 = List('a', 'b', 'c') listProd(l, l2) which produces: List((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (4,a), (4,b), (4,c), (5,a), (5,b), (5,c)) Note that Scala supports list comprehension via the for ... yield construct. We could re-express listProd as follows: def listProd2[A,B](la:List[A], lb:List[B]):List[(A,B)] = for { a <- la b <- lb } yield (a,b) The Scala compiler desugars: for { x1 <- e1; x2 <- e2; ...; xn <- en } yield e ```` into: ```scala e1.flatMap( x1 => e2.flatMap(x2 => .... en.map( xn => e) ...)) The above syntactic sugar not only works for the list data type but any data type with flatMap and map defined (as we will see in the upcoming lessons). In its general form, we refer to it as for-comprehension . One extra note to take is that the for-comprehension should not be confused with the for-loop statement exists in the imperative style programming in Scala. var sum = 0 for (i <- 1 to 10) {sum = sum + i} println(sum)","title":"flatMap and for-comprehension"},{"location":"notes/fp_scala/#the-algebraic-datatype","text":"Like many other languages, Scala supports user defined data type. From an earlier section, we have discussed how to use classes and traits in Scala to define data types, making using of the OOP concepts that we have learned. This style of defining data types using abstraction and encapsulation is also known as the abstract datatype. In this section, we consider an alternative, the Algebraic Datatype. Consider the following EBNF of a math expression. \\[ \\begin{array}{rccl} {\\tt (Math Exp)} & e & ::= & e + e \\mid e - e \\mid e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} & c & ::= & ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] And we would like to implement a function eval() which evaluates a \\({\\tt (Math Exp)}\\) to a value. If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\) , and use if-else statements with instanceof to check for a specific subclass instance. Alternative, we can also rely on visitor pattern or delegation. It turns out that using Abstract Datatypes to model the above result in some engineering overhead. Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms) Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values) For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\) The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums. In Scala 3, it is recommended to use enum for Algebraic datatypes. enum MathExp: case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) end MathExp In the above the MathExp ( enum ) datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance Plus(e1:MathExp, e2:MathExp) , which states that a plus expression has two operands, both of which are of type MathExp . Note that the end MathExp is optional, as long as there is an extra line. Alternatively, we can use { } . enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } We can represent the math expression (1+2) * 3 as MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3)) . Note that we call Plus(_,_) , Minus(_,_) , Mult(_,_) , Div(_,_) and Const(_) \"data constructors\", as we use them to construct values of the enum algebraic datatype MathExp . Next let's implement an evaluation function based the specification: \\[ eval(e) = \\left [ \\begin{array}{cl} eval(e_1) + eval(e_2) & if\\ e = e_1+e_2 \\\\ eval(e_1) - eval(e_2) & if\\ e = e_1-e_2 \\\\ eval(e_1) * eval(e_2) & if\\ e = e_1*e_2 \\\\ eval(e_1) / eval(e_2) & if\\ e = e_1/e_2 \\\\ c & if\\ e = c \\end{array} \\right. \\] def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } In Scala, the `enum`` Algebraic datatype can be accessed (destructured) via pattern matching. If we run: eval(MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3))) we get 9 as result. Let's consider another example where we can implement some real-world data structures using the algebraic datatype. Suppose for experimental purposes, we would like to re-implement the list datatype in Scala (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. We will look into the generic version in the next lesson In the following we consider the specification of the MyList data type in EBNF: \\[ \\begin{array}{rccl} {\\tt (MyList)} & l & ::= & Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} & i & ::= & 1 \\mid 2 \\mid ... \\end{array} \\] And we implement using enum in Scala: enum MyList { case Nil case Cons(x:Int, xs:MyList) } Next we implement the map function based on the following specification \\[ map(f, l) = \\left [ \\begin{array}{ll} Nil & if\\ l = Nil\\\\ Cons(f(hd), map(f, tl)) & if\\ l = Cons(hd, tl) \\end{array} \\right . \\] Then we could implement the map function def mapML(f:Int=>Int, l:MyList):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(f,tl)) } Running mapML(x => x+1, MyList.Cons(1,MyList.Nil)) yields MyList.Cons(2,MyList.Nil) . But hang on a second! The map method from the Scala built-in list is a method of a list object, not a stand-alone function. In Scala 3, enum allows us to package the method inside enum values. enum MyList { case Nil case Cons(x:Int, xs:MyList) def mapML(f:Int=>Int):MyList = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Running: val l = MyList.Cons(1, MyList.Nil) l.mapML(x=> x+1) yields the same output as above.","title":"The Algebraic Datatype"},{"location":"notes/fp_scala/#summary","text":"In this lesson, we have discussed Scala's OOP vs Java's OOP Scala's FP vs Lambda Calculus How to use the List datatype to model and manipulate collections of multiple values. How to use the Algebraic data type to define user customized data type to solve complex problems.","title":"Summary"},{"location":"notes/fp_scala_poly/","text":"50.054 - Parametric Polymorphism and Adhoc Polymorphism Learning Outcomes By this end of this lesson, you should be able to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes develop generic programming style code using Functor type class. make use of Option and Either to handle and manipulate errors and exceptions. Currying In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments. For example, def sum(x:Int, y:Int):Int = x + y can be rewritten into def sum_curry(x:Int)(y:Int):Int = x + y These two functions are equivalent except that Their invocations are different, e.g. sum(1,2) sum_curry(1)(2) It is easier to reuse the curried version to define other function, e.g. def plus1(x:Int):Int = sum_curry(1)(x) Function Composition Every function and method in Scala is an object with a .compose() method. It works like the mathmethical composition. In math, let \\(g\\) and \\(f\\) be functions, then \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] Let g and f be Scala functions (or methods), then g.compose(f) is equivalent to x => g(f(x)) For example def f(x:Int):Int = 2 * x + 3 def g(x:Int):Int = x * x assert((g.compose(f))(2) == g(f(2))) Generics Generics is also known as type variables. It enables a language to support parametric polymoprhism. Polymorphic functions Recall that the reverse function introduced in the last lesson def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace Int by a type variable A . def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } Polymorphic Algebraic Datatype Recall that the following Algebraic Datatype from the last lesson. enum MyList { case Nil case Cons(x:Int, xs:MyList) } def mapML(l:MyList, f:Int => Int):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl, f)) } Same observation applies. MyList could have a generic element type A instead of Int and mapML should remains unchanged. enum MyList[A] { case Nil // type error case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A], f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML( tl, f)) } The caveat here is that the Scala compiler would complain about the Nil case above -- Error: ---------------------------------------------------------------------- 2 | case Nil | ^^^^^^^^ | cannot determine type argument for enum parent class MyList, | type parameter type A is invariant 1 error found To understand that error, we need to understand how Scala desugar the enum datatype. The above MyList datatype is desugared as enum MyList[A] { case Nil extends MyList[Nothing] // type error case Cons(x:A, xs:MyList[A]) extends MyList[A] } In which all sub cases within the enum type must be sub-class of the enum type. However it is not trivial for Nil . It can't be declared as a subtype of MyList[A] since type variable A is not mentioned in its definition, unlike Cons(x:A, xs:MyList[A]) . The best we can get is MyList[Nothing] where Nothing is the subtype of all other types in Scala. (As the dual, Any is the supertype of all other types in Scala). We are getting very close. Now we know that Nil extends MyList[Nothing] . If we can argue that MyList[Nothing] extends MyList[A] then we are all good. For MyList[Nothing] extends MyList[A] to hold, A must be covariant type parameter. In type system with subtyping, a type is covariant if it preserves the subtyping order when it is applied a type constructor. In the above situation, MyList[_] is a type constructor. The type parameter A is covarient because we note Nothing <: A for all A , thus MyList[Nothing] <: MyList[A] . a type is contravariant if it reverses the subtyping order when it is applied to a type constructor. For instance, given function type A => Boolean , the type parameter A is contravariant, because for A <: B , we have B => Boolean <: A => Boolean . (We can use functions of type B => Boolean in the context where a function A => Boolean is expected, but not the other way round.) a type is invariant if it does not preserve nor reverse the subtyping order when it is applied to a type constructor. Hence to fix the above type error with the MyList[A] datatype, we declared that A is covariant, +A . enum MyList[+A] { case Nil // type error is fixed. case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A])(f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl)(f)) } For easy of reasoning, we also rewrite mapML into currying style. Recall that we could make mapML function as a method of MyList enum MyList[+A] { case Nil case Cons(x:A, xs:MyList[A]) def mapML[B](f:A=>B):MyList[B] = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Scala Variances Type class Suppose we would like to convert some of the Scala value to JSON string. We could rely on overloading. def toJS(v:Int):String = v.toString def toJS(v:String):String = s\"'${v}'\" def toJS(v:Boolean):String = v.toString Given v is a Scala string value, s\"some_prefix ${v} some_suffix\" denotes a Scala string interpolation, which inserts v 's content into the \"place holder\" in the string \"some_prefix ${v} some_suffix\" where the ${v} is the place holder. However this becomes hard to manage as we consider complex datatype. enum Contact { case Email(e:String) case Phone(ph:String) } import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${toJS(e)}\" // compilation error case Phone(ph) => s\"'phone': ${toJS(ph)}\" // compilation error } When we try to define the toJS function for Contact datatype, we can't make use of the toJS function for string value because the compiler is confused that we are trying to make recursive calls. This is the first issue we faced. Let's pretend that the first issue has been addressed. There's still another issue. Consider case class Person(name:String, contacts:List[Contact]) case class Team(members:List[Person]) A case class is like a normal class we have seen earlier except that we can apply pattern matching to its values. Let's continue to overload toJS to handle Person and Team . def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${toJS(name)}, 'contacts':${toJS(contacts)} }\" } def toJS(cs:List[Contact]):String = { val j = cs.map(c=>toJS(c)).mkString(\",\") s\"[${j}]\" } def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${toJS(members)} }\" } def toJS(ps:List[Person]):String = { val j = ps.map(p=>toJS(p)).mkString(\",\") s\"[${j}]\" } The second issue is that the toJS(cs:List[Contact]) and toJS(ps:List[Person]) are the identical modulo the variable names. Can we combine two into one? def toJS[A](vs:List[A]):String = { val j = vs.map(v=>toJS(v)).mkString(\",\") // compiler error s\"[${j}]\" } However a compilation error occurs because the compiler is unable to resolve the toJS[A](v:A) used in the .map() . It seems that we need to give some extra information to the compiler so that it knows that when we use the above generic toJS we are referring to either Person or Contact , or whatever type that has a toJS defined. One solution to address the two above issues is to use type class . In Scala 3, a type class is defined by a polymoprhic trait and a set of type class instances. trait JS[A] { def toJS(v:A):String } given toJSInt:JS[Int] = new JS[Int]{ def toJS(v:Int):String = v.toString } given toJSString:JS[String] = new JS[String] { def toJS(v:String):String = s\"'${v}'\" } given toJSBoolean:JS[Boolean] = new JS[Boolean] { def toJS(v:Boolean):String = v.toString } given toJSContact(using jsstr:JS[String]):JS[Contact] = new JS[Contact] { import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${jsstr.toJS(e)}\" // compilation error is fixed case Phone(ph) => s\"'phone': ${jsstr.toJS(ph)}\" // compilation error is fixed } } given toJSPerson(using jsstr:JS[String], jsl:JS[List[Contact]]):JS[Person] = new JS[Person] { def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${jsstr.toJS(name)}, 'contacts':${jsl.toJS(contacts)} }\" } } given toJSTeam(using jsl:JS[List[Person]]):JS[Team] = new JS[Team] { def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${jsl.toJS(members)} }\" } } given toJSList[A](using jsa:JS[A]):JS[List[A]] = new JS[List[A]] { def toJS(as:List[A]):String = { val j = as.map(a=>jsa.toJS(a)).mkString(\",\") s\"[${j}]\" } } given defines a type class instance. An instance consists of a name and the context parameters (those with using ) and instance type. In the body of the type class instance, we instantiate an anonymous object that extends type class with the specific type and provide the defintion. We can refer to the particular type class instance by the instance's name. For instance import Contact.* val myTeam = Team( List( Person(\"kenny\", List(Email(\"kenny_lu@sutd.edu.sg\"))), Person(\"simon\", List(Email(\"simon_perrault@sutd.edu.sg\"))) )) toJSTeam.toJS(myTeam) yields 'team':{ 'members':['person':{ 'name':'kenny', 'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon', 'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] } We can also refer to the type class instance by the instace's type. For example, recall the last two instances. In the context of the toJSTeam , we refer to another instance of type JS[List[Person]] . Note that none of the defined instances has the required type. Scala is smart enought to synthesize it from the instances of toJSList and toJSPerson . Given the required type class instance is JS[List[Person]] , the type class resolver finds the instance toJSList having type JS[List[A]] , and it unifies both and find that A=Person . In the context of the instance toJSList , JS[A] is demanded. We can refine the required instance's type as JS[Person] , which is toJSPerson . Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. def printAsJSON[A](v:A)(using jsa:JS[A]):Unit = { println(jsa.toJS(v)) } printAsJSON(myTeam) Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming . In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects. In the next few section, we consider some common patterns in FP that are promoting generic programming. Functor Recall that we have a map method for list datatype. val l = List(1,2,3) l.map(x => x + 1) Can we make map to work for other data type? For example enum BTree[+A] { case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } It turns out that extending map to different datatypes is similar to toJS function that we implemented earlier. We consider introducing a type class for this purpose. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } In the above type class definition, T[_] denotes a polymorphic type that of kind * => * . A kind is a type of types. In the above, it means Functor takes any type constructors T . When T is instantiated, it could be List[_] or BTree[_] and etc. (C.f. In the type class JS[A] , the type argument has kind * .) given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } Some example val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1) Functor Laws All instances of functor must obey a set of mathematic laws for their computation to be predictable. Let i be a functor instance 1. Identity: i => map(i)(x => x) \\(\\equiv\\) x => x . When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: i=> map(i)(f.compose(g)) \\(\\equiv\\) (i => map(i)(f)).compose(j => map(j)(g)) . If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second. Foldable Similarly we can define a Foldable type class for generic and overloaded foldLeft ( and foldRight ). trait Foldable[T[_]]{ def foldLeft[A,B](t:T[B])(acc:A)(f:(A,B)=>A):A } given listFoldable:Foldable[List] = new Foldable[List] { def foldLeft[A,B](t:List[B])(acc:A)(f:(A,B)=>A):A = t.foldLeft(acc)(f) } given btreeFoldable:Foldable[BTree] = new Foldable[BTree] { import BTree.* def foldLeft[A,B](t:BTree[B])(acc:A)(f:(A,B)=>A):A = t match { case Empty => acc case Node(v, lft, rgt) => { val acc1 = f(acc,v) val acc2 = foldLeft(lft)(acc1)(f) foldLeft(rgt)(acc2)(f) } } } listFoldable.foldLeft(l)(0)((x:Int,y:Int) => x + y) btreeFoldable.foldLeft(t)(0)((x:Int,y:Int) => x + y) Option and Either Recall in the earlier lesson, we encountered the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } An error occurs when we try to evalue a MathExp which contains a division by zero sub-expression. Executing import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) yields java.lang.ArithmeticException: / by zero at rs$line$2$.eval(rs$line$2:5) ... 41 elided Like other main stream languages, we could use try-catch statement to handle the exception. try { import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) } catch { case e:java.lang.ArithmeticException => println(\"handinging div by zero\") } One downside of this approach is that at compile type it is hard to track the unhandled exceptions, (in particular with the presence of Java unchecked exceptions.) A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes). Consider the following builtin Scala datatype Option // no need to run this. enum Option[+A] { case None case Some(v:A) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } When we execute eval(Div(Const(1), Minus(Const(2), Const(2)))) , we get None as the result instead of the exception. One advantage of this is that whoever is using eval function has to respect that its return type is Option[Int] instead of just Int therefore, a match must be applied before using the result to look out for potential None value. There are still two drawbacks. Firstly, the updated version of the eval function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue. We could instead of using Option , use the Either datatype // no need to run this, it's builtin enum Either[+A, +B] { case Left(v:A) case Right(v:B) } type ErrMsg = String def eval(e: MathExp): Either[ErrMsg,Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(0) => Left(s\"div by zero caused by ${e.toString}\") case Right(v2) => Right(v1 / v2) } } case MathExp.Const(i) => Right(i) } Executing eval(Div(Const(1), Minus(Const(2), Const(2)))) will yield Left(div by zero caused by Div(Const(1),Minus(Const(2),Const(2)))) Summary In this lesson, we have discussed how to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes how to develop generic programming style code using Functor type class. how to make use of Option and Either to handle and manipulate errors and exceptions. Appendix Generalized Algebraic Data Type Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example. Firstly, we need some type acrobatics to encode nature numbers on the level of type. enum Zero { case Zero } enum Succ[A] { case Succ(v:A) } Next we define our GADT SList[S,A] which is a generic list of elements A and with size S . enum SList[S,+A] { case Nil extends SList[Zero,Nothing] // additional type constraint S = Zero case Cons[N,A](hd:A, tl:SList[N,A]) extends SList[Succ[N],A] // add'n type constraint S = Succ[N] } In the first subcase Nil , it is declared with the type of SList[Zero, Nothing] which indicates on type level that the list is empty. In the second case Cons , we define it to have the type SList[Succ[N],A] for some natural number N . This indicates on the type level that the list is non-empty. Having these information lifted to the type level allows us to define a type safe head function. import SList.* def head[A,N](sl:SList[Succ[N],A]):A = sl match { case Cons(hd, tl) => hd } Compiling head(Nil) yields a type error. Similarly we can define a size-aware function snoc which add an element at the tail of a list. def snoc[A,N](v:A, sl:SList[N,A]):SList[Succ[N],A] = sl match { case Nil => Cons(v,Nil) // case Cons(hd, tl) => snoc(v, tl) will result in compilation error. case Cons(hd, tl) => Cons(hd, snoc(v, tl)) }","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism"},{"location":"notes/fp_scala_poly/#50054-parametric-polymorphism-and-adhoc-polymorphism","text":"","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism"},{"location":"notes/fp_scala_poly/#learning-outcomes","text":"By this end of this lesson, you should be able to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes develop generic programming style code using Functor type class. make use of Option and Either to handle and manipulate errors and exceptions.","title":"Learning Outcomes"},{"location":"notes/fp_scala_poly/#currying","text":"In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments. For example, def sum(x:Int, y:Int):Int = x + y can be rewritten into def sum_curry(x:Int)(y:Int):Int = x + y These two functions are equivalent except that Their invocations are different, e.g. sum(1,2) sum_curry(1)(2) It is easier to reuse the curried version to define other function, e.g. def plus1(x:Int):Int = sum_curry(1)(x)","title":"Currying"},{"location":"notes/fp_scala_poly/#function-composition","text":"Every function and method in Scala is an object with a .compose() method. It works like the mathmethical composition. In math, let \\(g\\) and \\(f\\) be functions, then \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] Let g and f be Scala functions (or methods), then g.compose(f) is equivalent to x => g(f(x)) For example def f(x:Int):Int = 2 * x + 3 def g(x:Int):Int = x * x assert((g.compose(f))(2) == g(f(2)))","title":"Function Composition"},{"location":"notes/fp_scala_poly/#generics","text":"Generics is also known as type variables. It enables a language to support parametric polymoprhism.","title":"Generics"},{"location":"notes/fp_scala_poly/#polymorphic-functions","text":"Recall that the reverse function introduced in the last lesson def reverse(l:List[Int]):List[Int] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) } We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace Int by a type variable A . def reverse[A](l:List[A]):List[A] = l match { case Nil => Nil case (hd::tl) => reverse(tl) ++ List(hd) }","title":"Polymorphic functions"},{"location":"notes/fp_scala_poly/#polymorphic-algebraic-datatype","text":"Recall that the following Algebraic Datatype from the last lesson. enum MyList { case Nil case Cons(x:Int, xs:MyList) } def mapML(l:MyList, f:Int => Int):MyList = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl, f)) } Same observation applies. MyList could have a generic element type A instead of Int and mapML should remains unchanged. enum MyList[A] { case Nil // type error case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A], f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML( tl, f)) } The caveat here is that the Scala compiler would complain about the Nil case above -- Error: ---------------------------------------------------------------------- 2 | case Nil | ^^^^^^^^ | cannot determine type argument for enum parent class MyList, | type parameter type A is invariant 1 error found To understand that error, we need to understand how Scala desugar the enum datatype. The above MyList datatype is desugared as enum MyList[A] { case Nil extends MyList[Nothing] // type error case Cons(x:A, xs:MyList[A]) extends MyList[A] } In which all sub cases within the enum type must be sub-class of the enum type. However it is not trivial for Nil . It can't be declared as a subtype of MyList[A] since type variable A is not mentioned in its definition, unlike Cons(x:A, xs:MyList[A]) . The best we can get is MyList[Nothing] where Nothing is the subtype of all other types in Scala. (As the dual, Any is the supertype of all other types in Scala). We are getting very close. Now we know that Nil extends MyList[Nothing] . If we can argue that MyList[Nothing] extends MyList[A] then we are all good. For MyList[Nothing] extends MyList[A] to hold, A must be covariant type parameter. In type system with subtyping, a type is covariant if it preserves the subtyping order when it is applied a type constructor. In the above situation, MyList[_] is a type constructor. The type parameter A is covarient because we note Nothing <: A for all A , thus MyList[Nothing] <: MyList[A] . a type is contravariant if it reverses the subtyping order when it is applied to a type constructor. For instance, given function type A => Boolean , the type parameter A is contravariant, because for A <: B , we have B => Boolean <: A => Boolean . (We can use functions of type B => Boolean in the context where a function A => Boolean is expected, but not the other way round.) a type is invariant if it does not preserve nor reverse the subtyping order when it is applied to a type constructor. Hence to fix the above type error with the MyList[A] datatype, we declared that A is covariant, +A . enum MyList[+A] { case Nil // type error is fixed. case Cons(x:A, xs:MyList[A]) } def mapML[A,B](l:MyList[A])(f:A => B):MyList[B] = l match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), mapML(tl)(f)) } For easy of reasoning, we also rewrite mapML into currying style. Recall that we could make mapML function as a method of MyList enum MyList[+A] { case Nil case Cons(x:A, xs:MyList[A]) def mapML[B](f:A=>B):MyList[B] = this match { case MyList.Nil => MyList.Nil case MyList.Cons(hd, tl) => MyList.Cons(f(hd), tl.mapML(f)) } } Scala Variances","title":"Polymorphic Algebraic Datatype"},{"location":"notes/fp_scala_poly/#type-class","text":"Suppose we would like to convert some of the Scala value to JSON string. We could rely on overloading. def toJS(v:Int):String = v.toString def toJS(v:String):String = s\"'${v}'\" def toJS(v:Boolean):String = v.toString Given v is a Scala string value, s\"some_prefix ${v} some_suffix\" denotes a Scala string interpolation, which inserts v 's content into the \"place holder\" in the string \"some_prefix ${v} some_suffix\" where the ${v} is the place holder. However this becomes hard to manage as we consider complex datatype. enum Contact { case Email(e:String) case Phone(ph:String) } import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${toJS(e)}\" // compilation error case Phone(ph) => s\"'phone': ${toJS(ph)}\" // compilation error } When we try to define the toJS function for Contact datatype, we can't make use of the toJS function for string value because the compiler is confused that we are trying to make recursive calls. This is the first issue we faced. Let's pretend that the first issue has been addressed. There's still another issue. Consider case class Person(name:String, contacts:List[Contact]) case class Team(members:List[Person]) A case class is like a normal class we have seen earlier except that we can apply pattern matching to its values. Let's continue to overload toJS to handle Person and Team . def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${toJS(name)}, 'contacts':${toJS(contacts)} }\" } def toJS(cs:List[Contact]):String = { val j = cs.map(c=>toJS(c)).mkString(\",\") s\"[${j}]\" } def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${toJS(members)} }\" } def toJS(ps:List[Person]):String = { val j = ps.map(p=>toJS(p)).mkString(\",\") s\"[${j}]\" } The second issue is that the toJS(cs:List[Contact]) and toJS(ps:List[Person]) are the identical modulo the variable names. Can we combine two into one? def toJS[A](vs:List[A]):String = { val j = vs.map(v=>toJS(v)).mkString(\",\") // compiler error s\"[${j}]\" } However a compilation error occurs because the compiler is unable to resolve the toJS[A](v:A) used in the .map() . It seems that we need to give some extra information to the compiler so that it knows that when we use the above generic toJS we are referring to either Person or Contact , or whatever type that has a toJS defined. One solution to address the two above issues is to use type class . In Scala 3, a type class is defined by a polymoprhic trait and a set of type class instances. trait JS[A] { def toJS(v:A):String } given toJSInt:JS[Int] = new JS[Int]{ def toJS(v:Int):String = v.toString } given toJSString:JS[String] = new JS[String] { def toJS(v:String):String = s\"'${v}'\" } given toJSBoolean:JS[Boolean] = new JS[Boolean] { def toJS(v:Boolean):String = v.toString } given toJSContact(using jsstr:JS[String]):JS[Contact] = new JS[Contact] { import Contact.* def toJS(c:Contact):String = c match { case Email(e) => s\"'email': ${jsstr.toJS(e)}\" // compilation error is fixed case Phone(ph) => s\"'phone': ${jsstr.toJS(ph)}\" // compilation error is fixed } } given toJSPerson(using jsstr:JS[String], jsl:JS[List[Contact]]):JS[Person] = new JS[Person] { def toJS(p:Person):String = p match { case Person(name, contacts) => s\"'person':{ 'name':${jsstr.toJS(name)}, 'contacts':${jsl.toJS(contacts)} }\" } } given toJSTeam(using jsl:JS[List[Person]]):JS[Team] = new JS[Team] { def toJS(t:Team):String = t match { case Team(members) => s\"'team':{ 'members':${jsl.toJS(members)} }\" } } given toJSList[A](using jsa:JS[A]):JS[List[A]] = new JS[List[A]] { def toJS(as:List[A]):String = { val j = as.map(a=>jsa.toJS(a)).mkString(\",\") s\"[${j}]\" } } given defines a type class instance. An instance consists of a name and the context parameters (those with using ) and instance type. In the body of the type class instance, we instantiate an anonymous object that extends type class with the specific type and provide the defintion. We can refer to the particular type class instance by the instance's name. For instance import Contact.* val myTeam = Team( List( Person(\"kenny\", List(Email(\"kenny_lu@sutd.edu.sg\"))), Person(\"simon\", List(Email(\"simon_perrault@sutd.edu.sg\"))) )) toJSTeam.toJS(myTeam) yields 'team':{ 'members':['person':{ 'name':'kenny', 'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon', 'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] } We can also refer to the type class instance by the instace's type. For example, recall the last two instances. In the context of the toJSTeam , we refer to another instance of type JS[List[Person]] . Note that none of the defined instances has the required type. Scala is smart enought to synthesize it from the instances of toJSList and toJSPerson . Given the required type class instance is JS[List[Person]] , the type class resolver finds the instance toJSList having type JS[List[A]] , and it unifies both and find that A=Person . In the context of the instance toJSList , JS[A] is demanded. We can refine the required instance's type as JS[Person] , which is toJSPerson . Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. def printAsJSON[A](v:A)(using jsa:JS[A]):Unit = { println(jsa.toJS(v)) } printAsJSON(myTeam) Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming . In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects. In the next few section, we consider some common patterns in FP that are promoting generic programming.","title":"Type class"},{"location":"notes/fp_scala_poly/#functor","text":"Recall that we have a map method for list datatype. val l = List(1,2,3) l.map(x => x + 1) Can we make map to work for other data type? For example enum BTree[+A] { case Empty case Node(v:A, lft:BTree[A], rgt:BTree[A]) } It turns out that extending map to different datatypes is similar to toJS function that we implemented earlier. We consider introducing a type class for this purpose. trait Functor[T[_]] { def map[A,B](t:T[A])(f:A => B):T[B] } In the above type class definition, T[_] denotes a polymorphic type that of kind * => * . A kind is a type of types. In the above, it means Functor takes any type constructors T . When T is instantiated, it could be List[_] or BTree[_] and etc. (C.f. In the type class JS[A] , the type argument has kind * .) given listFunctor:Functor[List] = new Functor[List] { def map[A,B](t:List[A])(f:A => B):List[B] = t.map(f) } given btreeFunctor:Functor[BTree] = new Functor[BTree] { import BTree.* def map[A,B](t:BTree[A])(f:A => B):BTree[B] = t match { case Empty => Empty case Node(v, lft, rgt) => Node(f(v), map(lft)(f), map(rgt)(f)) } } Some example val l = List(1,2,3) listFunctor.map(l)((x:Int) => x + 1) val t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty)) btreeFunctor.map(t)((x:Int) => x + 1)","title":"Functor"},{"location":"notes/fp_scala_poly/#functor-laws","text":"All instances of functor must obey a set of mathematic laws for their computation to be predictable. Let i be a functor instance 1. Identity: i => map(i)(x => x) \\(\\equiv\\) x => x . When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: i=> map(i)(f.compose(g)) \\(\\equiv\\) (i => map(i)(f)).compose(j => map(j)(g)) . If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second.","title":"Functor Laws"},{"location":"notes/fp_scala_poly/#foldable","text":"Similarly we can define a Foldable type class for generic and overloaded foldLeft ( and foldRight ). trait Foldable[T[_]]{ def foldLeft[A,B](t:T[B])(acc:A)(f:(A,B)=>A):A } given listFoldable:Foldable[List] = new Foldable[List] { def foldLeft[A,B](t:List[B])(acc:A)(f:(A,B)=>A):A = t.foldLeft(acc)(f) } given btreeFoldable:Foldable[BTree] = new Foldable[BTree] { import BTree.* def foldLeft[A,B](t:BTree[B])(acc:A)(f:(A,B)=>A):A = t match { case Empty => acc case Node(v, lft, rgt) => { val acc1 = f(acc,v) val acc2 = foldLeft(lft)(acc1)(f) foldLeft(rgt)(acc2)(f) } } } listFoldable.foldLeft(l)(0)((x:Int,y:Int) => x + y) btreeFoldable.foldLeft(t)(0)((x:Int,y:Int) => x + y)","title":"Foldable"},{"location":"notes/fp_scala_poly/#option-and-either","text":"Recall in the earlier lesson, we encountered the following example. enum MathExp { case Plus(e1:MathExp, e2:MathExp) case Minus(e1:MathExp, e2:MathExp) case Mult(e1:MathExp, e2:MathExp) case Div(e1:MathExp, e2:MathExp) case Const(v:Int) } def eval(e:MathExp):Int = e match { case MathExp.Plus(e1, e2) => eval(e1) + eval(e2) case MathExp.Minus(e1, e2) => eval(e1) - eval(e2) case MathExp.Mult(e1, e2) => eval(e1) * eval(e2) case MathExp.Div(e1, e2) => eval(e1) / eval(e2) case MathExp.Const(i) => i } An error occurs when we try to evalue a MathExp which contains a division by zero sub-expression. Executing import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) yields java.lang.ArithmeticException: / by zero at rs$line$2$.eval(rs$line$2:5) ... 41 elided Like other main stream languages, we could use try-catch statement to handle the exception. try { import MathExp.* eval(Div(Const(1), Minus(Const(2), Const(2)))) } catch { case e:java.lang.ArithmeticException => println(\"handinging div by zero\") } One downside of this approach is that at compile type it is hard to track the unhandled exceptions, (in particular with the presence of Java unchecked exceptions.) A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes). Consider the following builtin Scala datatype Option // no need to run this. enum Option[+A] { case None case Some(v:A) } def eval(e:MathExp):Option[Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(v2) => Some(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case None => None case Some(v1) => eval(e2) match { case None => None case Some(0) => None case Some(v2) => Some(v1 / v2) } } case MathExp.Const(i) => Some(i) } When we execute eval(Div(Const(1), Minus(Const(2), Const(2)))) , we get None as the result instead of the exception. One advantage of this is that whoever is using eval function has to respect that its return type is Option[Int] instead of just Int therefore, a match must be applied before using the result to look out for potential None value. There are still two drawbacks. Firstly, the updated version of the eval function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue. We could instead of using Option , use the Either datatype // no need to run this, it's builtin enum Either[+A, +B] { case Left(v:A) case Right(v:B) } type ErrMsg = String def eval(e: MathExp): Either[ErrMsg,Int] = e match { case MathExp.Plus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 + v2) } } case MathExp.Minus(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 - v2) } } case MathExp.Mult(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(v2) => Right(v1 * v2) } } case MathExp.Div(e1, e2) => eval(e1) match { case Left(m) => Left(m) case Right(v1) => eval(e2) match { case Left(m) => Left(m) case Right(0) => Left(s\"div by zero caused by ${e.toString}\") case Right(v2) => Right(v1 / v2) } } case MathExp.Const(i) => Right(i) } Executing eval(Div(Const(1), Minus(Const(2), Const(2)))) will yield Left(div by zero caused by Div(Const(1),Minus(Const(2),Const(2))))","title":"Option and Either"},{"location":"notes/fp_scala_poly/#summary","text":"In this lesson, we have discussed how to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes how to develop generic programming style code using Functor type class. how to make use of Option and Either to handle and manipulate errors and exceptions.","title":"Summary"},{"location":"notes/fp_scala_poly/#appendix","text":"","title":"Appendix"},{"location":"notes/fp_scala_poly/#generalized-algebraic-data-type","text":"Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example. Firstly, we need some type acrobatics to encode nature numbers on the level of type. enum Zero { case Zero } enum Succ[A] { case Succ(v:A) } Next we define our GADT SList[S,A] which is a generic list of elements A and with size S . enum SList[S,+A] { case Nil extends SList[Zero,Nothing] // additional type constraint S = Zero case Cons[N,A](hd:A, tl:SList[N,A]) extends SList[Succ[N],A] // add'n type constraint S = Succ[N] } In the first subcase Nil , it is declared with the type of SList[Zero, Nothing] which indicates on type level that the list is empty. In the second case Cons , we define it to have the type SList[Succ[N],A] for some natural number N . This indicates on the type level that the list is non-empty. Having these information lifted to the type level allows us to define a type safe head function. import SList.* def head[A,N](sl:SList[Succ[N],A]):A = sl match { case Cons(hd, tl) => hd } Compiling head(Nil) yields a type error. Similarly we can define a size-aware function snoc which add an element at the tail of a list. def snoc[A,N](v:A, sl:SList[N,A]):SList[Succ[N],A] = sl match { case Nil => Cons(v,Nil) // case Cons(hd, tl) => snoc(v, tl) will result in compilation error. case Cons(hd, tl) => Cons(hd, snoc(v, tl)) }","title":"Generalized Algebraic Data Type"},{"location":"notes/handout/","text":"50.054 Compiler Design and Program Analysis Course Handout This page will be updated regularly. Sync up often. Course Description This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis. Module Learning Outcomes By the end of this module, students are able to Implement software solutions using functional programming language and applying design patterns Define the essential components in a program compilation pipeline Design a compiler for an imperative programming language Optimise the generated machine codes by applying program analysis techniques Detect bugs and security flaws in software by applying program analysis techniques Measurable Outcomes Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming Implement a type checker for the source language Develop a static analyser to eliminate dead codes Implement the register allocation algorithm in the target code generation module Develop a static analyser to identify potential security flaws in the source language Topics Functional Programming : Expression, Function, Conditional Functional Programming : List, Algebraic data type and Pattern Matching Functional Programming : Type class Functional Programming : Generic and Functor Functional Programming : Applicative and Monad Syntax analysis: Lexing Syntax analysis: Parsing (LL, LR, SLR) Syntax analysis: Parser Combinator Intermediate Representation: Pseudo-Assembly Intermediate Representation: SSA Semantic analysis: Dynamic Semantics Semantic analysis: Type checking Semantic analysis: Type Inference Semantic analysis: Sign analysis Semantic analysis: Liveness analysis Code Gen: Instruction selection Code Gen: Register allocation Memory Management Resource The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman Modern Compiler Implementation in ML by Andrew W. Appel Types and Programming Languages by Benjamin C. Pierce Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach Instructors Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Wednesday 3:00-4:30pm (please send email to arrange) Communication If you have course/assignment/project related questions, please post it on the dedicated MS teams channel. Assessment Mid-term 10% Project 35% Homework 20% Final 30% Class Participation 5% Things you need to prepare If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew. Make sure JVM >=11 is installed and ant is installed. Install Scala >= 3 https://www.scala-lang.org/download/ IDE: It's your choice, but VSCode works fine. Project The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. You may work as a team (up to max 3 members). Please register your team here . Lab 1 (10%, Deadline - 12 Nov 2023 23:59) Lab 2 (10%, Deadline - 26 Nov 2023 23:59) Lab 3 (15%, Deadline - 10 Dec 2023 23:59) Submission Policy and Plagiarism You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work. Schedule Week Session 1 Session 2 Session 3 Assessment 1 Intro FP: Expression, Function, Conditional, Recursion Cohort Problem 1 , Homework 1 Homework 1 no submission required 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2 , Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3 , Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4 , Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5 , Homework 3 (Cont'd) Homework 3 5% 6 Bottom-up Parsing IR: Pseudo-Assembly Cohort Problem 6 , Homework 4 7 Homework 4 5% 8 Mid-term , Semantic Analysis Dynamic Semantics Cohort Problem 7 Mid-term 10% 9 Static Semantics for SIMP Static Semantics for Lambda Calculus Cohort Problem 8 , Homework 5 Project Lab 1 10% 10 Public Holiday. No Class Scheduled Name Analysis, SSA Cohort Problem 9 11 Lattice, Sign Analysis Liveness Analysis Cohort Problem 10 Project Lab 2 10%, Homework 5 5% 12 Code Generation Information Flow Analysis Cohort Problem 11 13 Guest Lecture Memory Management Revision Project Lab 3 15% 14 Final Exam (13 Dec Wed 9:00AM-11:00AM) 30% Make Up and Alternative Assessment Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"Handout"},{"location":"notes/handout/#50054-compiler-design-and-program-analysis-course-handout","text":"","title":"50.054 Compiler Design and Program Analysis Course Handout"},{"location":"notes/handout/#this-page-will-be-updated-regularly-sync-up-often","text":"","title":"This page will be updated regularly. Sync up often."},{"location":"notes/handout/#course-description","text":"This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis.","title":"Course Description"},{"location":"notes/handout/#module-learning-outcomes","text":"By the end of this module, students are able to Implement software solutions using functional programming language and applying design patterns Define the essential components in a program compilation pipeline Design a compiler for an imperative programming language Optimise the generated machine codes by applying program analysis techniques Detect bugs and security flaws in software by applying program analysis techniques","title":"Module Learning Outcomes"},{"location":"notes/handout/#measurable-outcomes","text":"Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming Implement a type checker for the source language Develop a static analyser to eliminate dead codes Implement the register allocation algorithm in the target code generation module Develop a static analyser to identify potential security flaws in the source language","title":"Measurable Outcomes"},{"location":"notes/handout/#topics","text":"Functional Programming : Expression, Function, Conditional Functional Programming : List, Algebraic data type and Pattern Matching Functional Programming : Type class Functional Programming : Generic and Functor Functional Programming : Applicative and Monad Syntax analysis: Lexing Syntax analysis: Parsing (LL, LR, SLR) Syntax analysis: Parser Combinator Intermediate Representation: Pseudo-Assembly Intermediate Representation: SSA Semantic analysis: Dynamic Semantics Semantic analysis: Type checking Semantic analysis: Type Inference Semantic analysis: Sign analysis Semantic analysis: Liveness analysis Code Gen: Instruction selection Code Gen: Register allocation Memory Management","title":"Topics"},{"location":"notes/handout/#resource","text":"The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics. Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman Modern Compiler Implementation in ML by Andrew W. Appel Types and Programming Languages by Benjamin C. Pierce Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach","title":"Resource"},{"location":"notes/handout/#instructors","text":"Kenny Lu (kenny_lu@sutd.edu.sg) Office Hour: Wednesday 3:00-4:30pm (please send email to arrange)","title":"Instructors"},{"location":"notes/handout/#communication","text":"If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.","title":"Communication"},{"location":"notes/handout/#assessment","text":"Mid-term 10% Project 35% Homework 20% Final 30% Class Participation 5%","title":"Assessment"},{"location":"notes/handout/#things-you-need-to-prepare","text":"If you are using Windows 10 or Windows 11, please install ubuntu subsystems Win10 Win11 If you are using Linux, it should be perfect. If you are using Mac, please install homebrew. Make sure JVM >=11 is installed and ant is installed. Install Scala >= 3 https://www.scala-lang.org/download/ IDE: It's your choice, but VSCode works fine.","title":"Things you need to prepare"},{"location":"notes/handout/#project","text":"The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. You may work as a team (up to max 3 members). Please register your team here . Lab 1 (10%, Deadline - 12 Nov 2023 23:59) Lab 2 (10%, Deadline - 26 Nov 2023 23:59) Lab 3 (15%, Deadline - 10 Dec 2023 23:59)","title":"Project"},{"location":"notes/handout/#submission-policy-and-plagiarism","text":"You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else. You will not post any solutions related to this course to a private/public repository that is accessible by the public/others. Students are allowed to have a private repository for their assignment which no one can access. For projects, students can only invite their partners as collaborators to a private repository. Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.","title":"Submission Policy and Plagiarism"},{"location":"notes/handout/#schedule","text":"Week Session 1 Session 2 Session 3 Assessment 1 Intro FP: Expression, Function, Conditional, Recursion Cohort Problem 1 , Homework 1 Homework 1 no submission required 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2 , Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3 , Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4 , Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5 , Homework 3 (Cont'd) Homework 3 5% 6 Bottom-up Parsing IR: Pseudo-Assembly Cohort Problem 6 , Homework 4 7 Homework 4 5% 8 Mid-term , Semantic Analysis Dynamic Semantics Cohort Problem 7 Mid-term 10% 9 Static Semantics for SIMP Static Semantics for Lambda Calculus Cohort Problem 8 , Homework 5 Project Lab 1 10% 10 Public Holiday. No Class Scheduled Name Analysis, SSA Cohort Problem 9 11 Lattice, Sign Analysis Liveness Analysis Cohort Problem 10 Project Lab 2 10%, Homework 5 5% 12 Code Generation Information Flow Analysis Cohort Problem 11 13 Guest Lecture Memory Management Revision Project Lab 3 15% 14 Final Exam (13 Dec Wed 9:00AM-11:00AM) 30%","title":"Schedule"},{"location":"notes/handout/#make-up-and-alternative-assessment","text":"Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test.","title":"Make Up and Alternative Assessment"},{"location":"notes/introduction/","text":"50.054 - Introduction Module Description This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis. Module Learning Objective By the end of this module, you should be able to Comprehend and reason functional programming Develop functional program to solve real world problem Identify the major components in compiler development Explain and implement different techniques and algorithms used in compiler development What is compilation? Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc. A compiler is a software system that manages the program compliation process. What properties a good compiler should posess? An ideal compiler should be: Correct. The produced target program should behave the same as the the source program. Reliable. Any errors that could arise in the program should be detected and reported before execution. Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform. Some optional properties, User friendly. The error report should be comprehensive. Intelligent. Helps to automate some of the repeatitive tasks. ...","title":"50.054 - Introduction"},{"location":"notes/introduction/#50054-introduction","text":"","title":"50.054 - Introduction"},{"location":"notes/introduction/#module-description","text":"This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis.","title":"Module Description"},{"location":"notes/introduction/#module-learning-objective","text":"By the end of this module, you should be able to Comprehend and reason functional programming Develop functional program to solve real world problem Identify the major components in compiler development Explain and implement different techniques and algorithms used in compiler development","title":"Module Learning Objective"},{"location":"notes/introduction/#what-is-compilation","text":"Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc. A compiler is a software system that manages the program compliation process.","title":"What is compilation?"},{"location":"notes/introduction/#what-properties-a-good-compiler-should-posess","text":"An ideal compiler should be: Correct. The produced target program should behave the same as the the source program. Reliable. Any errors that could arise in the program should be detected and reported before execution. Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform. Some optional properties, User friendly. The error report should be comprehensive. Intelligent. Helps to automate some of the repeatitive tasks. ...","title":"What properties a good compiler should posess?"},{"location":"notes/ir_pseudo_assembly/","text":"50.054 - Pseudo Assembly Learning Outcomes By the end of this lesson, you should be able to Describe the syntax of the source language SIMP. Describe the syntax of the intermediate representation language pseudo-assembly. Describe how pseudo-assembly program is executed. Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code. Recap the compiler pipeline Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation. For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly. The SIMP Language We consider the syntax of SIMP as follows \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. For example (Example SIMP1) x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Pseudo Assembly We consider the Pseudo Assembly language as follows. \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] where \\(li\\) , a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\) . For simplicity, we use positive integers as labels. An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\) ), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use 0 to denote false and 1 to denote true . \\(r_{ret}\\) is a special register for the return statement. Example (PA1) 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Informal Specification of Pseudo Assembly We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. For example when we execute the above program with input = 2 Program Counter Local Memory Next Instr 1 {input: 2, x : 2} 2 2 {input: 2, x : 2, s : 0} 3 3 {input: 2, x : 2, s : 0, c : 0} 4 4 {input: 2, x : 2, s : 0, c : 0, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 0, t : 1} 6 6 {input: 2, x : 2, s : 0, c : 0, t : 1} 7 7 {input: 2, x : 2, s : 0, c : 1, t : 1} 8 8 {input: 2, x : 2, s : 0, c : 1, t : 1} 4 4 {input: 2, x : 2, s : 0, c : 1, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 1, t : 1} 6 6 {input: 2, x : 2, s : 1, c : 1, t : 1} 7 7 {input: 2, x : 2, s : 1, c : 2, t : 1} 8 8 {input: 2, x : 2, s : 1, c : 2, t : 1} 4 4 {input: 2, x : 2, s : 1, c : 2, t : 0} 5 5 {input: 2, x : 2, s : 1, c : 2, t : 0} 9 9 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} 10 10 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} - For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons. Maximal Munch Algorithm To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. \\[ \\begin{array}{rc} {\\tt (mAssign)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_s(X = E) \\vdash lis \\end{array} \\\\ \\end{array} \\] In case we have an assignment statement \\(X = E\\) , we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions. \\[ \\begin{array}{rc} {\\tt (mReturn)} & \\begin{array}{c} G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_s(return\\ X) \\vdash lis + [ l: ret ] \\end{array} \\end{array} \\] In case we have a return statement \\(return\\ E\\) , we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\) . We then generate a new label \\(l\\) , and append \\(l:ret\\) to the instructions. \\[ \\begin{array}{rc} {\\tt (mSequence)} & \\begin{array}{c} {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\ \\hline G_s(S_1;...;S_n) \\vdash lis_1 + ... + lis_n \\end{array} \\end{array} \\] In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation. \\[ \\begin{array}{rl} {\\tt (mIf)} & \\begin{array}{c} t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3' \\end{array} \\\\ \\end{array} \\] In case we have a if-else statement, we 1. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\) . 6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\) , which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) \\[ \\begin{array}{rl} {\\tt (mWhile)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2' \\end{array} \\\\ \\end{array} \\] In case we have a while statement, we 1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\) , which can be used later as the reference for the backward jump. 2. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\) . (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) The above summarizes all cases of \\(G_s(S)\\) . We now consider the sub algorithm, \\(G_a(d)(E)\\) , it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash [] \\] The case of \\(nop\\) statement is stratight-forward. \\[ \\begin{array}{rc} {\\tt (mConst)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\ \\hline G_a(X)(C) \\vdash [l : X \\leftarrow c] \\end{array} \\\\ \\end{array} \\] In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\) . where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. \\[ \\begin{array}{rcl} conv(true) & = & 1\\\\ conv(false) & = & 0\\\\ conv(C) & =& C \\end{array} \\] For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. \\[ \\begin{array}{rc} {\\tt (mVar)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(Y) \\vdash [l : X \\leftarrow Y] \\end{array} \\\\ \\end{array} \\] In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\) . The treat is similar to the case of \\({\\tt (Const)}\\) . \\[ \\begin{array}{rc} {\\tt (mParen)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_a(X)((E)) \\vdash lis \\end{array} \\end{array} \\] In the rule \\({\\tt (mParen)}\\) , we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression. \\[ \\begin{array}{rc} {\\tt (mOp)} & \\begin{array}{c} t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_1)(E_1) \\vdash lis_1 \\\\ t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_2)(E_2) \\vdash lis_2 \\\\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2] \\end{array} \\\\ \\end{array} \\] The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\) , we generate two temp variables \\(t_1\\) and \\(t_2\\) , and apply the generation function recursively to \\(E_1\\) and \\(E_2\\) . Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\) . For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation. Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement, Gs(x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;) ---> Gs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c < x { s = c + s; c = c + 1;}) ; Gs(return s); The derivation for Gs(x = input) is trivial, we apply \\({\\tt (mAssign)}\\) rule. Gs(x = input) ---> # using (mAssign) rule Ga(x)(input) ---> # using (mVar) rule ---> [ 1: x <- input ] Similarly we generate Gs( s = 0) ---> # using (mAssign) rule Ga(s)(0) ---> # using (mConst) rule ---> [ 2: s <- 0 ] and Gs(c = 0) ---> # using (mAssign) rule Ga(c)(0) ---> # using (mConst) rule ---> [ 3: c <- 0 ] Next we consider the while statement Gs( while c < x { s = c + s; c = c + 1; } ) ---> # using (mWhile) rule # the condition exp t is a fresh var Ga(t)(c<x) ---> # using (mOp) rule t1 is a fresh var Ga(t1)(x) ---> [4: t1 <- x] t2 is a fresh var Ga(t2)(c) ---> [5: t2 <- c] ---> [4: t1 <- x, 5: t2 <-c, 6: t <- t1 < t2 ] # the conditional jump, we generate a new label 7 reserved for whilecondjump # the while loop body Gs[ s = c + s; c = c + 1] ---> # (mSequence), (mOp) and (mOp) rules [ 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7 ] # end of the while loop [ 14: goto 4 ] # the conditional jump ---> [7: ifn t goto 15 ] ---> # putting altogther [4: t1 <- x, 5: t2 <- c, 6: t <- t1 < t2, 7: ifn t goto 15, 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7, 14: goto 4] Finally we convert the return statement Gs(return s) ---> # (mReturn) rule [15: r_ret <- s, 16: ret] Putting 1,2,3 together 1: x <- input 2: s <- 0 3: c <- 0 4: t1 <- x 5: t2 <- c 6: t <- t1 < t2 7: ifn t goto 15 8: t3 <- c 9: t4 <- s 10: t5 <- t3 + t4 11: t6 <- c 12: t7 <- 1 13: t8 <- t6 + t7 14: goto 4 15: rret <- s 16: ret As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction). Maximal Munch Algorithm V2 Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR. We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\hat{e}, \\check{e}\\) . where \\(\\check{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\hat{e}\\) is the \"result\" operand storing the final result of \\(\\check{e}\\) . The adjusted \\({\\tt (mConst)}\\) , \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows, \\[ \\begin{array}{rc} {\\tt (m2Const)} & \\begin{array}{c} G_e(C) \\vdash (conv(C), []) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} & \\begin{array}{c} G_e(Y) \\vdash (Y, []) \\end{array} \\end{array} \\] The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\hat{e}\\) with an empty set of label instructions. \\[ \\begin{array}{rc} {\\tt (m2Paren)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ \\hline G_e((E)) \\vdash (\\hat{e}, \\check{e}) \\end{array} \\end{array} \\] In the rule \\({\\tt (m2Paren)}\\) , we generate the results by recursivelly applying the algorithm to the inner expression. \\[ \\begin{array}{rc} {\\tt (m2Op)} & \\begin{array}{c} G_e(E_1) \\vdash (\\hat{e}_1, \\check{e}_1) \\\\ G_e(E_2) \\vdash (\\hat{e}_2, \\check{e}_2) \\\\ t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\ l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_e(E_1 OP E_2) \\vdash (t, \\check{e}_1 + \\check{e}_2 + [l : t \\leftarrow \\hat{e}_1 OP \\hat{e}_2]) \\end{array} \\\\ \\end{array} \\] In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the results for \\(E_1\\) and \\(E_2\\) , namely \\((\\hat{e}_1, \\check{e}_1)\\) and \\((\\hat{e}_2, \\check{e}_2)\\) . We then use them to synthesis the final output. The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\) . \\[ \\begin{array}{rc} {\\tt (m2Assign)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_s(X = E) \\vdash \\check{e} + [ l : X \\leftarrow \\hat{e}] \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Return)} & \\begin{array}{c} G_s(return\\ X) \\vdash \\check{e} + [ l_1 : r_{ret} \\leftarrow X, l_2: ret ] \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\check{e} + lis_1 + lis_2' + lis_3' \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash \\check{e} + lis_1 + lis_2' \\end{array} \\end{array} \\] By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.","title":"50.054 - Pseudo Assembly"},{"location":"notes/ir_pseudo_assembly/#50054-pseudo-assembly","text":"","title":"50.054 - Pseudo Assembly"},{"location":"notes/ir_pseudo_assembly/#learning-outcomes","text":"By the end of this lesson, you should be able to Describe the syntax of the source language SIMP. Describe the syntax of the intermediate representation language pseudo-assembly. Describe how pseudo-assembly program is executed. Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code.","title":"Learning Outcomes"},{"location":"notes/ir_pseudo_assembly/#recap-the-compiler-pipeline","text":"Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation. For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly.","title":"Recap the compiler pipeline"},{"location":"notes/ir_pseudo_assembly/#the-simp-language","text":"We consider the syntax of SIMP as follows \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. For example (Example SIMP1) x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;","title":"The SIMP Language"},{"location":"notes/ir_pseudo_assembly/#pseudo-assembly","text":"We consider the Pseudo Assembly language as follows. \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] where \\(li\\) , a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\) . For simplicity, we use positive integers as labels. An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\) ), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use 0 to denote false and 1 to denote true . \\(r_{ret}\\) is a special register for the return statement. Example (PA1) 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret","title":"Pseudo Assembly"},{"location":"notes/ir_pseudo_assembly/#informal-specification-of-pseudo-assembly","text":"We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. For example when we execute the above program with input = 2 Program Counter Local Memory Next Instr 1 {input: 2, x : 2} 2 2 {input: 2, x : 2, s : 0} 3 3 {input: 2, x : 2, s : 0, c : 0} 4 4 {input: 2, x : 2, s : 0, c : 0, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 0, t : 1} 6 6 {input: 2, x : 2, s : 0, c : 0, t : 1} 7 7 {input: 2, x : 2, s : 0, c : 1, t : 1} 8 8 {input: 2, x : 2, s : 0, c : 1, t : 1} 4 4 {input: 2, x : 2, s : 0, c : 1, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 1, t : 1} 6 6 {input: 2, x : 2, s : 1, c : 1, t : 1} 7 7 {input: 2, x : 2, s : 1, c : 2, t : 1} 8 8 {input: 2, x : 2, s : 1, c : 2, t : 1} 4 4 {input: 2, x : 2, s : 1, c : 2, t : 0} 5 5 {input: 2, x : 2, s : 1, c : 2, t : 0} 9 9 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} 10 10 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} - For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons.","title":"Informal Specification of Pseudo Assembly"},{"location":"notes/ir_pseudo_assembly/#maximal-munch-algorithm","text":"To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. \\[ \\begin{array}{rc} {\\tt (mAssign)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_s(X = E) \\vdash lis \\end{array} \\\\ \\end{array} \\] In case we have an assignment statement \\(X = E\\) , we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions. \\[ \\begin{array}{rc} {\\tt (mReturn)} & \\begin{array}{c} G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_s(return\\ X) \\vdash lis + [ l: ret ] \\end{array} \\end{array} \\] In case we have a return statement \\(return\\ E\\) , we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\) . We then generate a new label \\(l\\) , and append \\(l:ret\\) to the instructions. \\[ \\begin{array}{rc} {\\tt (mSequence)} & \\begin{array}{c} {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\ \\hline G_s(S_1;...;S_n) \\vdash lis_1 + ... + lis_n \\end{array} \\end{array} \\] In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation. \\[ \\begin{array}{rl} {\\tt (mIf)} & \\begin{array}{c} t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3' \\end{array} \\\\ \\end{array} \\] In case we have a if-else statement, we 1. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\) . 6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\) , which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) \\[ \\begin{array}{rl} {\\tt (mWhile)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ t\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t)(E) \\vdash lis_0 \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2' \\end{array} \\\\ \\end{array} \\] In case we have a while statement, we 1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\) , which can be used later as the reference for the backward jump. 2. generate a fresh variable \\(t\\) , and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\) . (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) The above summarizes all cases of \\(G_s(S)\\) . We now consider the sub algorithm, \\(G_a(d)(E)\\) , it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash [] \\] The case of \\(nop\\) statement is stratight-forward. \\[ \\begin{array}{rc} {\\tt (mConst)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\ \\hline G_a(X)(C) \\vdash [l : X \\leftarrow c] \\end{array} \\\\ \\end{array} \\] In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\) . where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. \\[ \\begin{array}{rcl} conv(true) & = & 1\\\\ conv(false) & = & 0\\\\ conv(C) & =& C \\end{array} \\] For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. \\[ \\begin{array}{rc} {\\tt (mVar)} & \\begin{array}{c} l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(Y) \\vdash [l : X \\leftarrow Y] \\end{array} \\\\ \\end{array} \\] In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\) . The treat is similar to the case of \\({\\tt (Const)}\\) . \\[ \\begin{array}{rc} {\\tt (mParen)} & \\begin{array}{c} G_a(X)(E) \\vdash lis \\\\ \\hline G_a(X)((E)) \\vdash lis \\end{array} \\end{array} \\] In the rule \\({\\tt (mParen)}\\) , we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression. \\[ \\begin{array}{rc} {\\tt (mOp)} & \\begin{array}{c} t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_1)(E_1) \\vdash lis_1 \\\\ t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\ G_a(t_2)(E_2) \\vdash lis_2 \\\\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\ \\hline G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2] \\end{array} \\\\ \\end{array} \\] The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\) , we generate two temp variables \\(t_1\\) and \\(t_2\\) , and apply the generation function recursively to \\(E_1\\) and \\(E_2\\) . Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\) . For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation. Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement, Gs(x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s;) ---> Gs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c < x { s = c + s; c = c + 1;}) ; Gs(return s); The derivation for Gs(x = input) is trivial, we apply \\({\\tt (mAssign)}\\) rule. Gs(x = input) ---> # using (mAssign) rule Ga(x)(input) ---> # using (mVar) rule ---> [ 1: x <- input ] Similarly we generate Gs( s = 0) ---> # using (mAssign) rule Ga(s)(0) ---> # using (mConst) rule ---> [ 2: s <- 0 ] and Gs(c = 0) ---> # using (mAssign) rule Ga(c)(0) ---> # using (mConst) rule ---> [ 3: c <- 0 ] Next we consider the while statement Gs( while c < x { s = c + s; c = c + 1; } ) ---> # using (mWhile) rule # the condition exp t is a fresh var Ga(t)(c<x) ---> # using (mOp) rule t1 is a fresh var Ga(t1)(x) ---> [4: t1 <- x] t2 is a fresh var Ga(t2)(c) ---> [5: t2 <- c] ---> [4: t1 <- x, 5: t2 <-c, 6: t <- t1 < t2 ] # the conditional jump, we generate a new label 7 reserved for whilecondjump # the while loop body Gs[ s = c + s; c = c + 1] ---> # (mSequence), (mOp) and (mOp) rules [ 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7 ] # end of the while loop [ 14: goto 4 ] # the conditional jump ---> [7: ifn t goto 15 ] ---> # putting altogther [4: t1 <- x, 5: t2 <- c, 6: t <- t1 < t2, 7: ifn t goto 15, 8: t3 <- c, 9: t4 <- s, 10: t5 <- t3 + t4, 11: t6 <- c, 12: t7 <- 1, 13: t8 <- t6 + t7, 14: goto 4] Finally we convert the return statement Gs(return s) ---> # (mReturn) rule [15: r_ret <- s, 16: ret] Putting 1,2,3 together 1: x <- input 2: s <- 0 3: c <- 0 4: t1 <- x 5: t2 <- c 6: t <- t1 < t2 7: ifn t goto 15 8: t3 <- c 9: t4 <- s 10: t5 <- t3 + t4 11: t6 <- c 12: t7 <- 1 13: t8 <- t6 + t7 14: goto 4 15: rret <- s 16: ret As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction).","title":"Maximal Munch Algorithm"},{"location":"notes/ir_pseudo_assembly/#maximal-munch-algorithm-v2","text":"Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR. We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\hat{e}, \\check{e}\\) . where \\(\\check{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\hat{e}\\) is the \"result\" operand storing the final result of \\(\\check{e}\\) . The adjusted \\({\\tt (mConst)}\\) , \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows, \\[ \\begin{array}{rc} {\\tt (m2Const)} & \\begin{array}{c} G_e(C) \\vdash (conv(C), []) \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} & \\begin{array}{c} G_e(Y) \\vdash (Y, []) \\end{array} \\end{array} \\] The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\hat{e}\\) with an empty set of label instructions. \\[ \\begin{array}{rc} {\\tt (m2Paren)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ \\hline G_e((E)) \\vdash (\\hat{e}, \\check{e}) \\end{array} \\end{array} \\] In the rule \\({\\tt (m2Paren)}\\) , we generate the results by recursivelly applying the algorithm to the inner expression. \\[ \\begin{array}{rc} {\\tt (m2Op)} & \\begin{array}{c} G_e(E_1) \\vdash (\\hat{e}_1, \\check{e}_1) \\\\ G_e(E_2) \\vdash (\\hat{e}_2, \\check{e}_2) \\\\ t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\ l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_e(E_1 OP E_2) \\vdash (t, \\check{e}_1 + \\check{e}_2 + [l : t \\leftarrow \\hat{e}_1 OP \\hat{e}_2]) \\end{array} \\\\ \\end{array} \\] In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the results for \\(E_1\\) and \\(E_2\\) , namely \\((\\hat{e}_1, \\check{e}_1)\\) and \\((\\hat{e}_2, \\check{e}_2)\\) . We then use them to synthesis the final output. The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\) . \\[ \\begin{array}{rc} {\\tt (m2Assign)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label.} \\\\ \\hline G_s(X = E) \\vdash \\check{e} + [ l : X \\leftarrow \\hat{e}] \\end{array} \\\\ \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (m2Return)} & \\begin{array}{c} G_s(return\\ X) \\vdash \\check{e} + [ l_1 : r_{ret} \\leftarrow X, l_2: ret ] \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} & \\begin{array}{c} G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S_2) \\vdash lis_2 \\\\ l_{EndThen}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\ G_s(S_3) \\vdash lis_3 \\\\ l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{IfCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{Else} ] \\\\ lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\ lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\ \\hline G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\check{e} + lis_1 + lis_2' + lis_3' \\end{array} \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} & \\begin{array}{c} l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\ l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ G_s(S) \\vdash lis_2\\\\ l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\ l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\ lis_1 = [l_{WhileCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{EndWhile}] \\\\ lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\ \\hline G_s(while\\ E\\ \\{S\\}) \\vdash \\check{e} + lis_1 + lis_2' \\end{array} \\end{array} \\] By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.","title":"Maximal Munch Algorithm V2"},{"location":"notes/liveness_analysis/","text":"50.054 - Liveness Analysis Learning Outcomes Define the liveness analysis problem Apply lattice and fixed point algorithm to solve the liveness analysis problem Recall // SIMP1 x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; In the above program the statement t = s is redundant as t is not used. It can be statically detected by a liveness analysis. Liveness Analysis A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\) . Otherwise, the variable is considered not live or dead . Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite. By applying this analysis to the above program, we can find out at the program locations where variables must be dead. Defining the Lattice for Livenesss Analysis Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\) . Applying this approach the liveness analysis, we consider the powerset the set of all variables in the program. Let's recast the SIMP1 program into pseudo assembly, let's label it as PA1 1: x <- input 2: y <- 0 3: s <- 0 4: b <- y < x 5: ifn b goto 10 6: y <- y + 1 7: t <- s 8: s <- s + y 9: goto 4 10: rret <- s 11: ret In PA1 we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\) , if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\) , we see the following hasse diagram graph TD; N58[\"{b}\"] --- N64[\"{}\"] N59[\"{t}\"] --- N64[\"{}\"] N60[\"{s}\"] --- N64[\"{}\"] N61[\"{y}\"] --- N64[\"{}\"] N62[\"{x}\"] --- N64[\"{}\"] N63[\"{input}\"] --- N64[\"{}\"] N43[\"{t,b}\"] --- N58[\"{b}\"] N44[\"{s,b}\"] --- N58[\"{b}\"] N46[\"{y,b}\"] --- N58[\"{b}\"] N49[\"{x,b}\"] --- N58[\"{b}\"] N53[\"{input,b}\"] --- N58[\"{b}\"] N43[\"{t,b}\"] --- N59[\"{t}\"] N45[\"{s,t}\"] --- N59[\"{t}\"] N47[\"{y,t}\"] --- N59[\"{t}\"] N50[\"{x,t}\"] --- N59[\"{t}\"] N54[\"{input,t}\"] --- N59[\"{t}\"] N44[\"{s,b}\"] --- N60[\"{s}\"] N45[\"{s,t}\"] --- N60[\"{s}\"] N48[\"{y,s}\"] --- N60[\"{s}\"] N51[\"{x,s}\"] --- N60[\"{s}\"] N55[\"{input,s}\"] --- N60[\"{s}\"] N46[\"{y,b}\"] --- N61[\"{y}\"] N47[\"{y,t}\"] --- N61[\"{y}\"] N48[\"{y,s}\"] --- N61[\"{y}\"] N52[\"{x,y}\"] --- N61[\"{y}\"] N56[\"{input,y}\"] --- N61[\"{y}\"] N49[\"{x,b}\"] --- N62[\"{x}\"] N50[\"{x,t}\"] --- N62[\"{x}\"] N51[\"{x,s}\"] --- N62[\"{x}\"] N52[\"{x,y}\"] --- N62[\"{x}\"] N57[\"{input,x}\"] --- N62[\"{x}\"] N53[\"{input,b}\"] --- N63[\"{input}\"] N54[\"{input,t}\"] --- N63[\"{input}\"] N55[\"{input,s}\"] --- N63[\"{input}\"] N56[\"{input,y}\"] --- N63[\"{input}\"] N57[\"{input,x}\"] --- N63[\"{input}\"] N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\) . The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\) . Defining the Monotone Constraint for Liveness Analysis In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed. In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\) . In Sign Analysis the \\(join(s_i)\\) function is defined as the least upper bound of all the states that are preceding \\(s_i\\) in the control flow. In Liveness Analysis, we define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup succ(s_i) \\] where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l:ret\\) , \\(s_l = \\{\\}\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\) . \\[ \\begin{array}{rcl} var(r) & = & \\{ \\} \\\\ var(t) & = & \\{ t \\} \\\\ var(c) & = & \\{ \\} \\end{array} \\] By applying the PA program above we have s11 = {} s10 = join(s10) U {s} = {s} s9 = join(s9) = s4 s8 = (join(s8) - {s}) U {s, y} = (s9 - {s}) U {s, y} s7 = (join(s7) - {t}) U {s} = (s8 - {t}) U {s} s6 = (join(s6) - {y}) U {y} = (s7 - {y}) U {y} s5 = join(s5) U {b} = s6 U s10 U {b} s4 = (join(s4) - {b}) U {y, x} = (s5 - {b}) U {y, x} s3 = join(s3) - {s} = s4 - {s} s2 = join(s2) - {y} = s3 - {y} s1 = (join(s1) - {x}) U {input} = (s2 - {x}) U {input} For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order. By turning the above equation system to a monotonic function \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) & = & \\left ( \\begin{array}{c} \\{\\}, \\\\ \\{s\\}, \\\\ s_4, \\\\ (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\ (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\ (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\ s_6 \\cup s_{10} \\cup \\{b\\}, \\\\ (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\ s_4 - \\{s\\}, \\\\ s_3 - \\{y\\}, \\\\ (s_2 - \\{x\\}) \\cup \\{ input \\} \\end{array} \\right ) \\end{array} \\] Question, can you show that \\(f_1\\) is a monotonic function? By applying the naive fixed point algorithm (or its optimized version) with starting states s1 = ... = s11 = {} , we solve the above constraints and find s11 = {} s10 = {s} s9 = {y,x,s} s8 = {y,x,s} s7 = {y,x,s} s6 = {y,x,s} s5 = {y,x,s,b} s4 = {y,x,s} s3 = {y, x} s2 = {x} s1 = {input} From which we can identify at least two possible optimization opportunities. t is must be dead throughout the entire program. Hence instruction 7 is redundant. input only lives at instruction 1. If it is not holding any heap references, it can be freed. x,y,b lives until instruction 9. If they are not holding any heap references, they can be freed. Forward vs Backward Analysis Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis . Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a backward analysis . For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis. May Analysis vs Must Analysis Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\) ) instead of \\(\\sqcup\\) (which is \\(\\cup\\) ).","title":"50.054 - Liveness Analysis"},{"location":"notes/liveness_analysis/#50054-liveness-analysis","text":"","title":"50.054 - Liveness Analysis"},{"location":"notes/liveness_analysis/#learning-outcomes","text":"Define the liveness analysis problem Apply lattice and fixed point algorithm to solve the liveness analysis problem","title":"Learning Outcomes"},{"location":"notes/liveness_analysis/#recall","text":"// SIMP1 x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; In the above program the statement t = s is redundant as t is not used. It can be statically detected by a liveness analysis.","title":"Recall"},{"location":"notes/liveness_analysis/#liveness-analysis","text":"A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\) . Otherwise, the variable is considered not live or dead . Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite. By applying this analysis to the above program, we can find out at the program locations where variables must be dead.","title":"Liveness Analysis"},{"location":"notes/liveness_analysis/#defining-the-lattice-for-livenesss-analysis","text":"Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\) . Applying this approach the liveness analysis, we consider the powerset the set of all variables in the program. Let's recast the SIMP1 program into pseudo assembly, let's label it as PA1 1: x <- input 2: y <- 0 3: s <- 0 4: b <- y < x 5: ifn b goto 10 6: y <- y + 1 7: t <- s 8: s <- s + y 9: goto 4 10: rret <- s 11: ret In PA1 we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\) , if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\) , we see the following hasse diagram graph TD; N58[\"{b}\"] --- N64[\"{}\"] N59[\"{t}\"] --- N64[\"{}\"] N60[\"{s}\"] --- N64[\"{}\"] N61[\"{y}\"] --- N64[\"{}\"] N62[\"{x}\"] --- N64[\"{}\"] N63[\"{input}\"] --- N64[\"{}\"] N43[\"{t,b}\"] --- N58[\"{b}\"] N44[\"{s,b}\"] --- N58[\"{b}\"] N46[\"{y,b}\"] --- N58[\"{b}\"] N49[\"{x,b}\"] --- N58[\"{b}\"] N53[\"{input,b}\"] --- N58[\"{b}\"] N43[\"{t,b}\"] --- N59[\"{t}\"] N45[\"{s,t}\"] --- N59[\"{t}\"] N47[\"{y,t}\"] --- N59[\"{t}\"] N50[\"{x,t}\"] --- N59[\"{t}\"] N54[\"{input,t}\"] --- N59[\"{t}\"] N44[\"{s,b}\"] --- N60[\"{s}\"] N45[\"{s,t}\"] --- N60[\"{s}\"] N48[\"{y,s}\"] --- N60[\"{s}\"] N51[\"{x,s}\"] --- N60[\"{s}\"] N55[\"{input,s}\"] --- N60[\"{s}\"] N46[\"{y,b}\"] --- N61[\"{y}\"] N47[\"{y,t}\"] --- N61[\"{y}\"] N48[\"{y,s}\"] --- N61[\"{y}\"] N52[\"{x,y}\"] --- N61[\"{y}\"] N56[\"{input,y}\"] --- N61[\"{y}\"] N49[\"{x,b}\"] --- N62[\"{x}\"] N50[\"{x,t}\"] --- N62[\"{x}\"] N51[\"{x,s}\"] --- N62[\"{x}\"] N52[\"{x,y}\"] --- N62[\"{x}\"] N57[\"{input,x}\"] --- N62[\"{x}\"] N53[\"{input,b}\"] --- N63[\"{input}\"] N54[\"{input,t}\"] --- N63[\"{input}\"] N55[\"{input,s}\"] --- N63[\"{input}\"] N56[\"{input,y}\"] --- N63[\"{input}\"] N57[\"{input,x}\"] --- N63[\"{input}\"] N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\) . The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\) .","title":"Defining the Lattice for Livenesss Analysis"},{"location":"notes/liveness_analysis/#defining-the-monotone-constraint-for-liveness-analysis","text":"In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed. In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\) . In Sign Analysis the \\(join(s_i)\\) function is defined as the least upper bound of all the states that are preceding \\(s_i\\) in the control flow. In Liveness Analysis, we define the \\(join(s_i)\\) function as follows \\[ join(s_i) = \\bigsqcup succ(s_i) \\] where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph. The monotonic functions can be defined by the following cases. case \\(l:ret\\) , \\(s_l = \\{\\}\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\) case \\(l: r \\leftarrow src\\) , \\(s_l = join(s_l) \\cup var(src)\\) case \\(l: r \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\) case \\(l: ifn\\ t\\ goto\\ l'\\) , \\(s_l = join(s_l) \\cup \\{ t \\}\\) other cases: \\(s_l = join(s_l)\\) The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\) . \\[ \\begin{array}{rcl} var(r) & = & \\{ \\} \\\\ var(t) & = & \\{ t \\} \\\\ var(c) & = & \\{ \\} \\end{array} \\] By applying the PA program above we have s11 = {} s10 = join(s10) U {s} = {s} s9 = join(s9) = s4 s8 = (join(s8) - {s}) U {s, y} = (s9 - {s}) U {s, y} s7 = (join(s7) - {t}) U {s} = (s8 - {t}) U {s} s6 = (join(s6) - {y}) U {y} = (s7 - {y}) U {y} s5 = join(s5) U {b} = s6 U s10 U {b} s4 = (join(s4) - {b}) U {y, x} = (s5 - {b}) U {y, x} s3 = join(s3) - {s} = s4 - {s} s2 = join(s2) - {y} = s3 - {y} s1 = (join(s1) - {x}) U {input} = (s2 - {x}) U {input} For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order. By turning the above equation system to a monotonic function \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) & = & \\left ( \\begin{array}{c} \\{\\}, \\\\ \\{s\\}, \\\\ s_4, \\\\ (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\ (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\ (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\ s_6 \\cup s_{10} \\cup \\{b\\}, \\\\ (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\ s_4 - \\{s\\}, \\\\ s_3 - \\{y\\}, \\\\ (s_2 - \\{x\\}) \\cup \\{ input \\} \\end{array} \\right ) \\end{array} \\] Question, can you show that \\(f_1\\) is a monotonic function? By applying the naive fixed point algorithm (or its optimized version) with starting states s1 = ... = s11 = {} , we solve the above constraints and find s11 = {} s10 = {s} s9 = {y,x,s} s8 = {y,x,s} s7 = {y,x,s} s6 = {y,x,s} s5 = {y,x,s,b} s4 = {y,x,s} s3 = {y, x} s2 = {x} s1 = {input} From which we can identify at least two possible optimization opportunities. t is must be dead throughout the entire program. Hence instruction 7 is redundant. input only lives at instruction 1. If it is not holding any heap references, it can be freed. x,y,b lives until instruction 9. If they are not holding any heap references, they can be freed.","title":"Defining the Monotone Constraint for Liveness Analysis"},{"location":"notes/liveness_analysis/#forward-vs-backward-analysis","text":"Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis . Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a backward analysis . For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis.","title":"Forward vs Backward Analysis"},{"location":"notes/liveness_analysis/#may-analysis-vs-must-analysis","text":"Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\) ) instead of \\(\\sqcup\\) (which is \\(\\cup\\) ).","title":"May Analysis vs Must Analysis"},{"location":"notes/name_analysis/","text":"50.054 - Name Analysis Learning Outcomes Articulate the purpose of name analysis. Describe the properties of the static single assignment forms. Implement the static single assignment construction and deconstruction algorithms. What is Name Analysis Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name). Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement. Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit. What is the scope of the variable? Has the variable been declared before used? Where is the defined variable used? Variable Scope Consider the following Python program, x = -1 def f(): x = 1 return g() def g(): print(x) f() When the program is executed, we observe -1 being printed. The variable x=1 in f() does not modify the x=-1 in the outer scope. Hence when g() is called, the variable x being printed is from the global scope x=-1 . This is known as static scoping . Static Variable Scoping For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree). graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; Main --> g; g --> usex1[\"print(x)\"]; Thus the print(x) of g uses the x defined in its parent node. Dynamic Variable Scoping For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. $x = -1; sub f { local $x = 1; return g(); } sub g { print $x; } f() In the above, it is the same program coded in perl . Except that in perl, variables with local are defined using dynamic scoping. As a result, 1 is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; f --> g; g --> usex1[\"print(x)\"]; As illustrated by the dynamic call graph above, the variable x in print(x) refers to g 's caller, i.e. f , which is 1 . More On Static Variable Scoping Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation. Consider the following Python program. def main(argv): x = 1 if len(argv) == 0: x = 2 else: y = 1 print(y) when the input argv is a non-empty list, the function main prints 1 as results. However when argv is an empty list, a run-time error arises. Consider the \"nearly-the-same\" program in Java. class Main { public static int main(String[] argv) { int x = 1; if (argv.length > 0){ x = 2; } else { int y = 1; } System.out.println(y.toString()); return 1; } } Java returns a compilation error, complaining variable y being use in the System.out.println function can't be resolved. The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable y 's scope is only within the else branch but not outside. In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. However how might we detect the run-time error similar to what we've observed from the last Python example? Let's recast the example in SIMP, let's call it SIMP_ERR1 // SIMP_ERR1 x = 1; if input == 0 { x = 2; } else { y = 1; } return y; The above program will cause an error when input == 0 . It is typeable based on the type inference algorithm we studied in the previous class. Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program. // PA_ERR1 1: x <- 1 2: t <- input == 0 3: ifn t goto 6 4: x <- 2 5: goto 7 6: y <- 1 7: rret <- y 8: ret Same error arises when input == 0 . Static Single Assignment form Static Single Assignment (SSA) form is an intermediate representation widely used in compiler design and program verification. In a static single assignment form, Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. \\(\\phi\\) -assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). SSA form construction is one of the effective ways to analysis the scope of variables the use-def relationship of variables Unstructured SSA Form Suppose we extend the pseudo assembly with \\(\\phi\\) -assignment statements, \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : \\overline{\\phi}\\ i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt PhiAssignment) & \\phi & ::= & d \\leftarrow phi(\\overline{l:s}) \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\mid ... \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\) . (which could be empty) before the actual instruction \\(i\\) . When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax. we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms. Suppose we have the following pseudo assembly program // PA1 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that variables s and c are re-assigned in the loop. The SSA form of the above is // SSA_PA1 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: rret <- s1 10: ret In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable. There are two possible preceding instructions that lead us to the following instruction 4: s1 <- phi(3:s0, 9:s2) c1 <- phi(3:c0, 9:c2) namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign s0 to s1 and c0 to c1 . Otherwise, s2 is assigned to s1 and c2 is assigned to c1 . To cater for the phi assignment, we extend the small step operational semantics from \\( \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) \\) to \\[P \\vdash (L, li, p) \\longrightarrow (L', li', p')\\] The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l: d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\\\ \\\\ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\\\ \\\\ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\\\ \\\\ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l) \\end{array} \\\\ {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l) \\end{array} \\\\ {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l) \\end{array} \\end{array} \\] All the existing rules are required some minor changes to accomodate the third component in the program context. The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments. In the presence of phi-assignments, we need the following rules to guide the execution. \\[ \\begin{array}{rc} {\\tt (pPhi1)} & \\begin{array}{c} (L, l: []\\ i, p) \\longrightarrow (L, l: i, p) \\end{array} \\\\ \\\\ {\\tt (pPhi2)} & \\begin{array}{c} l_i = p\\ \\ \\ c_i = L(s_i) \\\\ j \\in [1,i-1]: l_j \\neq p \\\\ \\hline (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p) \\end{array} \\end{array} \\] The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\) 's operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\) , i.e. \\(c_i\\) . Add the new entry \\((d,c_i)\\) to the local environment \\(L\\) . Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\) . Given \\(input = 1\\) , excuting SSA_PA1 yields the following derivation P |- {(input,1)}, 1: x0 <- input, undef ---> # (pTempVar) P |- {(input,1), (x0,1)}, 2: s0 <- 0, 1 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0)}, 3: c0 <- 0, 2 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 <- c1 < x0, 3 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 <- c1 < x0, 3 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 <- c1 + s1, 5 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 <- c1 + 1, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7 ---> # (pGoto) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 <- c1 < x0, 8 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 <- c1 < x0, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret <- s1, 5 ---> # (pTempVar) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9 Minimality One may argue that instead of generating SSA_PA1 , one might generate the following static single assignment // SSA_PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: s3 <- phi(5:s1) rret <- s3 10: ret which will yield the same output. However we argue that SSA_PA1 is preferred as it has the minimal number of phi assignments. SSA Construction Algorithm The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al. https://doi.org/10.1145/115372.115320 The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form. Control flow graph We can model a Pseudo Assembly program using a graph, namely the contorl flow graph. For example, PA1 can be represented as the following Control flow graph Graph1_PA1 graph TD; B1(\"1: x <- input 2: s <- 0 3: c <- 0\")-->B2; B2-->B3; B2(\"4: t <- c < x 5: ifn t goto 9\")-->B4(\"9: rret <- s 10: ret\"); B3(\"6: s <- c + s 7: c <- c + 1 8: goto 4\")-->B2; For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it Graph2_PA1 graph TD; V1(\"1: x <- input\") --> V2; V2(\"2: s <- 0\") --> V3; V3(\"3: c <- 0\") --> V4; V4(\"4: t <- c < x\") --> V5; V5(\"5: ifn t goto 9\") --> V9; V9(\"9: rret <- s\") --> V10(\"10: ret\") V5(\"5: ifn t goto 9\") --> V6; V6(\"6: s <- c + s\") --> V7; V7(\"7: c <- c + 1\") --> V8; V8(\"8: goto 4\") --> V4; Now we refer to the vertex in a control flow graph by the label. The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG. Identifying the \"right\" locations Definition 1 - Graph Let \\(G\\) be a graph, \\(G = (V, E)\\) , where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\) , \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\) . Occassionally, we also refer to a vertex as a node in the graph. For convenience, we also write \\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and \\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\) . Definition 2 - Path Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say a path from \\(v_1\\) to \\(v_2\\) , written as \\(path(v_1,v_2)\\) , exists iff \\(v_1 = v_2\\) or the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\) . For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\) . Definition 3 - Connectedness Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\) , iff \\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\) . Definition 4 - Source and Sink Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Assumption Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\) , we have \\(connect(v_1,v_2)\\) Has only one source vertex, which means there is only one entry point to the program. Has only one sink vertex, which means there is only one return statement. Definition 5 - Dominance Relation Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) dominates \\(v_2\\) , written as \\(v_1 \\preceq v_2\\) , iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\) . In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\) , we definitely pass through location \\(v_1\\) . For instance, in the earlier control flow graph for Graph2_PA1 , the vertex 1 dominates all vertices. the vertex 4 dominates itself, the vertices 5,6,7,8,9,10 . Lemma 1 - Dominance is transitive \\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\) . Lemma 2 - Domaince is reflexive For any vertex \\(v\\) , we have \\(v \\preceq v\\) . Defintion 6 - Strict Dominance We say \\(v_1\\) stricly domainates \\(v_2\\) , written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\) . Defintion 7 - Immediate Dominator We say \\(v_1\\) is the immediate dominator of \\(v_2\\) , written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\) . Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists. Dominator Tree Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\) . Each vertex \\(v \\in G\\) forms a node in the dominator tree. For vertices \\(v_1, v_2 \\in G\\) , \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\) . For example, from the CFG Graph2_PA1 , we construct a dominator tree Tree2_PA1 , as follows, graph TD; 1 --> 2; 2 --> 3; 3 --> 4; 4 --> 5; 5 --> 6; 6 --> 7; 7 --> 8; 5 --> 9; 9 --> 10; Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\) . Definition 8 - Dominance Frontier Let \\(v\\) be vertex in a graph \\(G\\) , we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$ In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\) ). For instance, in our running example, the dominance frontier of vertex 6 is the set containing vertex 4 This is because vertex 8 is one of the predecesors of the vertex 4 and vertex 8 is dominated by vertex 6 , but not the vertex 4 is not domainated by vertex 6 . Question: what is the dominance frontier of vertex 5 ? Computing Dominance Frontier The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG. Re-definining Dominance Frontier The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\) . We define $$ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G) ~~~(E1) $$ where \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] and \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] \\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\) \\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\) ) that are not dominated by \\(v\\) . \\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\) , by finding vertices \\(w\\) in \\(v\\) 's dominance frontier, such that \\(w\\) is not dominated by \\(v\\) 's immediate dominator (i.e. \\(v\\) 's parent in the dominator tree). Cytron et al shows that \\((E1)\\) defines the same result as Definition 6. Dominance frontier algorithm As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex). The algorithm is structured as follows For each vertex \\(v\\) by traversing the dominator tree bottom up: compute \\(df_{local}(v,G)\\) compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\) , which can be looked up from the a memoization table. save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table. For instance, we make use of Graph2_PA1 and Tree2_PA1 to construct the following memoization table Table2_PA1 vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} From the above table, we conclude that variables that are updated in vertices 5,6,7,8 should be merged via phi-assignments at the entry point of vertex 4 . As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid (x,y)\\in G \\wedge idom(y) \\neq x \\}\\) Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\) Note that in Cytron's paper, they include two special vertices, entry the entry vertex, and exit as the exit, and entry dominates everything, and exit is only dominated by entry . The purpose is to handle langugage allowing multiple return statements. Definition 9 - Iterative Dominance Frontier As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\) , a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\) . However inserting a phi assignment at the dominance fronter of \\(v\\) introduces a new location of modifying the variable \\(x\\) . This leads to some \"cascading effect\" in computing the phi-assignment locations. We extend the dominance frontier to handle a set of vertices. Let \\(S\\) denote a set of vertices of a graph \\(G\\) . We define \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] We define the iterative dominance frontier recursively as follows \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\) , i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound. It follows that if a variable \\(x\\) is modified in locations \\(S\\) , then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\) . SSA construction algorithm Given the control flow graph \\(G\\) , the dominator tree \\(T\\) , and the dominance frontier table \\(DFT\\) , the SSA construction algorithm consists of two steps. insert phi assignments to the original program \\(P\\) . rename variables to ensure the single assignment property. Inserting Phi assignments Before inserting the phi assignments to \\(P\\) , we need some intermediate data structure. A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables. \\((l, S) \\in E\\) implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\) . \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation. Input: the original program P , can be viewed as a list of labeled instructions. Output: the modified program Q . can be viewed as a list of labeled instructions. The phi-assignment insertion process can be described as follows, Q = List() for each l:i in P match E.get(l) with case None add l:i to Q case Some(xs) phis = xs.map( x => x <- phi( k:x | (k in pred(l,G))) if phis has more than 1 operand, add l:phis i to Q \\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\) . For example, given PA1 , variable \\(x\\) is modified at 1 variable \\(s\\) is modified at 2,6 variable \\(c\\) is modified at 3,7 variable \\(t\\) is modified at 4 We construct \\(E\\) by consulting the dominance frontier table Table2_PA1 . E = Map( 4 -> Set(\"s\",\"c\", \"t\") ) which says that in node/vertex 4 , we should insert the phi-assignments for variable s and c . Now we apply the above algorithm to PA1 which generates // PRE_SSA_PA1 1: x <- input 2: s <- 0 3: c <- 0 4: s <- phi(3:s, 8:s) c <- phi(3:c, 8:c) t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that when we try to insert the phi assignment for t at 4 , we realize that there is only one operand. This is because t is not defined before label 4 . In this case we remove the phi assignment for t . Renaming Variables Given an intermediate output like PRE_SSA_PA1 , we need to rename the variable so that there is only one assignment for each variable. Inputs: a dictionary of stacks K where the keys are the variable names in the original PA program. e.g. K(x) returns the stack for variable x . the input program in with phi assignment but oweing the variable renaming, e.g. PRE_SSA_PA1 . We view the program as a dictionary mapping labels to labeled instructions. For each variable x in the program, initialize K(x) = Stack() . Let label l be the root of the dominator tree \\(T\\) . Let vars be an empty list Match P(l) with case l: phis r <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) set Q(l) to l: phis' r <- s' case l: phis r <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) set Q(l) to l: phis' r <- s1' op s2' case l: phis x <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s' case l: phis x <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s1' op s2' case l: phis ifn t goto l' (phis', K, result_list) = processphi(phis, K, vars) t' = ren(K, t) set Q(l) to l: phis' ifn t' goto l' case l: phis ret (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' ret case l: phis goto l' (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' goto l' For each successor k of l in the CFG \\(G\\) R = if k in Q { Q } else { R } Pattern match R(k) case k: phis i for each x <- phi(j:x', m:x'') in phis if K(origin(x)) is empty, do not add this phi assignment in the result list**. if j == l , x <- phi(j:ren(K,x'), m:x'') into the result list if m == l , x <- phi(j:x', m:ren(K,x'')) into the result list the result list is phis' update R(k) to k: phis' i case others , no change Recursively apply step 3 to the children of l in the \\(T\\) . For each x in vars , K(x).pop() Where ren(K, s) is defined as ren(K,c) = c ren(K, input) = input ren(K, r) = r ren(K, t) = K(t).peek() match case None => error(\"variable use before being defined.\") case Some(i) => t_i and next(K, x) is defined as next(K, x) = K(x).peek() match case None => K(x).push(1) 0 case Some(i) => K(x).push(i+1) i and processphi(phis, K) is defined as prcessphi(phis, K, vars) = foreach x <- phi(j:x', k:x'') in phis i = K(x).peek() + 1 K(x).push(i) append x to vars put x_i <- phi(j:x', k:x'') into result_list return (result_list, K, vars) and stem(x) returns the original version of x before renaming, e.g. stem(x) = x and stem(x1) = x . We assume there exists some book-keeping mechanism to keep track of that the fact that x is the origin form of x_1 . Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch. We describe the application the algorithm to PRE_SSA_PA1 (with the dominator tree Tree2_PA1 and CFG Graph1_PA1 ) with the following table. label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 1:x<-input 1:x0<-input {x:[0], s:[], c:[], t:[]} {1:{x}} 2 2:s<-0 2:s0<-0 {x:[0], s:[0], c:[], t:[]} {1:{x}, 2:{s}} 3 3:c<-0 3:c0<-0 {x:[0], s:[0], c:[0], t:[]} 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x {1:{x}, 2:{s}. 3:{c}} 4 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x 4:s1<-phi(3:s0,8:s);c1<-phi(3:c0,8:c);t0<-c1<x0 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 5 5:ifn t goto 9 5:ifn t0 goto 9 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 6 6:s<-c+s 6:s2<-c1+s1 {x:[0], s:[0,1,2], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}} 7 7:c<-c+1 7:c2<-c1+1 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}} 8 8:goto 4 8:goto 4 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} 4:s1<-phi(3:s0,8:s2);c1<-phi(3:c0,8:c2);t0<-c1<x0 9 9:rret<-s 9:rret<-s1 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 10 10:ret 10:ret {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} The label column denotes the current label being considered. The P(l) column denotes the input labeled instruction being considered. The Q(l) column denotes the output labeled instruction. The K column denotes the set of stacks after the current recursive call. The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q) The Q(succ(l)) column denotes the modified successor instruction in Q. The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call). The above derivation eventually yield SSA_PA1 . Note that in case of a variable being use before initialized, ren(K, t) will raise an error. SSA back to Pseudo Assembly To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating SSA_PA1 back to PA while keeping the renamed variables, we have // PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 4: t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: s1 <- s2 8: c1 <- c2 8: goto 4 9: rret <- s1 10: ret In the above we break the phi-assignments found in 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 into PhiFor3 s1 <- s0 // for label 3 c1 <- c0 and PhiFor8 s1 <- s2 // for label 8 c1 <- c2 We move PhiFor3 to label 3 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 and PhiFor8 to label 8 8: s1 <- s2 8: c1 <- c2 8: goto 4 The \"moving\" phi-assignment operation can be defined in the following algorithm. Relocating the phi-assignments Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\) ) For each \\(l: \\overline{\\phi}\\ i \\in P\\) , append \\(l: i\\) to \\(Q\\) . For each \\(l: \\overline{\\phi}\\ i\\) . For each x = phi(l1:x1, l2:x2) in \\(\\overline{\\phi}\\) append l1:x <- x1 and l2:x <- x2 to \\(Q\\) . note that the relocated assignment must be placed before the control flow transition from l1 to succ(l1) (and l2 to succ(l2) ) Sort \\(Q\\) by labels using a stable sorting algorithm. Now since there are repeated labels in PA2 , we need an extra relabelling step to convert PA2 to PA3 // PA3 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- s0 5: c1 <- c0 6: t0 <- c1 < x0 7: ifn t0 goto 11 8: s2 <- c1 + s1 9: c2 <- c1 + 1 10: s1 <- s2 11: c1 <- c2 12: goto 4 13: rret <- s1 14: ret Relabelling This re-labeling step can be described in the following algorithm. Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\) ) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. Initialize a counter c = 1 , Initialize a mapping from old label to new label, M = Map() . Initialize \\(Q\\) as an empty list For each l: i \\(\\in P\\) M = M + (l -> c) incremeant c by 1 For each l: i \\(\\in P\\) append M(l): relabel(i, M) to \\(Q\\) where relabel(i, M) is defined as follows relabel(ifn t goto l,M) = ifn t goto M(l) relabel(goto l, M) = goto M(l) relabel(i, M) = i Structured SSA Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Can be converted into a structured SSA x1 = input; s1 = 0; c1 = 0; join { s2 = phi(s1,s3); c2 = phi(c1,c3); } while c2 < x1 { s3 = c2 + s2; c3 = c2 + 1; } return s2; In the above SSA form, we have a join ... while ... loop. The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and also the body of the loop. (Similarly we can introduce a if ... else ... join ... statement). Structured SSA allows us to conduct name analysis closer to the source language. conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. perform code obfuscation. Futher Readings https://dl.acm.org/doi/10.1145/2955811.2955813 https://dl.acm.org/doi/abs/10.1145/3605156.3606457 https://dl.acm.org/doi/10.1145/202530.202532","title":"50.054 - Name Analysis"},{"location":"notes/name_analysis/#50054-name-analysis","text":"","title":"50.054 - Name Analysis"},{"location":"notes/name_analysis/#learning-outcomes","text":"Articulate the purpose of name analysis. Describe the properties of the static single assignment forms. Implement the static single assignment construction and deconstruction algorithms.","title":"Learning Outcomes"},{"location":"notes/name_analysis/#what-is-name-analysis","text":"Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name). Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement. Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit. What is the scope of the variable? Has the variable been declared before used? Where is the defined variable used?","title":"What is Name Analysis"},{"location":"notes/name_analysis/#variable-scope","text":"Consider the following Python program, x = -1 def f(): x = 1 return g() def g(): print(x) f() When the program is executed, we observe -1 being printed. The variable x=1 in f() does not modify the x=-1 in the outer scope. Hence when g() is called, the variable x being printed is from the global scope x=-1 . This is known as static scoping .","title":"Variable Scope"},{"location":"notes/name_analysis/#static-variable-scoping","text":"For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree). graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; Main --> g; g --> usex1[\"print(x)\"]; Thus the print(x) of g uses the x defined in its parent node.","title":"Static Variable Scoping"},{"location":"notes/name_analysis/#dynamic-variable-scoping","text":"For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. $x = -1; sub f { local $x = 1; return g(); } sub g { print $x; } f() In the above, it is the same program coded in perl . Except that in perl, variables with local are defined using dynamic scoping. As a result, 1 is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the graph TD; Main --> x1[\"x=-1\"]; Main --> f; f --> x2[x=1]; f --> g; g --> usex1[\"print(x)\"]; As illustrated by the dynamic call graph above, the variable x in print(x) refers to g 's caller, i.e. f , which is 1 .","title":"Dynamic Variable Scoping"},{"location":"notes/name_analysis/#more-on-static-variable-scoping","text":"Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation. Consider the following Python program. def main(argv): x = 1 if len(argv) == 0: x = 2 else: y = 1 print(y) when the input argv is a non-empty list, the function main prints 1 as results. However when argv is an empty list, a run-time error arises. Consider the \"nearly-the-same\" program in Java. class Main { public static int main(String[] argv) { int x = 1; if (argv.length > 0){ x = 2; } else { int y = 1; } System.out.println(y.toString()); return 1; } } Java returns a compilation error, complaining variable y being use in the System.out.println function can't be resolved. The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable y 's scope is only within the else branch but not outside. In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. However how might we detect the run-time error similar to what we've observed from the last Python example? Let's recast the example in SIMP, let's call it SIMP_ERR1 // SIMP_ERR1 x = 1; if input == 0 { x = 2; } else { y = 1; } return y; The above program will cause an error when input == 0 . It is typeable based on the type inference algorithm we studied in the previous class. Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program. // PA_ERR1 1: x <- 1 2: t <- input == 0 3: ifn t goto 6 4: x <- 2 5: goto 7 6: y <- 1 7: rret <- y 8: ret Same error arises when input == 0 .","title":"More On Static Variable Scoping"},{"location":"notes/name_analysis/#static-single-assignment-form","text":"Static Single Assignment (SSA) form is an intermediate representation widely used in compiler design and program verification. In a static single assignment form, Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. \\(\\phi\\) -assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). SSA form construction is one of the effective ways to analysis the scope of variables the use-def relationship of variables","title":"Static Single Assignment form"},{"location":"notes/name_analysis/#unstructured-ssa-form","text":"Suppose we extend the pseudo assembly with \\(\\phi\\) -assignment statements, \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) & li & ::= & l : \\overline{\\phi}\\ i \\\\ (\\tt Instruction) & i & ::= & d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\ (\\tt PhiAssignment) & \\phi & ::= & d \\leftarrow phi(\\overline{l:s}) \\\\ (\\tt Labeled\\ Instructions) & lis & ::= & li \\mid li\\ lis \\\\ (\\tt Operand) & d,s & ::= & r \\mid c \\mid t \\\\ (\\tt Temp\\ Var) & t & ::= & x \\mid y \\mid ... \\\\ (\\tt Label) & l & ::= & 1 \\mid 2 \\mid ... \\\\ (\\tt Operator) & op & ::= & + \\mid - \\mid < \\mid == \\mid ... \\\\ (\\tt Constant) & c & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\\\ (\\tt Register) & r & ::= & r_{ret} \\mid r_1 \\mid r_2 \\mid ... \\end{array} \\] The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\) . (which could be empty) before the actual instruction \\(i\\) . When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax. we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms. Suppose we have the following pseudo assembly program // PA1 1: x <- input 2: s <- 0 3: c <- 0 4: t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that variables s and c are re-assigned in the loop. The SSA form of the above is // SSA_PA1 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: rret <- s1 10: ret In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable. There are two possible preceding instructions that lead us to the following instruction 4: s1 <- phi(3:s0, 9:s2) c1 <- phi(3:c0, 9:c2) namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign s0 to s1 and c0 to c1 . Otherwise, s2 is assigned to s1 and c2 is assigned to c1 . To cater for the phi assignment, we extend the small step operational semantics from \\( \\(P \\vdash (L, li) \\longrightarrow (L', li')\\) \\) to \\[P \\vdash (L, li, p) \\longrightarrow (L', li', p')\\] The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l: d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\\\ \\\\ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\\\ \\\\ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\\\ \\\\ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} & \\begin{array}{c} c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2 \\\\ \\hline P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l) \\end{array} \\\\ {\\tt (pIfn0)} & \\begin{array}{c} L(s) = 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l) \\end{array} \\\\ {\\tt (pIfnNot0)} & \\begin{array}{c} L(s) \\neq 0 \\\\ \\hline P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l) \\end{array} \\end{array} \\] All the existing rules are required some minor changes to accomodate the third component in the program context. The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments. In the presence of phi-assignments, we need the following rules to guide the execution. \\[ \\begin{array}{rc} {\\tt (pPhi1)} & \\begin{array}{c} (L, l: []\\ i, p) \\longrightarrow (L, l: i, p) \\end{array} \\\\ \\\\ {\\tt (pPhi2)} & \\begin{array}{c} l_i = p\\ \\ \\ c_i = L(s_i) \\\\ j \\in [1,i-1]: l_j \\neq p \\\\ \\hline (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p) \\end{array} \\end{array} \\] The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\) 's operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\) , i.e. \\(c_i\\) . Add the new entry \\((d,c_i)\\) to the local environment \\(L\\) . Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\) . Given \\(input = 1\\) , excuting SSA_PA1 yields the following derivation P |- {(input,1)}, 1: x0 <- input, undef ---> # (pTempVar) P |- {(input,1), (x0,1)}, 2: s0 <- 0, 1 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0)}, 3: c0 <- 0, 2 ---> # (pConst) P |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 3 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 <- c1 < x0, 3 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 <- c1 < x0, 3 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 <- c1 + s1, 5 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 <- c1 + 1, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7 ---> # (pGoto) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 <- phi(3:s0, 9:s2); c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 <- phi(3:c0, 9:c2) t0 <- c1 < x0, 8 ---> # (pPhi2) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 <- c1 < x0, 8 ---> # (pPhi1) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 <- c1 < x0, 8 ---> # (pOp) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---> # (pIfn0) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret <- s1, 5 ---> # (pTempVar) P |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9","title":"Unstructured SSA Form"},{"location":"notes/name_analysis/#minimality","text":"One may argue that instead of generating SSA_PA1 , one might generate the following static single assignment // SSA_PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: goto 4 9: s3 <- phi(5:s1) rret <- s3 10: ret which will yield the same output. However we argue that SSA_PA1 is preferred as it has the minimal number of phi assignments.","title":"Minimality"},{"location":"notes/name_analysis/#ssa-construction-algorithm","text":"The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al. https://doi.org/10.1145/115372.115320 The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form.","title":"SSA Construction Algorithm"},{"location":"notes/name_analysis/#control-flow-graph","text":"We can model a Pseudo Assembly program using a graph, namely the contorl flow graph. For example, PA1 can be represented as the following Control flow graph Graph1_PA1 graph TD; B1(\"1: x <- input 2: s <- 0 3: c <- 0\")-->B2; B2-->B3; B2(\"4: t <- c < x 5: ifn t goto 9\")-->B4(\"9: rret <- s 10: ret\"); B3(\"6: s <- c + s 7: c <- c + 1 8: goto 4\")-->B2; For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it Graph2_PA1 graph TD; V1(\"1: x <- input\") --> V2; V2(\"2: s <- 0\") --> V3; V3(\"3: c <- 0\") --> V4; V4(\"4: t <- c < x\") --> V5; V5(\"5: ifn t goto 9\") --> V9; V9(\"9: rret <- s\") --> V10(\"10: ret\") V5(\"5: ifn t goto 9\") --> V6; V6(\"6: s <- c + s\") --> V7; V7(\"7: c <- c + 1\") --> V8; V8(\"8: goto 4\") --> V4; Now we refer to the vertex in a control flow graph by the label. The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG.","title":"Control flow graph"},{"location":"notes/name_analysis/#identifying-the-right-locations","text":"","title":"Identifying the \"right\" locations"},{"location":"notes/name_analysis/#definition-1-graph","text":"Let \\(G\\) be a graph, \\(G = (V, E)\\) , where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\) , \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\) . Occassionally, we also refer to a vertex as a node in the graph. For convenience, we also write \\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and \\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\) .","title":"Definition 1 - Graph"},{"location":"notes/name_analysis/#definition-2-path","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say a path from \\(v_1\\) to \\(v_2\\) , written as \\(path(v_1,v_2)\\) , exists iff \\(v_1 = v_2\\) or the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\) . For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\) .","title":"Definition 2 - Path"},{"location":"notes/name_analysis/#definition-3-connectedness","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\) , iff \\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\) .","title":"Definition 3 - Connectedness"},{"location":"notes/name_analysis/#definition-4-source-and-sink","text":"Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\) . Let \\(v\\) be a vertex in a graph \\(G\\) , we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\) .","title":"Definition 4 - Source and Sink"},{"location":"notes/name_analysis/#assumption","text":"Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\) , we have \\(connect(v_1,v_2)\\) Has only one source vertex, which means there is only one entry point to the program. Has only one sink vertex, which means there is only one return statement.","title":"Assumption"},{"location":"notes/name_analysis/#definition-5-dominance-relation","text":"Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\) . We say \\(v_1\\) dominates \\(v_2\\) , written as \\(v_1 \\preceq v_2\\) , iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\) . In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\) , we definitely pass through location \\(v_1\\) . For instance, in the earlier control flow graph for Graph2_PA1 , the vertex 1 dominates all vertices. the vertex 4 dominates itself, the vertices 5,6,7,8,9,10 .","title":"Definition 5 - Dominance Relation"},{"location":"notes/name_analysis/#lemma-1-dominance-is-transitive","text":"\\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\) .","title":"Lemma 1 - Dominance is transitive"},{"location":"notes/name_analysis/#lemma-2-domaince-is-reflexive","text":"For any vertex \\(v\\) , we have \\(v \\preceq v\\) .","title":"Lemma 2 - Domaince is reflexive"},{"location":"notes/name_analysis/#defintion-6-strict-dominance","text":"We say \\(v_1\\) stricly domainates \\(v_2\\) , written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\) .","title":"Defintion 6 - Strict Dominance"},{"location":"notes/name_analysis/#defintion-7-immediate-dominator","text":"We say \\(v_1\\) is the immediate dominator of \\(v_2\\) , written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\) . Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists.","title":"Defintion 7 - Immediate Dominator"},{"location":"notes/name_analysis/#dominator-tree","text":"Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\) . Each vertex \\(v \\in G\\) forms a node in the dominator tree. For vertices \\(v_1, v_2 \\in G\\) , \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\) . For example, from the CFG Graph2_PA1 , we construct a dominator tree Tree2_PA1 , as follows, graph TD; 1 --> 2; 2 --> 3; 3 --> 4; 4 --> 5; 5 --> 6; 6 --> 7; 7 --> 8; 5 --> 9; 9 --> 10; Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\) .","title":"Dominator Tree"},{"location":"notes/name_analysis/#definition-8-dominance-frontier","text":"Let \\(v\\) be vertex in a graph \\(G\\) , we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$ In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\) ). For instance, in our running example, the dominance frontier of vertex 6 is the set containing vertex 4 This is because vertex 8 is one of the predecesors of the vertex 4 and vertex 8 is dominated by vertex 6 , but not the vertex 4 is not domainated by vertex 6 . Question: what is the dominance frontier of vertex 5 ?","title":"Definition 8 - Dominance Frontier"},{"location":"notes/name_analysis/#computing-dominance-frontier","text":"The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG.","title":"Computing Dominance Frontier"},{"location":"notes/name_analysis/#re-definining-dominance-frontier","text":"The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\) . We define $$ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G) ~~~(E1) $$ where \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] and \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] \\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\) \\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\) ) that are not dominated by \\(v\\) . \\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\) , by finding vertices \\(w\\) in \\(v\\) 's dominance frontier, such that \\(w\\) is not dominated by \\(v\\) 's immediate dominator (i.e. \\(v\\) 's parent in the dominator tree). Cytron et al shows that \\((E1)\\) defines the same result as Definition 6.","title":"Re-definining Dominance Frontier"},{"location":"notes/name_analysis/#dominance-frontier-algorithm","text":"As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex). The algorithm is structured as follows For each vertex \\(v\\) by traversing the dominator tree bottom up: compute \\(df_{local}(v,G)\\) compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\) , which can be looked up from the a memoization table. save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table. For instance, we make use of Graph2_PA1 and Tree2_PA1 to construct the following memoization table Table2_PA1 vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} From the above table, we conclude that variables that are updated in vertices 5,6,7,8 should be merged via phi-assignments at the entry point of vertex 4 . As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid (x,y)\\in G \\wedge idom(y) \\neq x \\}\\) Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\) Note that in Cytron's paper, they include two special vertices, entry the entry vertex, and exit as the exit, and entry dominates everything, and exit is only dominated by entry . The purpose is to handle langugage allowing multiple return statements.","title":"Dominance frontier algorithm"},{"location":"notes/name_analysis/#definition-9-iterative-dominance-frontier","text":"As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\) , a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\) . However inserting a phi assignment at the dominance fronter of \\(v\\) introduces a new location of modifying the variable \\(x\\) . This leads to some \"cascading effect\" in computing the phi-assignment locations. We extend the dominance frontier to handle a set of vertices. Let \\(S\\) denote a set of vertices of a graph \\(G\\) . We define \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] We define the iterative dominance frontier recursively as follows \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\) , i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound. It follows that if a variable \\(x\\) is modified in locations \\(S\\) , then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\) .","title":"Definition 9 - Iterative Dominance Frontier"},{"location":"notes/name_analysis/#ssa-construction-algorithm_1","text":"Given the control flow graph \\(G\\) , the dominator tree \\(T\\) , and the dominance frontier table \\(DFT\\) , the SSA construction algorithm consists of two steps. insert phi assignments to the original program \\(P\\) . rename variables to ensure the single assignment property.","title":"SSA construction algorithm"},{"location":"notes/name_analysis/#inserting-phi-assignments","text":"Before inserting the phi assignments to \\(P\\) , we need some intermediate data structure. A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables. \\((l, S) \\in E\\) implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\) . \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation. Input: the original program P , can be viewed as a list of labeled instructions. Output: the modified program Q . can be viewed as a list of labeled instructions. The phi-assignment insertion process can be described as follows, Q = List() for each l:i in P match E.get(l) with case None add l:i to Q case Some(xs) phis = xs.map( x => x <- phi( k:x | (k in pred(l,G))) if phis has more than 1 operand, add l:phis i to Q \\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\) . For example, given PA1 , variable \\(x\\) is modified at 1 variable \\(s\\) is modified at 2,6 variable \\(c\\) is modified at 3,7 variable \\(t\\) is modified at 4 We construct \\(E\\) by consulting the dominance frontier table Table2_PA1 . E = Map( 4 -> Set(\"s\",\"c\", \"t\") ) which says that in node/vertex 4 , we should insert the phi-assignments for variable s and c . Now we apply the above algorithm to PA1 which generates // PRE_SSA_PA1 1: x <- input 2: s <- 0 3: c <- 0 4: s <- phi(3:s, 8:s) c <- phi(3:c, 8:c) t <- c < x 5: ifn t goto 9 6: s <- c + s 7: c <- c + 1 8: goto 4 9: rret <- s 10: ret Note that when we try to insert the phi assignment for t at 4 , we realize that there is only one operand. This is because t is not defined before label 4 . In this case we remove the phi assignment for t .","title":"Inserting Phi assignments"},{"location":"notes/name_analysis/#renaming-variables","text":"Given an intermediate output like PRE_SSA_PA1 , we need to rename the variable so that there is only one assignment for each variable. Inputs: a dictionary of stacks K where the keys are the variable names in the original PA program. e.g. K(x) returns the stack for variable x . the input program in with phi assignment but oweing the variable renaming, e.g. PRE_SSA_PA1 . We view the program as a dictionary mapping labels to labeled instructions. For each variable x in the program, initialize K(x) = Stack() . Let label l be the root of the dominator tree \\(T\\) . Let vars be an empty list Match P(l) with case l: phis r <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) set Q(l) to l: phis' r <- s' case l: phis r <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) set Q(l) to l: phis' r <- s1' op s2' case l: phis x <- s (phis', K, result_list) = processphi(phis, K, vars) s' = ren(K, s) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s' case l: phis x <- s1 op s2 (phis', K, result_list) = processphi(phis, K, vars) s1' = ren(K, s1) s2' = ren(K, s2) i = next(K,x) append x to vars set Q(l) to l: phis' x_i <- s1' op s2' case l: phis ifn t goto l' (phis', K, result_list) = processphi(phis, K, vars) t' = ren(K, t) set Q(l) to l: phis' ifn t' goto l' case l: phis ret (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' ret case l: phis goto l' (phis', K, result_list) = processphi(phis, K, vars) set Q(l) to l: phis' goto l' For each successor k of l in the CFG \\(G\\) R = if k in Q { Q } else { R } Pattern match R(k) case k: phis i for each x <- phi(j:x', m:x'') in phis if K(origin(x)) is empty, do not add this phi assignment in the result list**. if j == l , x <- phi(j:ren(K,x'), m:x'') into the result list if m == l , x <- phi(j:x', m:ren(K,x'')) into the result list the result list is phis' update R(k) to k: phis' i case others , no change Recursively apply step 3 to the children of l in the \\(T\\) . For each x in vars , K(x).pop() Where ren(K, s) is defined as ren(K,c) = c ren(K, input) = input ren(K, r) = r ren(K, t) = K(t).peek() match case None => error(\"variable use before being defined.\") case Some(i) => t_i and next(K, x) is defined as next(K, x) = K(x).peek() match case None => K(x).push(1) 0 case Some(i) => K(x).push(i+1) i and processphi(phis, K) is defined as prcessphi(phis, K, vars) = foreach x <- phi(j:x', k:x'') in phis i = K(x).peek() + 1 K(x).push(i) append x to vars put x_i <- phi(j:x', k:x'') into result_list return (result_list, K, vars) and stem(x) returns the original version of x before renaming, e.g. stem(x) = x and stem(x1) = x . We assume there exists some book-keeping mechanism to keep track of that the fact that x is the origin form of x_1 . Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch. We describe the application the algorithm to PRE_SSA_PA1 (with the dominator tree Tree2_PA1 and CFG Graph1_PA1 ) with the following table. label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 1:x<-input 1:x0<-input {x:[0], s:[], c:[], t:[]} {1:{x}} 2 2:s<-0 2:s0<-0 {x:[0], s:[0], c:[], t:[]} {1:{x}, 2:{s}} 3 3:c<-0 3:c0<-0 {x:[0], s:[0], c:[0], t:[]} 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x {1:{x}, 2:{s}. 3:{c}} 4 4:s<-phi(3:s0,8:s);c<-phi(3:c0,8:c);t<-c<x 4:s1<-phi(3:s0,8:s);c1<-phi(3:c0,8:c);t0<-c1<x0 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 5 5:ifn t goto 9 5:ifn t0 goto 9 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 6 6:s<-c+s 6:s2<-c1+s1 {x:[0], s:[0,1,2], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}} 7 7:c<-c+1 7:c2<-c1+1 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}} 8 8:goto 4 8:goto 4 {x:[0], s:[0,1,2], c:[0,1,2], t:[0]} 4:s1<-phi(3:s0,8:s2);c1<-phi(3:c0,8:c2);t0<-c1<x0 9 9:rret<-s 9:rret<-s1 {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} 10 10:ret 10:ret {x:[0], s:[0,1], c:[0,1], t:[0]} {1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}} The label column denotes the current label being considered. The P(l) column denotes the input labeled instruction being considered. The Q(l) column denotes the output labeled instruction. The K column denotes the set of stacks after the current recursive call. The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q) The Q(succ(l)) column denotes the modified successor instruction in Q. The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call). The above derivation eventually yield SSA_PA1 . Note that in case of a variable being use before initialized, ren(K, t) will raise an error.","title":"Renaming Variables"},{"location":"notes/name_analysis/#ssa-back-to-pseudo-assembly","text":"To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating SSA_PA1 back to PA while keeping the renamed variables, we have // PA2 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 4: t0 <- c1 < x0 5: ifn t0 goto 9 6: s2 <- c1 + s1 7: c2 <- c1 + 1 8: s1 <- s2 8: c1 <- c2 8: goto 4 9: rret <- s1 10: ret In the above we break the phi-assignments found in 4: s1 <- phi(3:s0, 8:s2) c1 <- phi(3:c0, 8:c2) t0 <- c1 < x0 into PhiFor3 s1 <- s0 // for label 3 c1 <- c0 and PhiFor8 s1 <- s2 // for label 8 c1 <- c2 We move PhiFor3 to label 3 3: c0 <- 0 3: s1 <- s0 3: c1 <- c0 and PhiFor8 to label 8 8: s1 <- s2 8: c1 <- c2 8: goto 4 The \"moving\" phi-assignment operation can be defined in the following algorithm.","title":"SSA back to Pseudo Assembly"},{"location":"notes/name_analysis/#relocating-the-phi-assignments","text":"Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\) ) For each \\(l: \\overline{\\phi}\\ i \\in P\\) , append \\(l: i\\) to \\(Q\\) . For each \\(l: \\overline{\\phi}\\ i\\) . For each x = phi(l1:x1, l2:x2) in \\(\\overline{\\phi}\\) append l1:x <- x1 and l2:x <- x2 to \\(Q\\) . note that the relocated assignment must be placed before the control flow transition from l1 to succ(l1) (and l2 to succ(l2) ) Sort \\(Q\\) by labels using a stable sorting algorithm. Now since there are repeated labels in PA2 , we need an extra relabelling step to convert PA2 to PA3 // PA3 1: x0 <- input 2: s0 <- 0 3: c0 <- 0 4: s1 <- s0 5: c1 <- c0 6: t0 <- c1 < x0 7: ifn t0 goto 11 8: s2 <- c1 + s1 9: c2 <- c1 + 1 10: s1 <- s2 11: c1 <- c2 12: goto 4 13: rret <- s1 14: ret","title":"Relocating the phi-assignments"},{"location":"notes/name_analysis/#relabelling","text":"This re-labeling step can be described in the following algorithm. Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\) ) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. Initialize a counter c = 1 , Initialize a mapping from old label to new label, M = Map() . Initialize \\(Q\\) as an empty list For each l: i \\(\\in P\\) M = M + (l -> c) incremeant c by 1 For each l: i \\(\\in P\\) append M(l): relabel(i, M) to \\(Q\\) where relabel(i, M) is defined as follows relabel(ifn t goto l,M) = ifn t goto M(l) relabel(goto l, M) = goto M(l) relabel(i, M) = i","title":"Relabelling"},{"location":"notes/name_analysis/#structured-ssa","text":"Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, x = input; s = 0; c = 0; while c < x { s = c + s; c = c + 1; } return s; Can be converted into a structured SSA x1 = input; s1 = 0; c1 = 0; join { s2 = phi(s1,s3); c2 = phi(c1,c3); } while c2 < x1 { s3 = c2 + s2; c3 = c2 + 1; } return s2; In the above SSA form, we have a join ... while ... loop. The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and also the body of the loop. (Similarly we can introduce a if ... else ... join ... statement). Structured SSA allows us to conduct name analysis closer to the source language. conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. perform code obfuscation.","title":"Structured SSA"},{"location":"notes/name_analysis/#futher-readings","text":"https://dl.acm.org/doi/10.1145/2955811.2955813 https://dl.acm.org/doi/abs/10.1145/3605156.3606457 https://dl.acm.org/doi/10.1145/202530.202532","title":"Futher Readings"},{"location":"notes/semantic_analysis/","text":"50.054 - Semantic Analysis Learning Outcomes Articulate the meaning of program semantics List different types of program semantics. Explain the limitation of static analysis. What is Program Semantic In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program. Dynamic Semantics Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean How does the program get executed? What does the program compute / return? Static Semantics Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value. Semantics Analysis Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] But in fact it could be graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Semantic Analysis Input: A parse tree or an internal representation a source parse tree is considered an internal representation Output: if succeeds, a parse tree or an internal representation otherwise, an error report Goal of Semantic Analysis There mainly two goals of semantic analysis. Optimization x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s; Fault Detection x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number. return y; Dynamic Semantics Analysis Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules). Testing Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification. Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests. Static Semantic Analysis Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it. Type checking and type inference Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced. Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location. Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints. The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation Limitation of Static Semantic Analysis It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs. For example, assume we can find an algorithm that determine whether the variable x in the following function is positive or negative without executing it. def f(path): p = open(path, \"r\") x = 1 if eval(p): x = -1 return x In the above program the analysis of x 's sign (positive or negative) is subject to whether eval(p) is true or false . If such an algorithm exists, as a side effect we can also statically detect whether the given program in path is terminating, which is of course undecidable.","title":"50.054 - Semantic Analysis"},{"location":"notes/semantic_analysis/#50054-semantic-analysis","text":"","title":"50.054 - Semantic Analysis"},{"location":"notes/semantic_analysis/#learning-outcomes","text":"Articulate the meaning of program semantics List different types of program semantics. Explain the limitation of static analysis.","title":"Learning Outcomes"},{"location":"notes/semantic_analysis/#what-is-program-semantic","text":"In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program.","title":"What is Program Semantic"},{"location":"notes/semantic_analysis/#dynamic-semantics","text":"Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean How does the program get executed? What does the program compute / return?","title":"Dynamic Semantics"},{"location":"notes/semantic_analysis/#static-semantics","text":"Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value.","title":"Static Semantics"},{"location":"notes/semantic_analysis/#semantics-analysis","text":"Recall the compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] But in fact it could be graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] D --> C Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Semantic Analysis Input: A parse tree or an internal representation a source parse tree is considered an internal representation Output: if succeeds, a parse tree or an internal representation otherwise, an error report","title":"Semantics Analysis"},{"location":"notes/semantic_analysis/#goal-of-semantic-analysis","text":"There mainly two goals of semantic analysis.","title":"Goal of Semantic Analysis"},{"location":"notes/semantic_analysis/#optimization","text":"x = input; y = 0; s = 0; while (y < x) { y = y + 1; t = s; // t is not used. s = s + y; } return s;","title":"Optimization"},{"location":"notes/semantic_analysis/#fault-detection","text":"x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number. return y;","title":"Fault Detection"},{"location":"notes/semantic_analysis/#dynamic-semantics-analysis","text":"Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules). Testing Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification. Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests.","title":"Dynamic Semantics Analysis"},{"location":"notes/semantic_analysis/#static-semantic-analysis","text":"Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it. Type checking and type inference Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced. Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location. Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints. The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation","title":"Static Semantic Analysis"},{"location":"notes/semantic_analysis/#limitation-of-static-semantic-analysis","text":"It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs. For example, assume we can find an algorithm that determine whether the variable x in the following function is positive or negative without executing it. def f(path): p = open(path, \"r\") x = 1 if eval(p): x = -1 return x In the above program the analysis of x 's sign (positive or negative) is subject to whether eval(p) is true or false . If such an algorithm exists, as a side effect we can also statically detect whether the given program in path is terminating, which is of course undecidable.","title":"Limitation of Static Semantic Analysis"},{"location":"notes/sign_analysis_lattice/","text":"50.054 - Sign Analysis and Lattice Theory Learning Outcomes Explain the objective of Sign Analysis Define Lattice and Complete Lattice Define Monotonic Functions Explain the fixed point theorem Apply the fixed pointed theorem to solve equation constraints of sign analysis Recap Recall that one of the goals of semantic analyses is to detect faults without executing the program. // SIMP1 x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number return y; Note that our current SIMP syntax does not support >= . We could extend both SIMP and Pseudo Assembly to support a new binary operator || so that we can x>=0 into (x > 0) || (x == 0) Note that for In Pseudo Assembly we use 0 to encode false and 1 to encode true . Hence || can be encoded as + . To detect that the application of sqrt(x) is causing an error, we could apply the sign analysis. Sign Analysis Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example // SIMP1 x = input; // x could be +, - or 0 while (x >= 0) { // x could be +, - or 0 x = x - 1; // x could be +, - or 0 } // x must be - y = Math.sqrt(x); // x must be -, y could be +, - or 0 return y; // x must be -, y could be +, - or 0 We put the comments as the results of the analysis. Can we turn Sign Analysis into a type inference problem? The answer is yes, but it is rather imprecise. Let's consider a simple example. // SIMP2 x = 0; x = x + 1; return x; Suppose we introduce 3 subtypes of the Int type, namely Zero , PosInt and NegInt The first statement, we infer x has type Zero . The second statement, we infer x on the RHS, has type Int , the LHS x has type Int . Unification would fail when we try to combine the result of (x : Zero) and (x : Int) . It is also unsound to conclude that Zero is the final type. This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom. Abstract Domain To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. For example, we may use \\(\\{\\}\\) to denote the empty set \\(+\\) to denote the set of all positive integers \\(-\\) to denote the set of all ngative integers \\(\\{0\\}\\) to denote the set containing 0 \\(+ \\cup - \\cup \\{0\\}\\) to denote all integers . For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\) , \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\) . These symbols are the abstract values of the sign property. Since they are sets of values, we can define the subset relation among them. \\[ \\begin{array}{c} \\bot \\subseteq 0 \\\\ \\bot \\subseteq + \\\\ \\bot \\subseteq - \\\\ 0 \\subseteq \\top \\\\ {+} \\subseteq \\top \\\\ {-} \\subseteq \\top \\end{array} \\] If we put each abstract domain values in a graph we have the following graph Graph1 graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] informally the above graph structure is called a lattice in math. We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of SIMP2 . For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.) // PA2 // x -> top 1: x <- 0 // x -> 0 2: x <- x + 1 // x -> 0 ++ + -> + 3: rret <- x // x -> + 4: ret we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to x , as x could be any value. After instruction 1, we deduce that x must be having the abstract value 0 , since we assign 0 to x . After instruction 2, we deduce that x has the abstract value + because we add ( ++ ) 1 to an abstract value 0 . (Note that the 0 , 1 and ++ in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable input (which is always \\(\\top\\) ) and the register rret (whose sign is not useful.) Let's consider another example // PA3 // x -> top, t -> top 1: x <- 0 // x -> 0, t -> top 2: t <- input < 0 // x -> 0, t -> top 3: ifn t goto 6 // x -> 0, t -> top 4: x <- x + 1 // x -> +, t -> top 5: goto 6 // x -> +, t -> top 6: rret <- x // x -> upperbound(+, 0) -> top, t -> top 7: ret We start off by assigning \\(\\top\\) to x , then 0 to x at the instruction 1. At instruction 2, we assign the result of the boolean condition to t which could be 0 or 1 hence top is the abstract value associated with t . Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update x 's sign to + . Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of x 's sign. If t 's value is 0, x 's sign is 0 , otherwise x 's sign is + . Hence we take the upperbound of + , 0 according to Graph1 which is \\(\\top\\) . Let's consider the formalism of the lattice and this approach we just presented. Lattice Theory Definition 1 - Partial Order A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition. reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\) transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\) . anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\) . For instance, the set of abstract values in Graph1 forms a partial order if we define \\(x \\sqsubseteq y\\) as \" \\(x\\) is at least as precise than \\(y\\) \", (i.e. \\(x\\) is the same or more precise than \\(y\\) ). Definition 2 - Upper Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\) ) iff \\(\\forall x \\in T, x \\sqsubseteq y\\) . Definition 3 - Least Upper Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\) ) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\) . For example, in Graph1 , 0 is an upper bound of \\(\\{\\bot\\}\\) , but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\) . Definition 4 - Lower Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\) ) iff \\(\\forall x \\in T, y \\sqsubseteq x\\) . Definition 5 - Greatest Lower Bound Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\) ) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\) . For example, in Graph2 , 0 is a lower bound of \\(\\{\\top\\}\\) , but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\) . Definition 6 - Join and Meet Let \\(S\\) be a partial order, and \\(x, y \\in S\\) . We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\) . We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\) . Definition 7 - Lattice A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\) , \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist. Definition 8 - Complete Lattice and Semi-Lattice A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist. A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) exists. A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\) , \\({\\Large \\sqcap} X\\) exists. For example the set of abstract values in Graph1 and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. Graph1 is the Hasse diagram of this complete lattice. Lemma 9 Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice. In the next few subsections, we introduce a few commonly use lattices. Powerset Lattice Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\) . Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice. We call it powerset lattice . The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\) , we find that for any \\(T \\subseteq {\\cal P}(A)\\) . \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\) . Can you show that the power set of {1,2,3,4} and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram? Product Lattice Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times L_n)\\) . For example in PA3 , to analyse the signs for variables we need two lattices, one for variable x and the other for variable t , which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . graph TD; tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"] tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"] tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"] tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"] t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"] t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"] t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"] t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"] t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"] t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"] t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"] t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"] tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"] tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"] tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"] tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"] +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"] +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"] +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"] 0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] 0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"] 0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"] 0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"] mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"] ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"] 0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"] 0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"] m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"] m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"] 00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"] 00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"] +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"] +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"] m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"] m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"] +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"] +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"] 0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"] 0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"] mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"] bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"] bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"] bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"] b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"] b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"] bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"] +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] 0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] Map Lattice Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] Then \\(A \\rightarrow L\\) is a complete lattice. Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Scala Map[A,L] object where elements of \\(A\\) are keys and elements of \\(L\\) are the values associated with the keys. Map lattice offers a compact alternative to lattices for sign analysis of variables in program like PA3 when there are many variables. We can define a map lattice consisting of functions that map variables ( x or t ) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . For instance, one of the element \"functions\" in the above-mentioned map lattice could be \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ] \\] another element function could be \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ] \\] We conclude that \\(m_1\\sqsubseteq m_2\\) . Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . m1 and m2 are elements of the complete lattice \\(Var \\rightarrow Sign\\) graph TD; tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"] +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] 0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] 00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] 00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] 0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"] b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"] +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] 0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] Sign analysis with Lattice As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements. // PA2 // s0 = [x -> top] 1: x <- 0 // s1 = s0[x -> 0] 2: x = x + 1 // s2 = s1[x -> s1(x) ++ +] 3: rret <- x // s3 = s2 4: ret In the above, we analyse SIMP2 program's sign by \"packaging\" the variable to sign bindings into some state variables, s1 , s2 , s3 and s4 . Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\) . Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. Note that we could also model the state variables as a tuple of lattice as a produce lattice. Next we would like to model the change of variable signs based on the previous instructions. We write s[x -> v] to denote a new state s' which is nearly the same as s except that the mapping of variable x is changed to v. (In Scala style syntax, assuming s is a Map[Var, Sign] object, then s[x->v] is actually s + (x -> v) in Scala.) We write s(x) to denote a query of variable x 's value in state s . (In Scala style syntax, it is s.get(x) match { case Some(v) => v } ) In the above example, we define s2 based on s1 by \"updating\" variable x 's sign to 0 . We update x 's sign in s3 based on s2 by querying x 's sign in s2 and modifying it by increasing by 1 . We define the ++ abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Where the first column from the 2nd rows onwards are the left operand and the first row from the 2nd column onwards are the right operand. Similarly we can define the other abstract operators -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) << \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Given the definitions of the abstract operators, our next task is to solve the equation among the state variable s0 , s1 , s2 and s3 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 Note that we can't use unification here as x is assocated with different sign abstract values at different states (instructions). Question: If we use SSA PA instead of PA, can the generated equations be solved using unification? To solve the set of equation constraints we could process the equations from top to bottom. s0 = [x -> top] s1 = [x -> 0] s2 = [x -> +] s3 = [x -> +] Then we can conclude that the sign of variable x at instruction 3 is positive. Note that all the states, s0 , s1 , s2 and s3 are elements in the map lattice \\(Var \\rightarrow Sign\\) . However, we need a more general solver as the equation systems could be recursive in the presence of loops. For example. // PA4 // s0 = [x -> top, y -> top, t -> top] 1: x <- input // s1 = s0 2: y <- 0 // s2 = s1[y -> 0] 3: t <- x > 0 // s3 = upperbound(s2,s7)[t -> top] 4: ifn t goto 8 // s4 = s3 5: y <- y + 1 // s5 = s4[y -> s4(y) ++ +] 6: x <- x - 1 // s6 = s5[x -> s5(x) -- +] 7: goto 3 // s7 = s6 8: rret <- y // s8 = s4 9: ret In the above the upperbound(s, t) can be define as \\(s \\sqcup t\\) , assuming \\(s\\) and \\(t\\) are elements of a complete lattice. Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\) , hence \\(s \\sqcup t\\) can be defined as \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] To solve equation systems like the above, we need some \"special\" functions that operates on lattices. Definition 10 - Monotonic Function Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\) . Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl. For instance given the lattice described in Graph1 , we define the following function \\[ \\begin{array}{rcl} f_1(x) & = & \\top \\end{array} \\] Function \\(f_1\\) is monotonic because \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\ f_1(+) = \\top \\\\ f_1(-) = \\top \\\\ f_1(\\top) = \\top \\end{array} \\] and \\(\\top \\sqsubseteq \\top\\) Let's consider another function \\(f_2\\) \\[ \\begin{array}{rcl} f_2(x) & = & x \\sqcup + \\end{array} \\] is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\ f_2(+) = + \\sqcup + = + \\\\ f_2(-) = - \\sqcup + = \\top \\\\ f_2(\\top) = \\top \\sqcup + = \\top \\end{array} \\] Note that \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} $$ when we apply $g$ to all the abstract values in the above inequalities, we find that $$ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] hold. Therefore \\(g\\) is monotonic. Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f (v_1', ..., v_n')\\) Lemma 11 - Constant Function is Monotonic. Every constant function \\(f\\) is monotonic. Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic. Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\) , then \\(\\sqcup\\) is monotonic. Similar observation applies to \\(\\sqcap\\) . Definition 13 - Fixed Point and Least Fixed Point Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\) . We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\) , \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\) . For example, for function \\(f_1\\) , \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\) , \\(+\\) , \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point. Theorem 14 - Fixed Point Theorem Let \\(L\\) be a complete lattice with finite height, every monotonic function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\) , defined as \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] Where \\(f^n(x)\\) is a short hand for \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] The height of a complete lattice is the length of the longest path from \\(\\top\\) to \\(\\bot\\) . The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\) , we will reach a fixed point and it must be the only least fixed point. The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result. For example, consider function \\(f_2\\) . If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point. Lemma 15 - Map update with monotonic function is Monotonic Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\) . Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\) . To gain some intuition of this lemma, let's try to think in terms of Scala. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as Map[A, L2] in Scala style, and L2 is a lattice. f : L1 => Map[A, L2] is a Scala function that's monotonic, g: L1=>L2 is another Scala function which is monotonic. Then we can conclude that val a:A = ... // a is an element of A, where A is a ground type. def h[L1,L2](x:L1):Map[A,L2] = f(x) + (a -> g(x)) h is also monotonic. Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\) , we have \\(f(x) \\sqsubseteq f(y)\\) . It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\) . Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\) . With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis. Naive Fixed Point Algorithm input: a function f . initialize x as \\(\\bot\\) apply f(x) as x1 check x1 == x if true return x else, update x = x1 , go back to step 2. For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in Graph1 , we have the following iterations. \\(x = \\bot, x_1 = f_2(x) = +\\) \\(x = +, x_1 = f_2(x) = +\\) fixed point is reached, return \\(x\\) . Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA2 Recall the set of equations generated from PA2 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\) . and \\(Sign\\) to denote the sign lattice described in Graph1 . We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\) . In total. we have four map lattices, one for s0 , one for s1 , and etc. Then we \"package\" these four map lattices into a product lattice \\(L = (Var \\rightarrow Sign)^4\\) . Since \\(Sign\\) is a complete lattice, so is \\(L\\) . Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems. The type of \\(f_3\\) should be \\(L \\rightarrow L\\) , or \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] in its unabridge form. Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\) , but it is like a Map[Var, Sign] . Next we re-model the relations among s0,s1,s2,s3 in above equation system in \\(f_3\\) as follows \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] Thanks to Lemma 15, \\(f_3\\) is monotonic. The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point. \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) . Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA4 Recall the set of equations generated from PA4 's sign analysis s0 = [x -> top, y -> top, t -> top] s1 = s0 s2 = s1[y -> 0] s3 = upperbound(s2,s7)[t -> top] s4 = s3 s5 = s4[y -> s4(y) ++ +] s6 = s5[x -> s5(x) -- +] s7 = s6 s8 = s4 We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_0, \\\\ s_1[y \\mapsto 0], \\\\ (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\ s_3, \\\\ s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\ s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\ s_6, \\\\ s_4 \\end{array} \\right ) \\end{array} \\] \\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} $$ \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] 8 . $$ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) functipn w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice. Optimization This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\) . A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed. Generalizing the monotone constraints for sign analysis We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis. Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\) . let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\) . For sign analysis, we define the following helper function \\[join(s) = \\bigsqcup pred(s)\\] To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\) . Let \\(V\\) denotes the set of variables in the PA program's being analysed. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & \\left \\{ \\begin{array}{cc} 0 & c == 0 \\\\ + & c > 0 \\\\ - & c < 0 \\end{array} \\right . \\\\ \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, \\[ \\begin{array}{rcl} abs(+) & = & ++\\\\ abs(-) & = & -- \\\\ abs(*) & = & ** \\\\ abs(<) & = & << \\\\ abs(==) & = & === \\end{array} \\] We have seen the definitions of \\(++, --, **\\) and \\(<<\\) Question: can you define \\(===\\) ? Question: the abstraction operations are pretty coarse (not accurate). For instance, << and === should return either 0 or 1 hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? Question: Convert SIMP1 into a PA. Can we apply the sign analysis to find out that the sqrt(x) is definifely failing?","title":"50.054 - Sign Analysis and Lattice Theory"},{"location":"notes/sign_analysis_lattice/#50054-sign-analysis-and-lattice-theory","text":"","title":"50.054 - Sign Analysis and Lattice Theory"},{"location":"notes/sign_analysis_lattice/#learning-outcomes","text":"Explain the objective of Sign Analysis Define Lattice and Complete Lattice Define Monotonic Functions Explain the fixed point theorem Apply the fixed pointed theorem to solve equation constraints of sign analysis","title":"Learning Outcomes"},{"location":"notes/sign_analysis_lattice/#recap","text":"Recall that one of the goals of semantic analyses is to detect faults without executing the program. // SIMP1 x = input; while (x >= 0) { x = x - 1; } y = Math.sqrt(x); // error, can't apply sqrt() to a negative number return y; Note that our current SIMP syntax does not support >= . We could extend both SIMP and Pseudo Assembly to support a new binary operator || so that we can x>=0 into (x > 0) || (x == 0) Note that for In Pseudo Assembly we use 0 to encode false and 1 to encode true . Hence || can be encoded as + . To detect that the application of sqrt(x) is causing an error, we could apply the sign analysis.","title":"Recap"},{"location":"notes/sign_analysis_lattice/#sign-analysis","text":"Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example // SIMP1 x = input; // x could be +, - or 0 while (x >= 0) { // x could be +, - or 0 x = x - 1; // x could be +, - or 0 } // x must be - y = Math.sqrt(x); // x must be -, y could be +, - or 0 return y; // x must be -, y could be +, - or 0 We put the comments as the results of the analysis.","title":"Sign Analysis"},{"location":"notes/sign_analysis_lattice/#can-we-turn-sign-analysis-into-a-type-inference-problem","text":"The answer is yes, but it is rather imprecise. Let's consider a simple example. // SIMP2 x = 0; x = x + 1; return x; Suppose we introduce 3 subtypes of the Int type, namely Zero , PosInt and NegInt The first statement, we infer x has type Zero . The second statement, we infer x on the RHS, has type Int , the LHS x has type Int . Unification would fail when we try to combine the result of (x : Zero) and (x : Int) . It is also unsound to conclude that Zero is the final type. This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom.","title":"Can we turn Sign Analysis into a type inference problem?"},{"location":"notes/sign_analysis_lattice/#abstract-domain","text":"To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. For example, we may use \\(\\{\\}\\) to denote the empty set \\(+\\) to denote the set of all positive integers \\(-\\) to denote the set of all ngative integers \\(\\{0\\}\\) to denote the set containing 0 \\(+ \\cup - \\cup \\{0\\}\\) to denote all integers . For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\) , \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\) . These symbols are the abstract values of the sign property. Since they are sets of values, we can define the subset relation among them. \\[ \\begin{array}{c} \\bot \\subseteq 0 \\\\ \\bot \\subseteq + \\\\ \\bot \\subseteq - \\\\ 0 \\subseteq \\top \\\\ {+} \\subseteq \\top \\\\ {-} \\subseteq \\top \\end{array} \\] If we put each abstract domain values in a graph we have the following graph Graph1 graph A[\"\u22a4\"]---B[-] A---C[0] A---D[+] B---E C---E D---E[\u22a5] informally the above graph structure is called a lattice in math. We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of SIMP2 . For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.) // PA2 // x -> top 1: x <- 0 // x -> 0 2: x <- x + 1 // x -> 0 ++ + -> + 3: rret <- x // x -> + 4: ret we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to x , as x could be any value. After instruction 1, we deduce that x must be having the abstract value 0 , since we assign 0 to x . After instruction 2, we deduce that x has the abstract value + because we add ( ++ ) 1 to an abstract value 0 . (Note that the 0 , 1 and ++ in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable input (which is always \\(\\top\\) ) and the register rret (whose sign is not useful.) Let's consider another example // PA3 // x -> top, t -> top 1: x <- 0 // x -> 0, t -> top 2: t <- input < 0 // x -> 0, t -> top 3: ifn t goto 6 // x -> 0, t -> top 4: x <- x + 1 // x -> +, t -> top 5: goto 6 // x -> +, t -> top 6: rret <- x // x -> upperbound(+, 0) -> top, t -> top 7: ret We start off by assigning \\(\\top\\) to x , then 0 to x at the instruction 1. At instruction 2, we assign the result of the boolean condition to t which could be 0 or 1 hence top is the abstract value associated with t . Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update x 's sign to + . Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of x 's sign. If t 's value is 0, x 's sign is 0 , otherwise x 's sign is + . Hence we take the upperbound of + , 0 according to Graph1 which is \\(\\top\\) . Let's consider the formalism of the lattice and this approach we just presented.","title":"Abstract Domain"},{"location":"notes/sign_analysis_lattice/#lattice-theory","text":"","title":"Lattice Theory"},{"location":"notes/sign_analysis_lattice/#definition-1-partial-order","text":"A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition. reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\) transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\) . anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\) . For instance, the set of abstract values in Graph1 forms a partial order if we define \\(x \\sqsubseteq y\\) as \" \\(x\\) is at least as precise than \\(y\\) \", (i.e. \\(x\\) is the same or more precise than \\(y\\) ).","title":"Definition 1 - Partial Order"},{"location":"notes/sign_analysis_lattice/#definition-2-upper-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\) ) iff \\(\\forall x \\in T, x \\sqsubseteq y\\) .","title":"Definition 2 - Upper Bound"},{"location":"notes/sign_analysis_lattice/#definition-3-least-upper-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\) ) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\) . For example, in Graph1 , 0 is an upper bound of \\(\\{\\bot\\}\\) , but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\) .","title":"Definition 3 - Least Upper Bound"},{"location":"notes/sign_analysis_lattice/#definition-4-lower-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) . We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\) ) iff \\(\\forall x \\in T, y \\sqsubseteq x\\) .","title":"Definition 4 - Lower Bound"},{"location":"notes/sign_analysis_lattice/#definition-5-greatest-lower-bound","text":"Let \\(S\\) be a partial order, and \\(T \\subseteq S\\) , \\(y \\in S\\) , We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\) ) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\) . For example, in Graph2 , 0 is a lower bound of \\(\\{\\top\\}\\) , but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\) .","title":"Definition 5 - Greatest Lower Bound"},{"location":"notes/sign_analysis_lattice/#definition-6-join-and-meet","text":"Let \\(S\\) be a partial order, and \\(x, y \\in S\\) . We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\) . We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\) .","title":"Definition 6 - Join and Meet"},{"location":"notes/sign_analysis_lattice/#definition-7-lattice","text":"A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\) , \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist.","title":"Definition 7 - Lattice"},{"location":"notes/sign_analysis_lattice/#definition-8-complete-lattice-and-semi-lattice","text":"A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist. A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\) , \\(\\bigsqcup X\\) exists. A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\) , \\({\\Large \\sqcap} X\\) exists. For example the set of abstract values in Graph1 and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. Graph1 is the Hasse diagram of this complete lattice.","title":"Definition 8 - Complete Lattice and Semi-Lattice"},{"location":"notes/sign_analysis_lattice/#lemma-9","text":"Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice. In the next few subsections, we introduce a few commonly use lattices.","title":"Lemma 9"},{"location":"notes/sign_analysis_lattice/#powerset-lattice","text":"Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\) . Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice. We call it powerset lattice . The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\) , we find that for any \\(T \\subseteq {\\cal P}(A)\\) . \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\) . Can you show that the power set of {1,2,3,4} and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram?","title":"Powerset Lattice"},{"location":"notes/sign_analysis_lattice/#product-lattice","text":"Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times L_n)\\) . For example in PA3 , to analyse the signs for variables we need two lattices, one for variable x and the other for variable t , which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . graph TD; tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"] tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"] tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"] tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"] tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"] t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"] t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"] t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"] t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"] t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"] t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"] t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"] t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"] tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"] tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"] tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"] tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"] +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"] +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"] +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"] 0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] 0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"] 0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"] 0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"] mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"] ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"] 0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"] 0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"] m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"] m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"] 00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"] 00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"] +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"] +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"] m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"] m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"] +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"] +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"] 0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"] 0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"] mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"] bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"] bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"] bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"] b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"] b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"] bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"] tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"] +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] 0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"] mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]","title":"Product Lattice"},{"location":"notes/sign_analysis_lattice/#map-lattice","text":"Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] Then \\(A \\rightarrow L\\) is a complete lattice. Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Scala Map[A,L] object where elements of \\(A\\) are keys and elements of \\(L\\) are the values associated with the keys. Map lattice offers a compact alternative to lattices for sign analysis of variables in program like PA3 when there are many variables. We can define a map lattice consisting of functions that map variables ( x or t ) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . For instance, one of the element \"functions\" in the above-mentioned map lattice could be \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ] \\] another element function could be \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ] \\] We conclude that \\(m_1\\sqsubseteq m_2\\) . Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\) . m1 and m2 are elements of the complete lattice \\(Var \\rightarrow Sign\\) graph TD; tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"] tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"] t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"] t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"] tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"] +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"] +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"] 0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] 0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"] m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] 00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] 00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"] m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"] 0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] 0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"] mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"] bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"] b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"] tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"] +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] 0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"] mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]","title":"Map Lattice"},{"location":"notes/sign_analysis_lattice/#sign-analysis-with-lattice","text":"As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements. // PA2 // s0 = [x -> top] 1: x <- 0 // s1 = s0[x -> 0] 2: x = x + 1 // s2 = s1[x -> s1(x) ++ +] 3: rret <- x // s3 = s2 4: ret In the above, we analyse SIMP2 program's sign by \"packaging\" the variable to sign bindings into some state variables, s1 , s2 , s3 and s4 . Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\) . Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. Note that we could also model the state variables as a tuple of lattice as a produce lattice. Next we would like to model the change of variable signs based on the previous instructions. We write s[x -> v] to denote a new state s' which is nearly the same as s except that the mapping of variable x is changed to v. (In Scala style syntax, assuming s is a Map[Var, Sign] object, then s[x->v] is actually s + (x -> v) in Scala.) We write s(x) to denote a query of variable x 's value in state s . (In Scala style syntax, it is s.get(x) match { case Some(v) => v } ) In the above example, we define s2 based on s1 by \"updating\" variable x 's sign to 0 . We update x 's sign in s3 based on s2 by querying x 's sign in s2 and modifying it by increasing by 1 . We define the ++ abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Where the first column from the 2nd rows onwards are the left operand and the first row from the 2nd column onwards are the right operand. Similarly we can define the other abstract operators -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) << \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) Given the definitions of the abstract operators, our next task is to solve the equation among the state variable s0 , s1 , s2 and s3 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 Note that we can't use unification here as x is assocated with different sign abstract values at different states (instructions). Question: If we use SSA PA instead of PA, can the generated equations be solved using unification? To solve the set of equation constraints we could process the equations from top to bottom. s0 = [x -> top] s1 = [x -> 0] s2 = [x -> +] s3 = [x -> +] Then we can conclude that the sign of variable x at instruction 3 is positive. Note that all the states, s0 , s1 , s2 and s3 are elements in the map lattice \\(Var \\rightarrow Sign\\) . However, we need a more general solver as the equation systems could be recursive in the presence of loops. For example. // PA4 // s0 = [x -> top, y -> top, t -> top] 1: x <- input // s1 = s0 2: y <- 0 // s2 = s1[y -> 0] 3: t <- x > 0 // s3 = upperbound(s2,s7)[t -> top] 4: ifn t goto 8 // s4 = s3 5: y <- y + 1 // s5 = s4[y -> s4(y) ++ +] 6: x <- x - 1 // s6 = s5[x -> s5(x) -- +] 7: goto 3 // s7 = s6 8: rret <- y // s8 = s4 9: ret In the above the upperbound(s, t) can be define as \\(s \\sqcup t\\) , assuming \\(s\\) and \\(t\\) are elements of a complete lattice. Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\) , hence \\(s \\sqcup t\\) can be defined as \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] To solve equation systems like the above, we need some \"special\" functions that operates on lattices.","title":"Sign analysis with Lattice"},{"location":"notes/sign_analysis_lattice/#definition-10-monotonic-function","text":"Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\) . Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl. For instance given the lattice described in Graph1 , we define the following function \\[ \\begin{array}{rcl} f_1(x) & = & \\top \\end{array} \\] Function \\(f_1\\) is monotonic because \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\ f_1(+) = \\top \\\\ f_1(-) = \\top \\\\ f_1(\\top) = \\top \\end{array} \\] and \\(\\top \\sqsubseteq \\top\\) Let's consider another function \\(f_2\\) \\[ \\begin{array}{rcl} f_2(x) & = & x \\sqcup + \\end{array} \\] is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\ f_2(+) = + \\sqcup + = + \\\\ f_2(-) = - \\sqcup + = \\top \\\\ f_2(\\top) = \\top \\sqcup + = \\top \\end{array} \\] Note that \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\ \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} $$ when we apply $g$ to all the abstract values in the above inequalities, we find that $$ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\ f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] hold. Therefore \\(g\\) is monotonic. Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f (v_1', ..., v_n')\\)","title":"Definition 10 - Monotonic Function"},{"location":"notes/sign_analysis_lattice/#lemma-11-constant-function-is-monotonic","text":"Every constant function \\(f\\) is monotonic.","title":"Lemma 11 - Constant Function is Monotonic."},{"location":"notes/sign_analysis_lattice/#lemma-12-sqcup-and-sqcap-are-monotonic","text":"Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\) , then \\(\\sqcup\\) is monotonic. Similar observation applies to \\(\\sqcap\\) .","title":"Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic."},{"location":"notes/sign_analysis_lattice/#definition-13-fixed-point-and-least-fixed-point","text":"Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\) . We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\) , \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\) . For example, for function \\(f_1\\) , \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\) , \\(+\\) , \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point.","title":"Definition 13 - Fixed Point and Least Fixed Point"},{"location":"notes/sign_analysis_lattice/#theorem-14-fixed-point-theorem","text":"Let \\(L\\) be a complete lattice with finite height, every monotonic function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\) , defined as \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] Where \\(f^n(x)\\) is a short hand for \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] The height of a complete lattice is the length of the longest path from \\(\\top\\) to \\(\\bot\\) . The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\) , we will reach a fixed point and it must be the only least fixed point. The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result. For example, consider function \\(f_2\\) . If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point.","title":"Theorem 14 - Fixed Point Theorem"},{"location":"notes/sign_analysis_lattice/#lemma-15-map-update-with-monotonic-function-is-monotonic","text":"Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\) . Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\) . To gain some intuition of this lemma, let's try to think in terms of Scala. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as Map[A, L2] in Scala style, and L2 is a lattice. f : L1 => Map[A, L2] is a Scala function that's monotonic, g: L1=>L2 is another Scala function which is monotonic. Then we can conclude that val a:A = ... // a is an element of A, where A is a ground type. def h[L1,L2](x:L1):Map[A,L2] = f(x) + (a -> g(x)) h is also monotonic. Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\) , we have \\(f(x) \\sqsubseteq f(y)\\) . It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\) . Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\) . With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis.","title":"Lemma 15 - Map update with monotonic function is Monotonic"},{"location":"notes/sign_analysis_lattice/#naive-fixed-point-algorithm","text":"input: a function f . initialize x as \\(\\bot\\) apply f(x) as x1 check x1 == x if true return x else, update x = x1 , go back to step 2. For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in Graph1 , we have the following iterations. \\(x = \\bot, x_1 = f_2(x) = +\\) \\(x = +, x_1 = f_2(x) = +\\) fixed point is reached, return \\(x\\) .","title":"Naive Fixed Point Algorithm"},{"location":"notes/sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa2","text":"Recall the set of equations generated from PA2 s0 = [x -> top] s1 = s0[x -> 0] s2 = s1[x -> s1(x) ++ +] s3 = s2 and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\) . and \\(Sign\\) to denote the sign lattice described in Graph1 . We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\) . In total. we have four map lattices, one for s0 , one for s1 , and etc. Then we \"package\" these four map lattices into a product lattice \\(L = (Var \\rightarrow Sign)^4\\) . Since \\(Sign\\) is a complete lattice, so is \\(L\\) . Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems. The type of \\(f_3\\) should be \\(L \\rightarrow L\\) , or \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] in its unabridge form. Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\) , but it is like a Map[Var, Sign] . Next we re-model the relations among s0,s1,s2,s3 in above equation system in \\(f_3\\) as follows \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] Thanks to Lemma 15, \\(f_3\\) is monotonic. The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point. \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) , $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) & = & ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++}\\ {\\tt +})], s_2) \\ & = & ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$ fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\) .","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA2"},{"location":"notes/sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa4","text":"Recall the set of equations generated from PA4 's sign analysis s0 = [x -> top, y -> top, t -> top] s1 = s0 s2 = s1[y -> 0] s3 = upperbound(s2,s7)[t -> top] s4 = s3 s5 = s4[y -> s4(y) ++ +] s6 = s5[x -> s5(x) -- +] s7 = s6 s8 = s4 We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_0, \\\\ s_1[y \\mapsto 0], \\\\ (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\ s_3, \\\\ s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\ s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\ s_6, \\\\ s_4 \\end{array} \\right ) \\end{array} \\] \\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\) , $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} $$ \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{l} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ $$ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] \\[ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] 8 . $$ \\begin{array}{c} s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\ s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\ s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\ s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\ s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} $$ \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) & = & \\left ( \\begin{array}{c} [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\ [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array} \\right ) \\end{array} \\] If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) functipn w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice.","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of PA4"},{"location":"notes/sign_analysis_lattice/#optimization","text":"This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\) . A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed.","title":"Optimization"},{"location":"notes/sign_analysis_lattice/#generalizing-the-monotone-constraints-for-sign-analysis","text":"We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis. Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\) . let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\) . For sign analysis, we define the following helper function \\[join(s) = \\bigsqcup pred(s)\\] To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\) . Let \\(V\\) denotes the set of variables in the PA program's being analysed. The monotonic functions can be defined by the following cases. case \\(l == 0\\) , \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\) case \\(l: t \\leftarrow src\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\) case \\(l: t \\leftarrow src_1\\ op\\ src_2\\) , \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\) other cases: \\(s_l = join(s_l)\\) Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows \\[ \\begin{array}{rcl} m(c) & = & \\left \\{ \\begin{array}{cc} 0 & c == 0 \\\\ + & c > 0 \\\\ - & c < 0 \\end{array} \\right . \\\\ \\\\ m(t) & = & \\left \\{ \\begin{array}{cc} v & t \\mapsto v \\in m \\\\ error & otherwise \\end{array} \\right . \\\\ \\\\ m(r) & = & error \\end{array} \\] Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, \\[ \\begin{array}{rcl} abs(+) & = & ++\\\\ abs(-) & = & -- \\\\ abs(*) & = & ** \\\\ abs(<) & = & << \\\\ abs(==) & = & === \\end{array} \\] We have seen the definitions of \\(++, --, **\\) and \\(<<\\) Question: can you define \\(===\\) ? Question: the abstraction operations are pretty coarse (not accurate). For instance, << and === should return either 0 or 1 hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? Question: Convert SIMP1 into a PA. Can we apply the sign analysis to find out that the sqrt(x) is definifely failing?","title":"Generalizing the monotone constraints for sign analysis"},{"location":"notes/static_semantics/","text":"50.054 Static Semantics For SIMP Learning Outcomes Explain what static semantics is. Apply type checking rules to verify the type correctness property of a SIMP program. Explain the relation between type system and operational semantics. Apply type inference algorithm to generate a type environment given a SIMP program. What is static semantics? While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program. For example, a statically correct program, must satisfy some properties all uses of variables in it must be defined somewhere earlier. all the use of variables, the types must be matching with the expected type in the context. ... Here is a statically correct SIMP program, x = 0; y = input; if y > x { y = 0; } return y; because it satifies the first two properties. The following program is not statically correct. x = 0; y = input; if y + x { // type error x = z; // the use of an undefined variable z } return x; Static checking is to rule out the statically incorrect programs. Type Checking for SIMP We consider the type checking for SIMP programs. Recall the syntax rules for SIMP \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (X \\times T) \\end{array} \\] We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . We define two different relations, \\(\\Gamma \\vdash E : T\\) , which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\) . \\(\\Gamma \\vdash \\overline{S}\\) , which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\) . Type checking rules for SIMP Expressions $$ \\begin{array}{rc} {\\tt (tVar)} & \\begin{array}{c} (X,T) \\in \\Gamma \\ \\hline \\Gamma \\vdash X : T \\end{array} \\end{array} $$ In the rule \\({\\tt (tVar)}\\) , we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\) . $$ \\begin{array}{rc} {\\tt (tInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash C : int \\end{array} \\ \\ {\\tt (tBool)} & \\begin{array}{c} C \\in {true,false} \\ \\hline \\Gamma \\vdash C : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tInt)}\\) , we type check an integer constant having type \\(int\\) . Similarly, we type check a boolean constant having type \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tOp1)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { +, -, * } \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : int \\end{array} \\ \\ {\\tt (tOp2)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP E_2 : bool \\end{array} \\ \\ {\\tt (tOp3)} & \\begin{array}{c} \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tOp1)}\\) , we type check an integer arithmetic operation having type \\(int\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp2)}\\) , we type check an integer comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp3)}\\) , we type check a boolean comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tParen)} & \\begin{array}{c} \\Gamma \\vdash E :T \\ \\hline \\Gamma \\vdash (E) :T \\end{array} \\end{array} $$ Lastly in rule \\({\\tt (tParen)}\\) , we type check a parenthesized expression by type-checking the inner expression. Type Checking rules for SIMP Statements The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of \\(\\Gamma \\vdash \\overline{S} : T\\) , this is because statements do not return a value (except for return statement, which returns a value for the entire program.) \\[ \\begin{array}{rc} {\\tt (tSeq)} & \\begin{array}{c} \\Gamma \\vdash S \\ \\ \\ \\Gamma \\vdash \\overline{S} \\\\ \\hline \\Gamma \\vdash S \\overline{S} \\end{array} \\end{array} \\] The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\) . It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) . \\[ \\begin{array}{rc} {\\tt (tAssign)} & \\begin{array}{c} \\Gamma \\vdash E : T \\ \\ \\ \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash X = E \\end{array} \\end{array} \\] The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\) . It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree. \\[ \\begin{array}{rc} {\\tt (tReturn)} & \\begin{array}{c} \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash return\\ X \\end{array} \\\\ \\\\ {\\tt (tNop)} & \\Gamma \\vdash nop \\end{array} \\] The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable. $$ \\begin{array}{rc} {\\tt (tIf)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2} \\ \\hline \\Gamma \\vdash if\\ E\\ {\\overline{S_1}}\\ else\\ { \\overline{S_2} } \\end{array} \\ \\ {\\tt (tWhile)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S} \\ \\hline \\Gamma \\vdash while\\ E\\ {\\overline{S}} \\end{array} \\end{array} $$ The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\) . It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\) . The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way. We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) , i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\) . On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\) , we consider the type checking derivation of \\[x = input; s = 0; while\\ s<x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] \u0393 |- s:int (tVar) \u0393 |- 0:int (tInt) \u0393 |- input:int (tVar) -----------------(tAssign) [sub tree 1] \u0393 |- x:int (tVar) \u0393 |- s=0 ------------------(tAssign) --------------------------------------(tSeq) \u0393 |- x=input; \u0393 |- s=0; while s<x { s = s + 1;} return s; ---------------------------------------------------------------------(tSeq) \u0393 |- x=input; s=0; while s<x { s = s + 1;} return s; Where [sub tree 1] is \u0393 |- 0:int (tInt) \u0393 |- s:int (tVar) \u0393 |- s:int (tVar) -----------------(tOp1) \u0393 |- x:int (tVar) \u0393 |-s:int (tVar) \u0393 |- s+1:int --------------(tOp2) -------------------------------(tAssign) \u0393 |- s<x:bool \u0393 |- s = s + 1 \u0393 |- s:int (tVar) ---------------------------------------------(tWhile) ---------------(tReturn) \u0393 |- while s<x { s = s + 1;} \u0393 |- return s --------------------------------------------------------------------(tSeq) \u0393 |- while s<x { s = s + 1;} return s; Note that the following two programs are not typeable. // untypeable 1 x = 1; y = 0; if x { y = 0; } else { y = 1; } return y; The above is untypeable because we use x of type int in a context where it is also expected as bool . // untypeable 2 x = input; if (x > 1) { y = true; } else { y = 0; } return y; The above is unteable because we can't find a type environment which has both (y,int) and (y,bool) . So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative . // untypeable 3 x = input; if (x > 1) { if ( x * x * x < x * x) { y = true; } else { y = 1; } } else { y = 0; } return y; Even though we note that when x > 1 , we have x * x * x < x * x == false hence the statement y = true is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units. Let's connect the type-checking rules for SIMP with it dynamic semantics. Definition 1 - Type and Value Environments Consistency We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\) . It means the type environments and value environments are consistent. Property 2 - Progress The following property says that a well typed SIMP program must not be stuck until it reaches the return statement. Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\overline{S}\\) is either 1. a return statement, or 1. a sequence of statements, and there exist \\(\\Delta\\) , \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Property 3 - Preservation The following property says that the evaluation of a SIMP program does not change its typeability. Let \\(\\Delta\\) , \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\) . What is Type Inference Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. For example, given the Scala program def f(x:Int) = x + 1 the Scala compiler is able to deduce that the return type of f is Int . Likewise for the following SIMP program y = y + 1 we can also deduce that y is a of type int . What we aim to achieve is a sound and systematic process to deduce the omitted type information. Type inference for SIMP program Given a SIMP program \\(\\overline{S}\\) , the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution. Definition - Most general type (envrionment) Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\) . \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\) . Type Inference Rules We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures. \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} & \\hat{T} & ::= &\\alpha \\mid T \\\\ {\\tt (Constraints)} & \\kappa & \\subseteq & (\\hat{T} \\times \\hat{T}) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\) . Type substititution replace type variable to some other type. \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta & = & \\beta & if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T & = & T \\end{array} \\] Type substiution can be compositional . \\[ \\begin{array}{rcll} (\\Psi_1 \\circ \\Psi_2) \\hat{T} & = & \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms. Type Inference Rules for SIMP statements The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\) , which reads give a sequence of statements \\(\\overline{S}\\) , we generate a set of type constraints \\(\\kappa\\) . $$ \\begin{array}{rc} {\\tt (tiNOP)} & nop\\vDash {} \\ \\ {\\tt (tiReturn)} & return\\ X \\vDash {} \\end{array} $$ The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned. Similar observation applies to the return statement. $$ \\begin{array}{rc} {\\tt (tiSeq)} & \\begin{array}{c} S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2 \\ \\hline S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} $$ The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\) . We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\) . $$ \\begin{array}{rc} {\\tt (tiAssign)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline X = E \\vDash { (\\alpha_X, \\hat{T}) } \\cup \\kappa \\end{array} \\ \\ \\end{array} $$ The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\) , the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\) , which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\) , it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\) 's type and the type of the assignment's RHS must agree. $$ \\begin{array}{rc} {\\tt (tiIf)} & \\begin{array}{c} E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3 \\ \\hline if\\ E\\ {\\overline{S_2}}\\ else {\\overline{S_3}} \\vDash {(\\hat{T_1}, bool)} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3 \\end{array} \\ \\ \\end{array} $$ The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\) 's type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\) . \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\) , \\(\\kappa_2\\) and \\(\\kappa_3\\) , in addition, requiring \\(E\\) 's type must be \\(bool\\) . \\[ \\begin{array}{rc} {\\tt (tiWhile)} & \\begin{array}{c} E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\\\ \\hline while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The inference for while statement is very similar to if-else statement. We skip the explanation. Type Inference Rules for SIMP expressions The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\) . \\[ \\begin{array}{rc} {\\tt (tiInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\\\ \\hline C \\vDash int, \\{\\} \\end{array} \\\\ \\\\ {\\tt (tiBool)} & \\begin{array}{c} C\\ \\in \\{true, false\\} \\\\ \\hline C \\vDash bool, \\{\\} \\end{array} \\end{array} \\] When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\) . \\[ \\begin{array}{rc} {\\tt (tiVar)} & X \\vDash \\alpha_X, \\{\\} \\end{array} \\] The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\) . A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\". For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form . \\[ \\begin{array}{rc} {\\tt (tiOp1)} & \\begin{array}{c} OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\\\ \\\\ {\\tt (tiOp2)} & \\begin{array}{c} OP \\in \\{<, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\) . \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. $$ \\begin{array}{rc} {\\tt (tiParen)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline (E) \\vDash \\hat{T}, \\kappa \\end{array} \\end{array} $$ The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression. Unification To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. \\[ \\begin{array}{rcl} mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(\\alpha, \\hat{T}) & = & [\\hat{T}/\\alpha] \\\\ mgu(\\hat{T}, \\alpha) & = & [\\hat{T}/\\alpha] \\\\ \\end{array} \\] The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier . Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows \\[ \\begin{array}{rcl} mgu(\\{\\}) & = & [] \\\\ mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) & = & let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\ & & \\ \\ \\ \\ \\ \\ \\kappa' = \\Psi_1(\\kappa) \\\\ & & \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa') \\\\ & & in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] There are two cases. the constraint set is empty, we return the empty (identity) substitution. the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\) , which yields a subsitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\) . Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\) . The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\) . Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\) . We see that in an example shortly. An Example Consider the following SIMP program x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < x) { // (\u03b1_y, \u03b1_x) y = y + 1; // (\u03b1_y, int) } For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows input|=\u03b1_input,{} (tiVar) -------------------------(tiAssign) [subtree 1] x=input|={(\u03b1_x,\u03b1_input)} -----------------------------------------------------------------------------(tiSeq) x=input; y=0; while (y<x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 1] is as follows y|=\u03b1_y,{} (tiVar) 0|=int,{} (tiInt) ------------------(tiAssign) [subtree 2] y=0|={(\u03b1_y,int)} --------------------------------------------------------(tiSeq) y=0; while (y<x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 2] is as follows y|=\u03b1_y,{} (tiVar) 1|=int,{} (tiInt) y|=\u03b1_y,{} (tiVar) --------------(tiOp1) x|=\u03b1_x,{} (tiVar) y+1|=int,{(\u03b1_y,int)} --------------(tiOp2) ----------------------(tiAssign) y<x|=bool,{(\u03b1_y,\u03b1_x)} y=y+1|= {(\u03b1_y,int)} ---------------------------------------------(tiWhile) --------------(tiReturn) while (y<x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} return y|= {} ---------------------------------------------------------------------(tiSeq) while (y<x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} From Type Substitution to Type Environment To derive the inferred type environment, we apply the type substitution to all the type variabales we created. Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\) . Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows, \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] Recall that the set of constraints generated from the running example is \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\] Unification from left to right Suppose the unification progress pick the entries from left to right \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) $$ \\begin{array}{ll} mgu({(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})}) & \\longrightarrow \\ let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} $$ Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] Unification from right to left Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) & \\longrightarrow \\\\ let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred the same type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\) . If you have time, you can try another order. Input's type In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\) . This is not always possible. Let's consider the following program. x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } In the genereated constraints, our algorithm can construct the subtitution \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\) . We may argue that this is an ill-defined program as input and x are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\) , we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\) . Uninitialized Variable There is another situatoin in which the inference algorithm fails to ground all the type variables. x = z; // (\u03b1_x, \u03b1_z) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as z is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis. Property 4: Type Inference Soundness The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\) . Property 5: Principality The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\) .","title":"50.054 Static Semantics For SIMP"},{"location":"notes/static_semantics/#50054-static-semantics-for-simp","text":"","title":"50.054 Static Semantics For SIMP"},{"location":"notes/static_semantics/#learning-outcomes","text":"Explain what static semantics is. Apply type checking rules to verify the type correctness property of a SIMP program. Explain the relation between type system and operational semantics. Apply type inference algorithm to generate a type environment given a SIMP program.","title":"Learning Outcomes"},{"location":"notes/static_semantics/#what-is-static-semantics","text":"While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program. For example, a statically correct program, must satisfy some properties all uses of variables in it must be defined somewhere earlier. all the use of variables, the types must be matching with the expected type in the context. ... Here is a statically correct SIMP program, x = 0; y = input; if y > x { y = 0; } return y; because it satifies the first two properties. The following program is not statically correct. x = 0; y = input; if y + x { // type error x = z; // the use of an undefined variable z } return x; Static checking is to rule out the statically incorrect programs.","title":"What is static semantics?"},{"location":"notes/static_semantics/#type-checking-for-simp","text":"We consider the type checking for SIMP programs. Recall the syntax rules for SIMP \\[ \\begin{array}{rccl} (\\tt Statement) & S & ::= & X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) & E & ::= & E\\ OP\\ E \\mid X \\mid C \\mid (E) \\\\ (\\tt Statements) & \\overline{S} & ::= & S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) & OP & ::= & + \\mid - \\mid * \\mid < \\mid == \\\\ (\\tt Constant) & C & ::= & 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\ (\\tt Variable) & X & ::= & a \\mid b \\mid c \\mid d \\mid ... \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (X \\times T) \\end{array} \\] We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . We define two different relations, \\(\\Gamma \\vdash E : T\\) , which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\) . \\(\\Gamma \\vdash \\overline{S}\\) , which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\) .","title":"Type Checking for SIMP"},{"location":"notes/static_semantics/#type-checking-rules-for-simp-expressions","text":"$$ \\begin{array}{rc} {\\tt (tVar)} & \\begin{array}{c} (X,T) \\in \\Gamma \\ \\hline \\Gamma \\vdash X : T \\end{array} \\end{array} $$ In the rule \\({\\tt (tVar)}\\) , we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\) . $$ \\begin{array}{rc} {\\tt (tInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash C : int \\end{array} \\ \\ {\\tt (tBool)} & \\begin{array}{c} C \\in {true,false} \\ \\hline \\Gamma \\vdash C : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tInt)}\\) , we type check an integer constant having type \\(int\\) . Similarly, we type check a boolean constant having type \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tOp1)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { +, -, * } \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : int \\end{array} \\ \\ {\\tt (tOp2)} & \\begin{array}{c} \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP E_2 : bool \\end{array} \\ \\ {\\tt (tOp3)} & \\begin{array}{c} \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in { ==, <} \\ \\hline \\Gamma \\vdash E_1\\ OP\\ E_2 : bool \\end{array} \\end{array} $$ In the rule \\({\\tt (tOp1)}\\) , we type check an integer arithmetic operation having type \\(int\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp2)}\\) , we type check an integer comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(int\\) . In the rule \\({\\tt (tOp3)}\\) , we type check a boolean comparison operation having type \\(bool\\) , if both operands can be type-checked against \\(bool\\) . $$ \\begin{array}{rc} {\\tt (tParen)} & \\begin{array}{c} \\Gamma \\vdash E :T \\ \\hline \\Gamma \\vdash (E) :T \\end{array} \\end{array} $$ Lastly in rule \\({\\tt (tParen)}\\) , we type check a parenthesized expression by type-checking the inner expression.","title":"Type checking rules for SIMP Expressions"},{"location":"notes/static_semantics/#type-checking-rules-for-simp-statements","text":"The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of \\(\\Gamma \\vdash \\overline{S} : T\\) , this is because statements do not return a value (except for return statement, which returns a value for the entire program.) \\[ \\begin{array}{rc} {\\tt (tSeq)} & \\begin{array}{c} \\Gamma \\vdash S \\ \\ \\ \\Gamma \\vdash \\overline{S} \\\\ \\hline \\Gamma \\vdash S \\overline{S} \\end{array} \\end{array} \\] The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\) . It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) . \\[ \\begin{array}{rc} {\\tt (tAssign)} & \\begin{array}{c} \\Gamma \\vdash E : T \\ \\ \\ \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash X = E \\end{array} \\end{array} \\] The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\) . It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree. \\[ \\begin{array}{rc} {\\tt (tReturn)} & \\begin{array}{c} \\Gamma \\vdash X : T \\\\ \\hline \\Gamma \\vdash return\\ X \\end{array} \\\\ \\\\ {\\tt (tNop)} & \\Gamma \\vdash nop \\end{array} \\] The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable. $$ \\begin{array}{rc} {\\tt (tIf)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2} \\ \\hline \\Gamma \\vdash if\\ E\\ {\\overline{S_1}}\\ else\\ { \\overline{S_2} } \\end{array} \\ \\ {\\tt (tWhile)} & \\begin{array}{c} \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S} \\ \\hline \\Gamma \\vdash while\\ E\\ {\\overline{S}} \\end{array} \\end{array} $$ The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\) . It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\) . The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way. We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\) , i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\) . On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\) , we consider the type checking derivation of \\[x = input; s = 0; while\\ s<x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] \u0393 |- s:int (tVar) \u0393 |- 0:int (tInt) \u0393 |- input:int (tVar) -----------------(tAssign) [sub tree 1] \u0393 |- x:int (tVar) \u0393 |- s=0 ------------------(tAssign) --------------------------------------(tSeq) \u0393 |- x=input; \u0393 |- s=0; while s<x { s = s + 1;} return s; ---------------------------------------------------------------------(tSeq) \u0393 |- x=input; s=0; while s<x { s = s + 1;} return s; Where [sub tree 1] is \u0393 |- 0:int (tInt) \u0393 |- s:int (tVar) \u0393 |- s:int (tVar) -----------------(tOp1) \u0393 |- x:int (tVar) \u0393 |-s:int (tVar) \u0393 |- s+1:int --------------(tOp2) -------------------------------(tAssign) \u0393 |- s<x:bool \u0393 |- s = s + 1 \u0393 |- s:int (tVar) ---------------------------------------------(tWhile) ---------------(tReturn) \u0393 |- while s<x { s = s + 1;} \u0393 |- return s --------------------------------------------------------------------(tSeq) \u0393 |- while s<x { s = s + 1;} return s; Note that the following two programs are not typeable. // untypeable 1 x = 1; y = 0; if x { y = 0; } else { y = 1; } return y; The above is untypeable because we use x of type int in a context where it is also expected as bool . // untypeable 2 x = input; if (x > 1) { y = true; } else { y = 0; } return y; The above is unteable because we can't find a type environment which has both (y,int) and (y,bool) . So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative . // untypeable 3 x = input; if (x > 1) { if ( x * x * x < x * x) { y = true; } else { y = 1; } } else { y = 0; } return y; Even though we note that when x > 1 , we have x * x * x < x * x == false hence the statement y = true is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units. Let's connect the type-checking rules for SIMP with it dynamic semantics.","title":"Type Checking rules for SIMP Statements"},{"location":"notes/static_semantics/#definition-1-type-and-value-environments-consistency","text":"We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\) . It means the type environments and value environments are consistent.","title":"Definition 1 - Type and Value Environments Consistency"},{"location":"notes/static_semantics/#property-2-progress","text":"The following property says that a well typed SIMP program must not be stuck until it reaches the return statement. Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\overline{S}\\) is either 1. a return statement, or 1. a sequence of statements, and there exist \\(\\Delta\\) , \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) .","title":"Property 2 - Progress"},{"location":"notes/static_semantics/#property-3-preservation","text":"The following property says that the evaluation of a SIMP program does not change its typeability. Let \\(\\Delta\\) , \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\) . Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\) . Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\) .","title":"Property 3 - Preservation"},{"location":"notes/static_semantics/#what-is-type-inference","text":"Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. For example, given the Scala program def f(x:Int) = x + 1 the Scala compiler is able to deduce that the return type of f is Int . Likewise for the following SIMP program y = y + 1 we can also deduce that y is a of type int . What we aim to achieve is a sound and systematic process to deduce the omitted type information.","title":"What is Type Inference"},{"location":"notes/static_semantics/#type-inference-for-simp-program","text":"Given a SIMP program \\(\\overline{S}\\) , the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\) . Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution.","title":"Type inference for SIMP program"},{"location":"notes/static_semantics/#definition-most-general-type-envrionment","text":"Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\) . \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\) .","title":"Definition - Most general type (envrionment)"},{"location":"notes/static_semantics/#type-inference-rules","text":"We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures. \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} & \\hat{T} & ::= &\\alpha \\mid T \\\\ {\\tt (Constraints)} & \\kappa & \\subseteq & (\\hat{T} \\times \\hat{T}) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\) . Type substititution replace type variable to some other type. \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha & = & \\hat{T} \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta & = & \\beta & if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T & = & T \\end{array} \\] Type substiution can be compositional . \\[ \\begin{array}{rcll} (\\Psi_1 \\circ \\Psi_2) \\hat{T} & = & \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms.","title":"Type Inference Rules"},{"location":"notes/static_semantics/#type-inference-rules-for-simp-statements","text":"The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\) , which reads give a sequence of statements \\(\\overline{S}\\) , we generate a set of type constraints \\(\\kappa\\) . $$ \\begin{array}{rc} {\\tt (tiNOP)} & nop\\vDash {} \\ \\ {\\tt (tiReturn)} & return\\ X \\vDash {} \\end{array} $$ The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned. Similar observation applies to the return statement. $$ \\begin{array}{rc} {\\tt (tiSeq)} & \\begin{array}{c} S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2 \\ \\hline S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} $$ The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\) . We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\) . $$ \\begin{array}{rc} {\\tt (tiAssign)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline X = E \\vDash { (\\alpha_X, \\hat{T}) } \\cup \\kappa \\end{array} \\ \\ \\end{array} $$ The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\) , the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\) , which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\) , it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\) 's type and the type of the assignment's RHS must agree. $$ \\begin{array}{rc} {\\tt (tiIf)} & \\begin{array}{c} E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3 \\ \\hline if\\ E\\ {\\overline{S_2}}\\ else {\\overline{S_3}} \\vDash {(\\hat{T_1}, bool)} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3 \\end{array} \\ \\ \\end{array} $$ The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\) 's type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\) . \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\) , \\(\\kappa_2\\) and \\(\\kappa_3\\) , in addition, requiring \\(E\\) 's type must be \\(bool\\) . \\[ \\begin{array}{rc} {\\tt (tiWhile)} & \\begin{array}{c} E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\\\ \\hline while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The inference for while statement is very similar to if-else statement. We skip the explanation.","title":"Type Inference Rules for SIMP statements"},{"location":"notes/static_semantics/#type-inference-rules-for-simp-expressions","text":"The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\) . \\[ \\begin{array}{rc} {\\tt (tiInt)} & \\begin{array}{c} C\\ {\\tt is\\ an\\ integer} \\\\ \\hline C \\vDash int, \\{\\} \\end{array} \\\\ \\\\ {\\tt (tiBool)} & \\begin{array}{c} C\\ \\in \\{true, false\\} \\\\ \\hline C \\vDash bool, \\{\\} \\end{array} \\end{array} \\] When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\) . \\[ \\begin{array}{rc} {\\tt (tiVar)} & X \\vDash \\alpha_X, \\{\\} \\end{array} \\] The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\) . A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\". For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form . \\[ \\begin{array}{rc} {\\tt (tiOp1)} & \\begin{array}{c} OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\\\ \\\\ {\\tt (tiOp2)} & \\begin{array}{c} OP \\in \\{<, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2 \\\\ \\hline E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2 \\end{array} \\end{array} \\] The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\) . \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. $$ \\begin{array}{rc} {\\tt (tiParen)} & \\begin{array}{c} E \\vDash \\hat{T}, \\kappa \\ \\hline (E) \\vDash \\hat{T}, \\kappa \\end{array} \\end{array} $$ The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression.","title":"Type Inference Rules for SIMP expressions"},{"location":"notes/static_semantics/#unification","text":"To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. \\[ \\begin{array}{rcl} mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(\\alpha, \\hat{T}) & = & [\\hat{T}/\\alpha] \\\\ mgu(\\hat{T}, \\alpha) & = & [\\hat{T}/\\alpha] \\\\ \\end{array} \\] The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier . Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows \\[ \\begin{array}{rcl} mgu(\\{\\}) & = & [] \\\\ mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) & = & let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\ & & \\ \\ \\ \\ \\ \\ \\kappa' = \\Psi_1(\\kappa) \\\\ & & \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa') \\\\ & & in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] There are two cases. the constraint set is empty, we return the empty (identity) substitution. the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\) , which yields a subsitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\) . Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\) . The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\) . Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\) . We see that in an example shortly.","title":"Unification"},{"location":"notes/static_semantics/#an-example","text":"Consider the following SIMP program x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < x) { // (\u03b1_y, \u03b1_x) y = y + 1; // (\u03b1_y, int) } For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows input|=\u03b1_input,{} (tiVar) -------------------------(tiAssign) [subtree 1] x=input|={(\u03b1_x,\u03b1_input)} -----------------------------------------------------------------------------(tiSeq) x=input; y=0; while (y<x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 1] is as follows y|=\u03b1_y,{} (tiVar) 0|=int,{} (tiInt) ------------------(tiAssign) [subtree 2] y=0|={(\u03b1_y,int)} --------------------------------------------------------(tiSeq) y=0; while (y<x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} Where [subtree 2] is as follows y|=\u03b1_y,{} (tiVar) 1|=int,{} (tiInt) y|=\u03b1_y,{} (tiVar) --------------(tiOp1) x|=\u03b1_x,{} (tiVar) y+1|=int,{(\u03b1_y,int)} --------------(tiOp2) ----------------------(tiAssign) y<x|=bool,{(\u03b1_y,\u03b1_x)} y=y+1|= {(\u03b1_y,int)} ---------------------------------------------(tiWhile) --------------(tiReturn) while (y<x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} return y|= {} ---------------------------------------------------------------------(tiSeq) while (y<x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)}","title":"An Example"},{"location":"notes/static_semantics/#from-type-substitution-to-type-environment","text":"To derive the inferred type environment, we apply the type substitution to all the type variabales we created. Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\) . Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows, \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] Recall that the set of constraints generated from the running example is \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\]","title":"From Type Substitution to Type Environment"},{"location":"notes/static_semantics/#unification-from-left-to-right","text":"Suppose the unification progress pick the entries from left to right \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) $$ \\begin{array}{ll} mgu({(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})}) & \\longrightarrow \\ let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}{(\\alpha_{y},\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\ \\ \\ \\ \\ \\ \\ \\kappa_2 = {(int,\\alpha_{input})} \\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} $$ Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\]","title":"Unification from left to right"},{"location":"notes/static_semantics/#unification-from-right-to-left","text":"Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) & \\longrightarrow \\\\ let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\\\ let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 & \\longrightarrow \\end{array} \\] Where derivation of \\(mgu(\\kappa_1)\\) \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) & \\longrightarrow \\\\ let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\ \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\ in\\ \\Psi_{22} \\circ \\Psi_{21} & \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] Hence the final result is \\[ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] We apply this type substitution to all the variables in the program. \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} & = \\\\ [int/\\alpha_{input}] \\alpha_{input} & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} & = \\\\ ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} & = \\\\ [int/\\alpha_{input}] int & = \\\\ int \\\\ \\\\ \\end{array} \\] So we have computed the inferred the same type environment \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\) . If you have time, you can try another order.","title":"Unification from right to left"},{"location":"notes/static_semantics/#inputs-type","text":"In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\) . This is not always possible. Let's consider the following program. x = input; // (\u03b1_x, \u03b1_input) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } In the genereated constraints, our algorithm can construct the subtitution \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\) . We may argue that this is an ill-defined program as input and x are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\) , we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\) .","title":"Input's type"},{"location":"notes/static_semantics/#uninitialized-variable","text":"There is another situatoin in which the inference algorithm fails to ground all the type variables. x = z; // (\u03b1_x, \u03b1_z) y = 0; // (\u03b1_y, int) while (y < 3) { // (\u03b1_y, int) y = y + 1; // (\u03b1_y, int) } in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as z is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis.","title":"Uninitialized Variable"},{"location":"notes/static_semantics/#property-4-type-inference-soundness","text":"The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\) .","title":"Property 4: Type Inference Soundness"},{"location":"notes/static_semantics/#property-5-principality","text":"The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment. Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\) .","title":"Property 5: Principality"},{"location":"notes/static_semantics_2/","text":"50.054 Static Semantics for Lambda Calculus Learning Outcomes Apply type checking algorithm to type check a simply typed lambda calculus expression. Apply Hindley Milner algorithm to type check lambda calculus expressions. Apply Algorithm W to infer type for lambda calculus. Type Checking for Lambda Calculus To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. Recall the lambda calculus syntax, with the following adaptation $$ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times T) \\end{array} $$ The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\) . The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\) . Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus . Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details. We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\) , where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\) , \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (lctBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean. $$ \\begin{array}{cc} {\\tt (lctVar)} & \\begin{array}{c} (x, T) \\in \\Gamma \\ \\hline \\Gamma \\vdash x : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctVar)}\\) , we type check a variable \\(x\\) against a type \\(T\\) , which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\ \\hline \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T' \\end{array} \\end{array} $$ In rule \\({\\tt (lctLam)}\\) , we type check a lambda abstraction against a type \\(T\\rightarrow T'\\) . This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\) . $$ \\begin{array}{cc} {\\tt (lctApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctApp)}\\) , we type check a function application, applying \\(t_1\\) to \\(t_2\\) , against a type \\(T_2\\) . This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\) . $$ \\begin{array}{cc} {\\tt (lctLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\ \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x:T_1 = t_1\\ in\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctLet)}\\) , we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\) . This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment \\(\\Gamma \\oplus (x, T_1)\\) . $$ \\begin{array}{cc} {\\tt (lctIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\ \\hline \\Gamma \\vdash if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctIf)}\\) , we type check a if-then-else expression against type \\(T\\) . This is only valid if \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\) . \\[ \\begin{array}{cc} {\\tt (lctOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\\\ \\\\ {\\tt (lctOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ {\\tt (lctOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ \\end{array} \\] The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\) . \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree. \\[ \\begin{array}{cc} {\\tt (lctFix)} & \\begin{array}{c} \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2 \\\\ \\hline \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2 \\end{array} \\end{array} \\] The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\) . We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\) . For example, we would like to type check the following simply typed lambda term. $$ fix\\ (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if\\ x == 0\\ then\\ 1\\ else\\ (f\\ (x-1))* x))) $$ against the type \\(int \\rightarrow int\\) We added the optional parantheses for readability. We find the the following type checking derivation (proof tree). Let \u0393 be the initial type environment. \u0393\u2295(f:int->int)\u2295(x:int)|- x:int (lctVar) \u0393\u2295(f:int->int)\u2295(x:int)|- 0:int (lctInt) ---------------------------------------(lctOp2) [sub tree 1] [sub tree 2] \u0393\u2295(f:int->int)\u2295(x:int)|- x == 0: bool ------------------------------------------------------------------------------- (lctIf) \u0393\u2295(f:int->int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int --------------------------------------------------------------------(lctLam) \u0393\u2295(f:int->int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int->int --------------------------------------------------------------------------------(lctLam) \u0393 |- \u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int->int)->int->int ---------------------------------------------------------------------------------(lctFix) \u0393 |- fix (\u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int->int Let \u03931=\u0393\u2295(f:int->int)\u2295(x:int) Where [sub tree 1] is \u03931|- 1:int (lctInt) and [sub tree 2] is \u03931|-x:int (lctVar) \u03931|-1:int (lctInt) -----------------(lctOp1) \u03931|- f:int->int (lctVar) \u03931|- x-1:int -------------------------------------------------(lctApp) \u03931|- f (x-1):int \u03931 |- x:int (lctVar) -------------------------------------------------------------------------(lctOp1) \u03931|- (f (x-1))*x:int Another (counter) example which shows that we can't type check the following program \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] against the type \\(int\\) . fail, no proof exists ---------------------- \u0393\u2295(x:int)|- x:bool ----------------------------------(lctIf) \u0393|-1:int (lctInt) \u0393\u2295(x:int)|-if x then x else 0:int --------------------------------------------------------(lctLet) \u0393|- let x:int = 1 in (if x then x else 0):int Property 1 - Uniqueness The following property states that if a lambda term is typable, its type must be unique. Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\) . Then \\(T\\) and \\(T'\\) must be the same. Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\) , i.e. all the variables being mapped. Property 2 - Progress The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck. Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) . Property 3 - Preservation The third property states that the type of a lambda term does not change over evaluation. Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\) . Then \\(\\Gamma \\vdash t':T\\) . Issue with let-binding The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term. \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\ \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to f , either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both. To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System. Hindley Milner Type System We define the lambda calculus syntax for Hindley Milner Type System as follows \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\ {\\tt (Type Scheme)} & \\sigma & ::= & \\forall \\alpha. \\sigma \\mid T \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times \\sigma ) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them. We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types. We describe the Hindley Milner Type Checking rules as follows $$ \\begin{array}{rc} {\\tt (hmInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (hmBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rules for constants remain unchanged. \\[ \\begin{array}{rc} {\\tt (hmVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\\\ \\hline \\Gamma \\vdash x : \\sigma \\end{array} \\end{array} \\] The rule for variable is adjusted to use type signatures instead of types. \\[ \\begin{array}{rc} {\\tt (hmLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\\\ \\hline \\Gamma \\vdash \\lambda x.t :T\\rightarrow T' \\end{array} \\\\ \\\\ {\\tt (hmApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\\\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} \\] In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\) . It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\) . The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\) . $$ \\begin{array}{rc} {\\tt (hmFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma \\ \\hline \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha \\end{array} \\end{array} $$ To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\) . $$ \\begin{array}{rc} {\\tt (hmIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : \\sigma \\ \\ \\ \\Gamma \\vdash t_3 : \\sigma \\ \\hline \\Gamma \\vdash if\\ t_1\\ { t_2}\\ else { t_3 }: \\sigma \\end{array} \\ \\ \\end{array} $$ We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\) . $$ \\begin{array}{rc} {\\tt (hmOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in{+,-,*,/} \\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\ \\ {\\tt (hmOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ {\\tt (hmOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ \\end{array} $$ The type checking rules for binary operation remain unchanged. $$ \\begin{array}{rc} {\\tt (hmLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\ \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x = t_1\\ in\\ t_2 :T_2 \\end{array} \\ \\ {\\tt (hmInst)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2 \\ \\hline \\Gamma \\vdash t : \\sigma_2 \\end{array} \\ \\ {\\tt (hmGen)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma) \\ \\hline \\Gamma \\vdash t : \\forall \\alpha.\\sigma \\end{array} \\end{array} $$ In the rule \\({\\tt (hmLet)}\\) , we first type check \\(t_1\\) againt \\(\\sigma_1\\) , which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\) . For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\) . In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\) , provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) . Definition - Type Instances Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\) . In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\) . Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\) , then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\) . The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. \\[ \\begin{array}{rcl} ftv(\\alpha) & = & \\{\\alpha \\} \\\\ ftv(int) & = & \\{ \\} \\\\ ftv(bool) & = & \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) & = & ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) & = & ftv(\\sigma) - \\{ \\alpha \\} \\end{array} \\] \\(ftv()\\) is also overloaded to extra free type variables from a type environment. \\[ \\begin{array}{rcl} ftv(\\Gamma) & = & \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] The application of a type substitution can be defined as \\[ \\begin{array}{rcll} [] \\sigma & = & \\sigma \\\\ [T/\\alpha] int & = & int \\\\ [T/\\alpha] bool & = & bool \\\\ [T/\\alpha] \\alpha & = & T \\\\ [T/\\alpha] \\beta & = & \\beta & \\beta \\neq \\alpha \\\\ [T/\\alpha] T_1 \\rightarrow T_2 & = & ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\ [T/\\alpha] \\forall \\beta. \\sigma & = & \\forall \\beta. ([T/\\alpha]\\sigma) & \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\ (\\Psi_1 \\circ \\Psi_2)\\sigma & = & \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\) . Example Let's consider the type-checking derivation of our running (counter) example. Let \u0393 = {} and \u03931 = {(f,\u2200\u03b1.\u03b1->\u03b1)} . -------------------(hmVar) \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 --------------------(hmLam) \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3->\u03b2 ------------(hmVar) -------------------(hmLam) \u0393\u2295(x,\u03b1)|-x:\u03b1 \u03931|-\u03bbx.\u03bby.x:\u03b2->\u03b3->\u03b2 \u03b3,\u03b2\u2209ftv(\u03931) ------------(hmLam) --------------------------(hmGen) \u0393|-\u03bbx.x:\u03b1->\u03b1 \u03b1\u2209ftv(\u0393) \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 1] -----------------(hmGen) -------------------------------------------(hmLet) \u0393|-\u03bbx.x:\u2200\u03b1.\u03b1->\u03b1 \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int ------------------------------------------------------------------- (hmLet) \u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int Let \u03932 = {(f,\u2200\u03b1.\u03b1->\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)} , we find [subtree 1] is as follows --------------------(hmVar) \u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2291 \u2200\u03b3.int->\u03b3->int ----------------------------------(hmInst) \u03932|-g:\u2200\u03b3.int->\u03b3->int [subtree 3] -----------------------------------------------(hmApp) \u03932|-g (f 1):\u2200\u03b3.\u03b3->int \u2200\u03b3.\u03b3->int \u2291 bool->int -------------------------------------------------(hmInst) \u03932|-g (f 1):bool->int [subtree 2] ---------------------------------------------------------------(hmApp) \u03932|-g (f 1) (f true):int Where [subtree 2] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 bool->bool -------------------(hmInst) ----------------(hmBool) \u03932|-f:bool->bool \u03932|-true:bool ----------------------------------------------------(hmApp) \u03932|-f true:bool Where [subtree 3] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 int->int -------------------(hmInst) ----------------(hmInt) \u03932|-f:int->int \u03932|-1:int ---------------------------------------------------(hmApp) \u03932|-f 1:int As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\) , we are able to give let-bound variables f and g some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule. Property 4 - Uniqueness The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming. Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\) . Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming. For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same. Property 5 - Progress The Progress property is valid for Hindley Milner type checking. Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) . Property 6 - Preservation The Presevation property is also held for Hindley Milner type checking. Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\) . Then \\(\\Gamma \\vdash t':\\sigma\\) . Type Inference for Lambda Calculus To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W . The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\) , which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\) , the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\) . \\[ \\begin{array}{rc} {\\tt (wInt)} & \\begin{array}{c} c\\ {\\tt is\\ an\\ integer} \\\\ \\hline \\Gamma, c \\vDash int, [] \\end{array} \\\\ \\\\ {\\tt (wBool)} & \\begin{array}{c} c\\in \\{true,false \\} \\\\ \\hline \\Gamma, c \\vDash bool, [] \\end{array} \\end{array} \\] The rules for integer and boolean constants are straight forward. We omit the explanation. \\[ \\begin{array}{rc} {\\tt (wVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T \\\\ \\hline \\Gamma, x \\vDash T, [] \\end{array} \\\\ \\\\ {\\tt (wFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T \\\\ \\hline \\Gamma, fix \\vDash T, [] \\end{array} \\end{array} \\] The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\) . Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\) , which serves as the starting input. \\[ \\begin{array}{rc} {\\tt (wLam)} & \\begin{array}{c} \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi \\\\ \\hline \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi \\end{array} \\end{array} \\] The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\) . Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\) . The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\) . The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\) . For instance \\(\\lambda x. x + 1\\) will ground \\(x\\) 's skolem type variable to \\(int\\) . \\[ \\begin{array}{rc} {\\tt (wApp)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3) \\\\ \\hline \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\) . We first apply the inference recursively to \\(t_1\\) , producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\) . Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\) 's type as \\(T_2\\) with a subsitution \\(\\Psi_2\\) . To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\) ), and \\(T_2 \\rightarrow \\alpha_3\\) . If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\) . \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\) . At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution. \\[ \\begin{array}{rc} {\\tt (wLet)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2 \\\\ \\hline \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\) . By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\) . We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\) , and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. $$ \\begin{array}{rc} {\\tt (wOp1)} & \\begin{array}{c} op \\in {+,-,*,/} \\ \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\ \\ {\\tt (wOp2)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} $$ The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\) . In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\) . Finally we need to unify \\(\\Psi_2(T_1)\\) , \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\) . Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\) , i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\) , or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\) . We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. In rule \\({\\tt (wOp2)}\\) , the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\) . \\[ \\begin{array}{rc} {\\tt (wIf)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\ \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\ \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\ \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3)) \\\\ \\hline \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)), \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1' \\end{array} \\end{array} \\] In the rule \\({\\tt (wIf)}\\) , we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\) . Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\) . We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\) . Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\) . Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution. Helper functions We find the list of helper functions defined in Algorithm W. Type Substitution \\[ \\begin{array}{rcl} \\Psi(\\Gamma) &= & \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\] Type Instantiation \\[ \\begin{array}{rcl} inst(T) & = & T \\\\ inst(\\forall \\alpha.\\sigma) & = & \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] The type instantation function instantiate a type scheme. In case of a simple type \\(T\\) , it returns \\(T\\) . In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\) , we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\) . In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification. Type Generalization \\[ \\begin{array}{rcl} gen(\\Gamma, T) & = & \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\) , i.e. skolem variables. Type Unification \\[ \\begin{array}{rcl} mgu(\\alpha, T) & = & [T/\\alpha] \\\\ mgu(T, \\alpha) & = & [T/\\alpha] \\\\ mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) & = & let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\ & & in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ & & \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\) , we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them. Examples Let's consider some examples Example 1 \\(\\lambda x.x\\) Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\) (x,\u03b11)\u2208\u0393\u2295(x,\u03b11) inst(\u03b11)=\u03b11 ----------------------------(wVar) \u03b11=newvar \u0393\u2295(x,\u03b11),x|=\u03b11,[] ------------------------------------------(wLam) \u0393,\u03bbx.x|= \u03b11->\u03b11, [] Example 2 \\(\\lambda x.\\lambda y.x\\) (x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21 --------------------------------(wVar) \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[] --------------------------------------(wLam) \u03b21=newvar \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31->\u03b21,[] -------------------------------------------------(wLam) \u0393,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\) [Example 1] ------------------- \u0393,\u03bbx.x|= \u03b11->\u03b11, [] gen(\u0393,\u03b11->\u03b11)=\u2200\u03b1.\u03b1->\u03b1 [subtree 1] ------------------------------------------------------------------(wLet) \u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41] Let \u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1) , where [subtree 1] is [Example 2] -------------------------- \u03931,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] gen(\u03931,\u03b21->\u03b31->\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 2] -------------------------------------------------------------------------------(wLet) \u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[] Let \u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2) , where [subtree 2] is [subtree 3] [subtree 5] \u03b41=newvar mgu(\u03b32->int,bool->\u03b41)=[bool/\u03b32,int/\u03b41] --------------------------------------------------------------------------(wApp) \u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41] Where [subtree 3] is (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)\u2208\u03932 \u03b51=newvar inst(\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)=\u03b22->\u03b32->\u03b22 mgu(\u03b22->\u03b32->\u03b22,int->\u03b51)=[int/\u03b22,\u03b32->int/\u03b51] --------------------------(wVar) \u03932, g|=\u03b22->\u03b32->\u03b22, [] [subtree 4] ---------------------------------------------------------------------(wApp) \u03932, g (f 1)|= [int/\u03b22,\u03b32->int/\u03b51](\u03b51),[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] Where [subtree 4] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b12->\u03b12 \u03b61=newvar -----------------(wVar) ------------(wInt) \u03932, f|=\u03b12->\u03b12,[] \u03932,1|=int,[] mgu(\u03b12->\u03b12,int->\u03b61)=[int/\u03b61,int/\u03b12] ---------------------------------------------------------------------(wApp) [](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12] Let \u03a83=[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] , note that \u03a83(\u03932) =\u03932 , where [subtree 5] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b13->\u03b13 \u03b71=newvar ----------------(wVar) ----------(wBool) \u03932,f|=\u03b13->\u03b13, [] \u03932,true|=bool,[] mgu(\u03b13->\u03b13,bool->\u03b71)=[bool/\u03b13,bool/\u03b71] -----------------------------------------------------------------------[wApp] \u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71] Property 7: Type Inference Soundness The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\) . Property 8: Principality The following property states that the type generated by Algorithm W is the principal type. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\) .","title":"50.054 Static Semantics for Lambda Calculus"},{"location":"notes/static_semantics_2/#50054-static-semantics-for-lambda-calculus","text":"","title":"50.054 Static Semantics for Lambda Calculus"},{"location":"notes/static_semantics_2/#learning-outcomes","text":"Apply type checking algorithm to type check a simply typed lambda calculus expression. Apply Hindley Milner algorithm to type check lambda calculus expressions. Apply Algorithm W to infer type for lambda calculus.","title":"Learning Outcomes"},{"location":"notes/static_semantics_2/#type-checking-for-lambda-calculus","text":"To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. Recall the lambda calculus syntax, with the following adaptation $$ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times T) \\end{array} $$ The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\) . The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\) . Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus . Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details. We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\) , where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\) , i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\) . We assume for all \\(x \\in dom(\\Gamma)\\) , there exists only one entry of \\((x,T) \\in \\Gamma\\) . Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\) , \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (lctBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean. $$ \\begin{array}{cc} {\\tt (lctVar)} & \\begin{array}{c} (x, T) \\in \\Gamma \\ \\hline \\Gamma \\vdash x : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctVar)}\\) , we type check a variable \\(x\\) against a type \\(T\\) , which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\) . $$ \\begin{array}{cc} {\\tt (lctLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\ \\hline \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T' \\end{array} \\end{array} $$ In rule \\({\\tt (lctLam)}\\) , we type check a lambda abstraction against a type \\(T\\rightarrow T'\\) . This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\) . $$ \\begin{array}{cc} {\\tt (lctApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctApp)}\\) , we type check a function application, applying \\(t_1\\) to \\(t_2\\) , against a type \\(T_2\\) . This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\) . $$ \\begin{array}{cc} {\\tt (lctLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\ \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x:T_1 = t_1\\ in\\ t_2 :T_2 \\end{array} \\end{array} $$ In rule \\({\\tt (lctLet)}\\) , we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\) . This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment \\(\\Gamma \\oplus (x, T_1)\\) . $$ \\begin{array}{cc} {\\tt (lctIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\ \\hline \\Gamma \\vdash if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T \\end{array} \\end{array} $$ In rule \\({\\tt (lctIf)}\\) , we type check a if-then-else expression against type \\(T\\) . This is only valid if \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\) . \\[ \\begin{array}{cc} {\\tt (lctOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\\\ \\\\ {\\tt (lctOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ {\\tt (lctOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\\\ \\\\ \\end{array} \\] The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\) . \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree. \\[ \\begin{array}{cc} {\\tt (lctFix)} & \\begin{array}{c} \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2 \\\\ \\hline \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2 \\end{array} \\end{array} \\] The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\) . We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\) . For example, we would like to type check the following simply typed lambda term. $$ fix\\ (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if\\ x == 0\\ then\\ 1\\ else\\ (f\\ (x-1))* x))) $$ against the type \\(int \\rightarrow int\\) We added the optional parantheses for readability. We find the the following type checking derivation (proof tree). Let \u0393 be the initial type environment. \u0393\u2295(f:int->int)\u2295(x:int)|- x:int (lctVar) \u0393\u2295(f:int->int)\u2295(x:int)|- 0:int (lctInt) ---------------------------------------(lctOp2) [sub tree 1] [sub tree 2] \u0393\u2295(f:int->int)\u2295(x:int)|- x == 0: bool ------------------------------------------------------------------------------- (lctIf) \u0393\u2295(f:int->int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int --------------------------------------------------------------------(lctLam) \u0393\u2295(f:int->int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int->int --------------------------------------------------------------------------------(lctLam) \u0393 |- \u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int->int)->int->int ---------------------------------------------------------------------------------(lctFix) \u0393 |- fix (\u03bbf:int->int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int->int Let \u03931=\u0393\u2295(f:int->int)\u2295(x:int) Where [sub tree 1] is \u03931|- 1:int (lctInt) and [sub tree 2] is \u03931|-x:int (lctVar) \u03931|-1:int (lctInt) -----------------(lctOp1) \u03931|- f:int->int (lctVar) \u03931|- x-1:int -------------------------------------------------(lctApp) \u03931|- f (x-1):int \u03931 |- x:int (lctVar) -------------------------------------------------------------------------(lctOp1) \u03931|- (f (x-1))*x:int Another (counter) example which shows that we can't type check the following program \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] against the type \\(int\\) . fail, no proof exists ---------------------- \u0393\u2295(x:int)|- x:bool ----------------------------------(lctIf) \u0393|-1:int (lctInt) \u0393\u2295(x:int)|-if x then x else 0:int --------------------------------------------------------(lctLet) \u0393|- let x:int = 1 in (if x then x else 0):int","title":"Type Checking for Lambda Calculus"},{"location":"notes/static_semantics_2/#property-1-uniqueness","text":"The following property states that if a lambda term is typable, its type must be unique. Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\) . Then \\(T\\) and \\(T'\\) must be the same. Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\) , i.e. all the variables being mapped.","title":"Property 1 - Uniqueness"},{"location":"notes/static_semantics_2/#property-2-progress","text":"The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck. Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) .","title":"Property 2 - Progress"},{"location":"notes/static_semantics_2/#property-3-preservation","text":"The third property states that the type of a lambda term does not change over evaluation. Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\) . Then \\(\\Gamma \\vdash t':T\\) .","title":"Property 3 - Preservation"},{"location":"notes/static_semantics_2/#issue-with-let-binding","text":"The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term. \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\ \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to f , either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both. To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System.","title":"Issue with let-binding"},{"location":"notes/static_semantics_2/#hindley-milner-type-system","text":"We define the lambda calculus syntax for Hindley Milner Type System as follows \\[ \\begin{array}{rccl} {\\tt (Lambda\\ Terms)} & t & ::= & x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\ {\\tt (Builtin\\ Operators)} & op & ::= & + \\mid - \\mid * \\mid / \\mid\\ == \\\\ {\\tt (Builtin\\ Constants)} & c & ::= & 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\ {\\tt (Types)} & T & ::= & int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\ {\\tt (Type Scheme)} & \\sigma & ::= & \\forall \\alpha. \\sigma \\mid T \\\\ {\\tt (Type\\ Environments)} & \\Gamma & \\subseteq & (x \\times \\sigma ) \\\\ {\\tt (Type\\ Substitution)} & \\Psi & ::= & [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi \\end{array} \\] In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them. We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types. We describe the Hindley Milner Type Checking rules as follows $$ \\begin{array}{rc} {\\tt (hmInt)} & \\begin{array}{c} \\ c\\ {\\tt is\\ an\\ integer} \\ \\hline \\Gamma \\vdash c : int \\end{array} \\ \\ {\\tt (hmBool)} & \\begin{array}{c} c\\in { true, false} \\ \\hline \\Gamma \\vdash c : bool \\end{array} \\end{array} $$ The rules for constants remain unchanged. \\[ \\begin{array}{rc} {\\tt (hmVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\\\ \\hline \\Gamma \\vdash x : \\sigma \\end{array} \\end{array} \\] The rule for variable is adjusted to use type signatures instead of types. \\[ \\begin{array}{rc} {\\tt (hmLam)} & \\begin{array}{c} \\Gamma \\oplus (x, T) \\vdash t : T' \\\\ \\hline \\Gamma \\vdash \\lambda x.t :T\\rightarrow T' \\end{array} \\\\ \\\\ {\\tt (hmApp)} & \\begin{array}{c} \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash t_2 : T_1 \\\\ \\hline \\Gamma \\vdash t_1\\ t_2 :T_2 \\end{array} \\end{array} \\] In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\) . It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\) . The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\) . $$ \\begin{array}{rc} {\\tt (hmFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma \\ \\hline \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha \\end{array} \\end{array} $$ To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\) . $$ \\begin{array}{rc} {\\tt (hmIf)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : \\sigma \\ \\ \\ \\Gamma \\vdash t_3 : \\sigma \\ \\hline \\Gamma \\vdash if\\ t_1\\ { t_2}\\ else { t_3 }: \\sigma \\end{array} \\ \\ \\end{array} $$ We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\) . $$ \\begin{array}{rc} {\\tt (hmOp1)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in{+,-,*,/} \\ \\hline \\Gamma \\vdash t_1\\ op\\ t_2 : int \\end{array} \\ \\ {\\tt (hmOp2)} & \\begin{array}{c} \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ {\\tt (hmOp3)} & \\begin{array}{c} \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\ \\hline \\Gamma \\vdash t_1\\ ==\\ t_2 : bool \\end{array} \\ \\ \\end{array} $$ The type checking rules for binary operation remain unchanged. $$ \\begin{array}{rc} {\\tt (hmLet)} & \\begin{array}{c} \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\ \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\ \\hline \\Gamma \\vdash let\\ x = t_1\\ in\\ t_2 :T_2 \\end{array} \\ \\ {\\tt (hmInst)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2 \\ \\hline \\Gamma \\vdash t : \\sigma_2 \\end{array} \\ \\ {\\tt (hmGen)} & \\begin{array}{c} \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma) \\ \\hline \\Gamma \\vdash t : \\forall \\alpha.\\sigma \\end{array} \\end{array} $$ In the rule \\({\\tt (hmLet)}\\) , we first type check \\(t_1\\) againt \\(\\sigma_1\\) , which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\) . For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\) . In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\) , provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) .","title":"Hindley Milner Type System"},{"location":"notes/static_semantics_2/#definition-type-instances","text":"Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\) . In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\) . Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\) , then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\) . The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. \\[ \\begin{array}{rcl} ftv(\\alpha) & = & \\{\\alpha \\} \\\\ ftv(int) & = & \\{ \\} \\\\ ftv(bool) & = & \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) & = & ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) & = & ftv(\\sigma) - \\{ \\alpha \\} \\end{array} \\] \\(ftv()\\) is also overloaded to extra free type variables from a type environment. \\[ \\begin{array}{rcl} ftv(\\Gamma) & = & \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] The application of a type substitution can be defined as \\[ \\begin{array}{rcll} [] \\sigma & = & \\sigma \\\\ [T/\\alpha] int & = & int \\\\ [T/\\alpha] bool & = & bool \\\\ [T/\\alpha] \\alpha & = & T \\\\ [T/\\alpha] \\beta & = & \\beta & \\beta \\neq \\alpha \\\\ [T/\\alpha] T_1 \\rightarrow T_2 & = & ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\ [T/\\alpha] \\forall \\beta. \\sigma & = & \\forall \\beta. ([T/\\alpha]\\sigma) & \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\ (\\Psi_1 \\circ \\Psi_2)\\sigma & = & \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\) .","title":"Definition - Type Instances"},{"location":"notes/static_semantics_2/#example","text":"Let's consider the type-checking derivation of our running (counter) example. Let \u0393 = {} and \u03931 = {(f,\u2200\u03b1.\u03b1->\u03b1)} . -------------------(hmVar) \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 --------------------(hmLam) \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3->\u03b2 ------------(hmVar) -------------------(hmLam) \u0393\u2295(x,\u03b1)|-x:\u03b1 \u03931|-\u03bbx.\u03bby.x:\u03b2->\u03b3->\u03b2 \u03b3,\u03b2\u2209ftv(\u03931) ------------(hmLam) --------------------------(hmGen) \u0393|-\u03bbx.x:\u03b1->\u03b1 \u03b1\u2209ftv(\u0393) \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 1] -----------------(hmGen) -------------------------------------------(hmLet) \u0393|-\u03bbx.x:\u2200\u03b1.\u03b1->\u03b1 \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int ------------------------------------------------------------------- (hmLet) \u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int Let \u03932 = {(f,\u2200\u03b1.\u03b1->\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)} , we find [subtree 1] is as follows --------------------(hmVar) \u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 \u2291 \u2200\u03b3.int->\u03b3->int ----------------------------------(hmInst) \u03932|-g:\u2200\u03b3.int->\u03b3->int [subtree 3] -----------------------------------------------(hmApp) \u03932|-g (f 1):\u2200\u03b3.\u03b3->int \u2200\u03b3.\u03b3->int \u2291 bool->int -------------------------------------------------(hmInst) \u03932|-g (f 1):bool->int [subtree 2] ---------------------------------------------------------------(hmApp) \u03932|-g (f 1) (f true):int Where [subtree 2] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 bool->bool -------------------(hmInst) ----------------(hmBool) \u03932|-f:bool->bool \u03932|-true:bool ----------------------------------------------------(hmApp) \u03932|-f true:bool Where [subtree 3] is as follows --------------(hmVar) \u03932|-f:\u2200\u03b1.\u03b1->\u03b1 \u2200\u03b1.\u03b1->\u03b1 \u2291 int->int -------------------(hmInst) ----------------(hmInt) \u03932|-f:int->int \u03932|-1:int ---------------------------------------------------(hmApp) \u03932|-f 1:int As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\) , we are able to give let-bound variables f and g some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule.","title":"Example"},{"location":"notes/static_semantics_2/#property-4-uniqueness","text":"The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming. Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\) , \\(x \\in dom(\\Gamma)\\) . Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\) . Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming. For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same.","title":"Property 4 - Uniqueness"},{"location":"notes/static_semantics_2/#property-5-progress","text":"The Progress property is valid for Hindley Milner type checking. Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\) . Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\) . Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\) .","title":"Property 5 - Progress"},{"location":"notes/static_semantics_2/#property-6-preservation","text":"The Presevation property is also held for Hindley Milner type checking. Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\) . Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\) . Then \\(\\Gamma \\vdash t':\\sigma\\) .","title":"Property 6 - Preservation"},{"location":"notes/static_semantics_2/#type-inference-for-lambda-calculus","text":"To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W . The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\) , which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\) , the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\) . \\[ \\begin{array}{rc} {\\tt (wInt)} & \\begin{array}{c} c\\ {\\tt is\\ an\\ integer} \\\\ \\hline \\Gamma, c \\vDash int, [] \\end{array} \\\\ \\\\ {\\tt (wBool)} & \\begin{array}{c} c\\in \\{true,false \\} \\\\ \\hline \\Gamma, c \\vDash bool, [] \\end{array} \\end{array} \\] The rules for integer and boolean constants are straight forward. We omit the explanation. \\[ \\begin{array}{rc} {\\tt (wVar)} & \\begin{array}{c} (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T \\\\ \\hline \\Gamma, x \\vDash T, [] \\end{array} \\\\ \\\\ {\\tt (wFix)} & \\begin{array}{c} (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T \\\\ \\hline \\Gamma, fix \\vDash T, [] \\end{array} \\end{array} \\] The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\) . Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\) , which serves as the starting input. \\[ \\begin{array}{rc} {\\tt (wLam)} & \\begin{array}{c} \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi \\\\ \\hline \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi \\end{array} \\end{array} \\] The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\) . Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\) . The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\) . The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\) . For instance \\(\\lambda x. x + 1\\) will ground \\(x\\) 's skolem type variable to \\(int\\) . \\[ \\begin{array}{rc} {\\tt (wApp)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3) \\\\ \\hline \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\) . We first apply the inference recursively to \\(t_1\\) , producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\) . Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\) 's type as \\(T_2\\) with a subsitution \\(\\Psi_2\\) . To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\) ), and \\(T_2 \\rightarrow \\alpha_3\\) . If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\) . \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\) . At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution. \\[ \\begin{array}{rc} {\\tt (wLet)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2 \\\\ \\hline \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} \\] The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\) . By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\) . We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\) , and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. $$ \\begin{array}{rc} {\\tt (wOp1)} & \\begin{array}{c} op \\in {+,-,*,/} \\ \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\ \\ {\\tt (wOp2)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\ mgu(\\Psi_2(T_1), T_2) = \\Psi_3 \\ \\hline \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1 \\end{array} \\end{array} $$ The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\) . In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\) . We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\) . Finally we need to unify \\(\\Psi_2(T_1)\\) , \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\) . Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\) , i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\) , or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\) . We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. In rule \\({\\tt (wOp2)}\\) , the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\) . \\[ \\begin{array}{rc} {\\tt (wIf)} & \\begin{array}{c} \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\ \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\ \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\ \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3)) \\\\ \\hline \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)), \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1' \\end{array} \\end{array} \\] In the rule \\({\\tt (wIf)}\\) , we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) . In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\) . Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\) . We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\) . Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\) . Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution.","title":"Type Inference for Lambda Calculus"},{"location":"notes/static_semantics_2/#helper-functions","text":"We find the list of helper functions defined in Algorithm W.","title":"Helper functions"},{"location":"notes/static_semantics_2/#type-substitution","text":"\\[ \\begin{array}{rcl} \\Psi(\\Gamma) &= & \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\]","title":"Type Substitution"},{"location":"notes/static_semantics_2/#type-instantiation","text":"\\[ \\begin{array}{rcl} inst(T) & = & T \\\\ inst(\\forall \\alpha.\\sigma) & = & \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] The type instantation function instantiate a type scheme. In case of a simple type \\(T\\) , it returns \\(T\\) . In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\) , we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\) . In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification.","title":"Type Instantiation"},{"location":"notes/static_semantics_2/#type-generalization","text":"\\[ \\begin{array}{rcl} gen(\\Gamma, T) & = & \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\) , i.e. skolem variables.","title":"Type Generalization"},{"location":"notes/static_semantics_2/#type-unification","text":"\\[ \\begin{array}{rcl} mgu(\\alpha, T) & = & [T/\\alpha] \\\\ mgu(T, \\alpha) & = & [T/\\alpha] \\\\ mgu(int, int) & = & [] \\\\ mgu(bool, bool) & = & [] \\\\ mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) & = & let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\ & & in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ & & \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\) , we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them.","title":"Type Unification"},{"location":"notes/static_semantics_2/#examples","text":"Let's consider some examples","title":"Examples"},{"location":"notes/static_semantics_2/#example-1-lambda-xx","text":"Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\) (x,\u03b11)\u2208\u0393\u2295(x,\u03b11) inst(\u03b11)=\u03b11 ----------------------------(wVar) \u03b11=newvar \u0393\u2295(x,\u03b11),x|=\u03b11,[] ------------------------------------------(wLam) \u0393,\u03bbx.x|= \u03b11->\u03b11, []","title":"Example 1 \\(\\lambda x.x\\)"},{"location":"notes/static_semantics_2/#example-2-lambda-xlambda-yx","text":"(x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21 --------------------------------(wVar) \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[] --------------------------------------(wLam) \u03b21=newvar \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31->\u03b21,[] -------------------------------------------------(wLam) \u0393,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[]","title":"Example 2 \\(\\lambda x.\\lambda y.x\\)"},{"location":"notes/static_semantics_2/#example-3-let-flambda-xx-in-let-glambda-xlambda-yx-in-g-f-1-f-true","text":"[Example 1] ------------------- \u0393,\u03bbx.x|= \u03b11->\u03b11, [] gen(\u0393,\u03b11->\u03b11)=\u2200\u03b1.\u03b1->\u03b1 [subtree 1] ------------------------------------------------------------------(wLet) \u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41] Let \u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1) , where [subtree 1] is [Example 2] -------------------------- \u03931,\u03bbx.\u03bby.x|= \u03b21->\u03b31->\u03b21,[] gen(\u03931,\u03b21->\u03b31->\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2 [subtree 2] -------------------------------------------------------------------------------(wLet) \u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[] Let \u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1->\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2) , where [subtree 2] is [subtree 3] [subtree 5] \u03b41=newvar mgu(\u03b32->int,bool->\u03b41)=[bool/\u03b32,int/\u03b41] --------------------------------------------------------------------------(wApp) \u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41] Where [subtree 3] is (g,\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)\u2208\u03932 \u03b51=newvar inst(\u2200\u03b2.\u2200\u03b3.\u03b2->\u03b3->\u03b2)=\u03b22->\u03b32->\u03b22 mgu(\u03b22->\u03b32->\u03b22,int->\u03b51)=[int/\u03b22,\u03b32->int/\u03b51] --------------------------(wVar) \u03932, g|=\u03b22->\u03b32->\u03b22, [] [subtree 4] ---------------------------------------------------------------------(wApp) \u03932, g (f 1)|= [int/\u03b22,\u03b32->int/\u03b51](\u03b51),[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] Where [subtree 4] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b12->\u03b12 \u03b61=newvar -----------------(wVar) ------------(wInt) \u03932, f|=\u03b12->\u03b12,[] \u03932,1|=int,[] mgu(\u03b12->\u03b12,int->\u03b61)=[int/\u03b61,int/\u03b12] ---------------------------------------------------------------------(wApp) [](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12] Let \u03a83=[int/\u03b22,\u03b32->int/\u03b51]\u25cb[int/\u03b61,int/\u03b12] , note that \u03a83(\u03932) =\u03932 , where [subtree 5] is (f,\u2200\u03b1.\u03b1->\u03b1)\u2208\u03932 inst(\u2200\u03b1.\u03b1->\u03b1)=\u03b13->\u03b13 \u03b71=newvar ----------------(wVar) ----------(wBool) \u03932,f|=\u03b13->\u03b13, [] \u03932,true|=bool,[] mgu(\u03b13->\u03b13,bool->\u03b71)=[bool/\u03b13,bool/\u03b71] -----------------------------------------------------------------------[wApp] \u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71]","title":"Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\)"},{"location":"notes/static_semantics_2/#property-7-type-inference-soundness","text":"The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\) .","title":"Property 7: Type Inference Soundness"},{"location":"notes/static_semantics_2/#property-8-principality","text":"The following property states that the type generated by Algorithm W is the principal type. Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\) . Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\) .","title":"Property 8: Principality"},{"location":"notes/syntax_analysis/","text":"50.054 - Syntax Analysis Learning Outcome By the end of this lesson, you should be able to Describe the roles and functionalities of lexers and parsers in a compiler pipeline Describe the difference between top-down parsing and bottom-up parsing Apply left-recursion elimination and left-factoring Construct a LL(1) predictive parsing table Explain first-first conflicts and first-follow conflicts A compiler pipeline graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation A parse tree can be considered the first intermediate representation (IR). Language, Grammar and Rules What is a language? A language is a set of strings. What is a grammar? A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words. One common way to define a grammar is by defining a set of production rules. A running example Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis. The grammar rule for JSON is as follows <<Grammar 1>> (JSON) J ::= i | 's' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 's':J In the above, the grammar consists of four production rules. Each production rule is of form (Name) LHS ::= RHS Sometimes, we omit the Name. Terms in upper case, are the non-terminal s, and terms in lower case, and symbol terms are the terminal s. For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by | . Each alternatives consists of terminals, non-terminals and mixture of both. For instance, the production rule (JSON) states that a JSON non-terminal J is either an i (an integer), a 's' (a quoted string), an empty list [] , an non-empty list [IS] and an object {NS} . A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the (JSON) production rule can be rewritten as follows, J ::= i J ::= 's' J ::= [] J ::= [IS] J ::= {NS} For each grammar, we expect the LHS of the first production rule is the starting symbol. Lexing Input: Source file in string Output: A sequence of valid tokens according to the language specification (grammar) The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing . Sometimes, a lexer is omitted, as the token validation task can be handled in the parser. Lexical Tokens The set of tokens of a grammar is basically all the terminals. In this JSON grammar example, {i, s, ', [, ], {, }, :, \\, } and white spaces are the Lexical Tokens of the language. If we are to represent it using Scala data types, we could use the following algebraic data type: enum LToken { // lexical Tokens case IntTok(v:Int) case StrTok(v:String) case SQuote case LBracket case RBracket case LBrace case RBrace case Colon case Comma case WhiteSpace } Note that in the above, we find that IntTok and StrTok have semantic components (i.e. the underlying values.) The rest of the tokens do not. Given the input {'k1':1,'k2':[]} the lexer function lex(s:String):List[LToken] should return List(LBRace,SQuote,StrTok(\"k1\"),SQuote,Colon,IntTok(1),Comma,SQuote, StrTok(\"k2\"), Colon,LBracket, RBracket, RBrace) One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows: <<Grammar 2>> (JSON) J ::= I | 'STR' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 'STR':J (Integer) I ::= dI | d (String) STR ::= aSTR | a where d denotes a single digit and a denotes a single ascii character. For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages. Implementing a Lexer using Regular Expression Perhaps one easier way to implement a lexer is to make use of regular expression. A simple example of using scala.util.matching.Regex We can specify a regex pattern as follows. This example was adopted from ( https://www.scala-lang.org/api/3.0.2/scala/util/matching/Regex.html ) val date = raw\"(\\d{4})-(\\d{2})-(\\d{2})\".r Next we can perform a match against the above regex pattern using the match expression. \"2004-01-20\" match { case date(year, month, day) => s\"$year was a good year for PLs.\" } The above expression is evaluated to 2004 was a good year for PLs. We could develop a simple lexer using the above trick. First we define the pattern for reach token. val integer = raw\"(\\d+)(.*)\".r val string = raw\"([^']*)(.*)\".r val squote = raw\"(')(.*)\".r val lbracket = raw\"(\\[)(.*)\".r val rbracket = raw\"(\\])(.*)\".r val lbrace = raw\"(\\{)(.*)\".r val rbrace = raw\"(\\})(.*)\".r val colon = raw\"(:)(.*)\".r val comma = raw\"(,)(.*)\".r For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration. Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned. import LToken.* type Error = String def lex_one(src:String):Either[String, (LToken, String)] = src match { case integer(s, rest) => Right((IntTok(s.toInt), rest)) case squote(_, rest) => Right((SQuote, rest)) case lbracket(_, rest) => Right((LBracket, rest)) case rbracket(_, rest) => Right((RBracket, rest)) case lbrace(_, rest) => Right((LBracket, rest)) case rbrace(_, rest) => Right((RBracket, rest)) case colon(_, rest) => Right((Colon, rest)) case comma(_, rest) => Right((Comma, rest)) case string(s, rest) => Right((StrTok(s), rest)) case _ => Left(s\"lexer error: unexpected token at ${src}\") } Note that the order of the Scala patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern string and the rest except for squote ). Lastly we define the top level lex function by calling lex_one in a recursive function. def lex(src:String):Either[Error, List[LToken]] = { def go(src:String, acc:List[LToken]):Either[Error, List[LToken]] = { if (src.length == 0) { Right(acc) } else { lex_one(src) match { case Left(error) => Left(error) case Right((ltoken, rest)) => go(rest, acc++List(ltoken)) } } } go(src, List()) } Implementing a Lexer using a Parser In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.) Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems. Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Why tree representation? Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation. Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules. Parsing Derivation Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS. Consider the JSON grammar in its unabridged form, (1) J ::= i (2) J ::= 's' (3) J ::= [] (4) J ::= [IS] (5) J ::= {NS} (6) IS ::= J,IS (7) IS ::= J (8) NS ::= N,NS (9) NS ::= N (10) N ::= 's':J We take the output from our lexer example as the input, with some simplification by removing the Scala constructors { , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , } For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom. Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } ... (for the steps skipped, please refer to syntax_analysis_annex.md) (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS). This algorithm is easy to understand but it has some flaws. It does not terminate when the grammar contains left recursion. It involves some trial-and-error (back-tracking), hence it is not efficient Ambiguous Grammar A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees. Consider the following <<Grammar 3>> E ::= E + E E ::= E * E E ::= i Consider the input 1 + 2 * 3 . Parsing this input with the above grammar produces graph E-->E1[\"E\"] E-->+ E-->E2[\"E\"] E1-->i1[\"i(1)\"] E2-->E3[\"E\"] E2-->* E2-->E4[\"E\"] E3-->i2[\"i(2)\"] E4-->i3[\"i(3)\"]; or graph E-->E1[\"E\"] E-->* E-->E2[\"E\"] E1-->E3[\"E\"] E1-->+ E1-->E4[\"E\"] E2-->i3[\"i(3)\"] E3-->i1[\"i(1)\"] E4-->i2[\"i(2)\"]; To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that * should bind stronger than + . Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset <<Grammar 4>> E::= T + E E::= T T::= T * F T::= F F::= i As a result, the input 1 + 2 * 3 is parsed as graph E-->T1[\"T\"] E-->+ E-->E1[\"E\"] T1-->F1[\"F\"] F1-->i1[\"i(1)\"] E1-->T2[\"T\"] T2-->F2[\"F\"] F2-->i2[\"i(2)\"] T2-->* T2-->F3[\"F\"] F3-->i3[\"i(3)\"] ; Grammar with Left Recursion Let's try to run a top-down recursive parsing algorithm over the following grammar <<Grammar 5>> E ::= E + T E ::= T T ::= i i and + are terminals, and E and T are the non-terminals. i denotes an integer. Consider applying the top-down recursive parsing mentioned above to the input 1 , if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation. Let N be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals) Left recursive grammar rules $$ \\begin{array}{rcl} N & ::= & N\\alpha_1 \\ & ... & \\ N & ::= & N\\alpha_n \\ N & ::= & \\beta_1 \\ & ... & \\ N & ::= & \\beta_m \\end{array} $$ can be transformed into \\[ \\begin{array}{rcl} N & ::= & \\beta_1 N' \\\\ & ... & \\\\ N & ::= & \\beta_m N' \\\\ N' & ::= & \\alpha_1 N' \\\\ & ... & \\\\ N' & ::= & \\alpha_n N' \\\\ N' & ::= & \\epsilon \\end{array} \\] Now apply the above to our running example. \\(N\\) is E and \\(\\alpha_1\\) is + T , T is \\(\\beta_1\\) . <<Grammar 6>> E ::= TE' E' ::= + TE' E' ::= epsilon T ::= i The resulting Grammar 6 is equivalent the original Grammar 5. Note that epsilon ( \\(\\epsilon\\) ) is a special terminal which denotes an empty sequence. There are few points to take note For indirect left recursion, some substitution steps are required before applying the above transformation. For instance <<Grammar 7>> G ::= H + G H ::= G + i H ::= i We need to substitute H into the first production rule. <<Grammar 8>> G ::= G + i + G G ::= i + G Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input 1 + 1 with Grammar 6 yields the following parse tree graph E-->T1[\"T\"] E-->Ep1[E'] T1-->i1[\"i(1)\"] Ep1-->+ Ep1-->T2[T] Ep1-->Ep2[E'] T2-->i2[\"i(1)\"] Ep2-->eps1[\u03b5] which needs to be transformed back to graph E-->E1[\"E\"] E-->+ E-->T1[\"T\"] E1-->T2[\"T\"] T1-->i1[\"i(1)\"] T2-->i2[\"i(1)\"] Predictive Recursive Parsing Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking. In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as LL(k) grammar. Here k refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking. BTW, LL(k) stands for left-to-right, left-most derivation with k tokens look-ahead algorithm. Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\) , \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\) \\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence. $$ \\begin{array}{rcl} null(t,G) & = & false \\ null(\\epsilon,G) & = & true \\ null(N,G) & = & \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\ null(\\sigma_1...\\sigma_n,G) & = & null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} $$ \\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\) . \\[ \\begin{array}{rcl} first(\\epsilon, G) & = & \\{\\} \\\\ first(t,G) & = & \\{t\\} \\\\ first(N,G) & = & \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) & = & \\left [ \\begin{array}{ll} first(\\sigma,G) \\cup first(\\overline{\\sigma},G) & {\\tt if}\\ null(\\sigma,G) \\\\ first(\\sigma,G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] \\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\) . \\[ \\begin{array}{rcl} follow(\\sigma,G) & = & \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G} \\left [ \\begin{array}{ll} first(\\overline{\\gamma}, G) \\cup follow(N,G) & {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\ first(\\overline{\\gamma}, G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] Sometimes, for convenience we omit the second parameter \\(G\\) . For example, let \\(G\\) be Grammar 6, then \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) = \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point . That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\) . We will discuss fix-point in-depth in some lesson later. Given \\(null\\) , \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in LL(k) . For simplicity, we check the case k = 1 , we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal. i + E E' T For each production rule \\(N ::= \\overline{\\sigma}\\) , we put the production rule in cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\) cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\) We fill up the table i + E E ::= TE' E' E' ::= + TE' T T ::= i We conclude that a grammar is in LL(1) if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a LL(1) grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol. In general, there are two kinds of conflicts found in grammar that violates the LL(1) grammar requirements. first-first conflict first-follow conflict First-first Conflict Consider the grammar <<Grammar 9>> S ::= Xb S ::= Yc X ::= a Y ::= a We compute \\(null\\) , \\(first\\) and \\(follow\\) . \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] We fill up the following predictive parsing table a b c S S::=Xb, S::=Yc X X::=a Y Y::=a From the above we find that there are two production rules in the cell (S,a) , namely S::=Xb , and S::=Yc . This is a first-first conflict, since both production rules' first set contains a . This prevents us from constructing a predictable parser by observing the leading symbol from the input. First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion). From our running example, we find that the cell (S,a) has more than one production rule applicable. This is caused by the fact that both X::=a and Y::=a start with the non-terminal a . We could apply substitution to eliminate X and Y . <<Grammar 10>> S ::= ab S ::= ac Then we could introduce a new non-terminal Z which capture the following languages after a . <<Grammar 11>> S ::= aZ Z ::= b Z ::= c First-Follow Conflict Consider the following grammar <<Grammar 12>> S ::= Xd X ::= C X ::= Ba C ::= epsilon B ::= d and the \\(null\\) , \\(first\\) and \\(follow\\) functions \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] We construct the predictive parsing table as follows a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d In the cell of (X,d) we find two production rules X::=Ba and X::=C (S::=Xd) . It is a first-follow conflict, because the first production rule is discovered through the first(X) set and the second one is from the follow(X) set. Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts Substitute B and C <<Grammar 13>> S ::= Xd X ::= epsilon X ::= da Substitute X <<Grammar 14>> S ::= d S ::= dad However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict. To LL(1) or not LL(1) Given a grammar, we could get a LL(1) grammar equivalent in most of the cases. Disambiguate the grammar if it is ambiguous Eliminate the left recursion Apply left-factoring if there exists some first-first conflict Apply substitution if there exists some first-follow conflict repeat 3 if first-first conflict is introduced Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm. Let's consider another example (a subset of Grammar 3). <<Grammar 15>> E ::= E + E E ::= i Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion <<Grammar 16>> E ::= iE' E' ::= + EE' E' ::= epsilon Next we compute the predictive parsing table. \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon As shown from the above, the grammar contains a first-follow conflict, therefore it is not a LL(1) . It is not possible to perform substitution to eliminate the first-follow conflict because it will lead to infinite expansion. A short summary so far for top-down recursive parsing Top-down parsing is simple, however might be inefficient. We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists. We need to eliminate left recursion so that the parsing will terminate. We construct the predictive parsing table to check whether the grammar is in LL(k) . If the grammar is in LL(k) we can always pick the right production rule given the first k leading symbols from the input. For most of the cases, LL(1) is sufficient for practical use. We also can conclude that a LL(k+1) grammar is also a LL(k) grammar, but the other way does not hold. Given a particular k and a grammar G , we can check whether G is LL(k) . However given a grammar G to find a k such that G is LL(k) is undecidable. Summary We have covered The roles and functionalities of lexers and parsers in a compiler pipeline There are two major types of parser, top-down parsing and bottom-up parsing How to eliminate left-recursion from a grammar, How to apply left-factoring How to construct a LL(1) predictive parsing table","title":"50.054 - Syntax Analysis"},{"location":"notes/syntax_analysis/#50054-syntax-analysis","text":"","title":"50.054 - Syntax Analysis"},{"location":"notes/syntax_analysis/#learning-outcome","text":"By the end of this lesson, you should be able to Describe the roles and functionalities of lexers and parsers in a compiler pipeline Describe the difference between top-down parsing and bottom-up parsing Apply left-recursion elimination and left-factoring Construct a LL(1) predictive parsing table Explain first-first conflicts and first-follow conflicts","title":"Learning Outcome"},{"location":"notes/syntax_analysis/#a-compiler-pipeline","text":"graph LR A[Lexing] -->B[Parsing] --> C[Semantic Analysis] --> D[Optimization] --> E[Target Code Generation] Lexing Input: Source file in String Output: A sequence of valid tokens according to the language specification (grammar) Parsing Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation A parse tree can be considered the first intermediate representation (IR).","title":"A compiler pipeline"},{"location":"notes/syntax_analysis/#language-grammar-and-rules","text":"","title":"Language, Grammar and Rules"},{"location":"notes/syntax_analysis/#what-is-a-language","text":"A language is a set of strings.","title":"What is a language?"},{"location":"notes/syntax_analysis/#what-is-a-grammar","text":"A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words. One common way to define a grammar is by defining a set of production rules.","title":"What is a grammar?"},{"location":"notes/syntax_analysis/#a-running-example","text":"Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis. The grammar rule for JSON is as follows <<Grammar 1>> (JSON) J ::= i | 's' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 's':J In the above, the grammar consists of four production rules. Each production rule is of form (Name) LHS ::= RHS Sometimes, we omit the Name. Terms in upper case, are the non-terminal s, and terms in lower case, and symbol terms are the terminal s. For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by | . Each alternatives consists of terminals, non-terminals and mixture of both. For instance, the production rule (JSON) states that a JSON non-terminal J is either an i (an integer), a 's' (a quoted string), an empty list [] , an non-empty list [IS] and an object {NS} . A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the (JSON) production rule can be rewritten as follows, J ::= i J ::= 's' J ::= [] J ::= [IS] J ::= {NS} For each grammar, we expect the LHS of the first production rule is the starting symbol.","title":"A running example"},{"location":"notes/syntax_analysis/#lexing","text":"Input: Source file in string Output: A sequence of valid tokens according to the language specification (grammar) The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing . Sometimes, a lexer is omitted, as the token validation task can be handled in the parser.","title":"Lexing"},{"location":"notes/syntax_analysis/#lexical-tokens","text":"The set of tokens of a grammar is basically all the terminals. In this JSON grammar example, {i, s, ', [, ], {, }, :, \\, } and white spaces are the Lexical Tokens of the language. If we are to represent it using Scala data types, we could use the following algebraic data type: enum LToken { // lexical Tokens case IntTok(v:Int) case StrTok(v:String) case SQuote case LBracket case RBracket case LBrace case RBrace case Colon case Comma case WhiteSpace } Note that in the above, we find that IntTok and StrTok have semantic components (i.e. the underlying values.) The rest of the tokens do not. Given the input {'k1':1,'k2':[]} the lexer function lex(s:String):List[LToken] should return List(LBRace,SQuote,StrTok(\"k1\"),SQuote,Colon,IntTok(1),Comma,SQuote, StrTok(\"k2\"), Colon,LBracket, RBracket, RBrace) One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows: <<Grammar 2>> (JSON) J ::= I | 'STR' | [] | [IS] | {NS} (Items) IS ::= J,IS | J (Named Objects) NS ::= N,NS | N (Named Object) N ::= 'STR':J (Integer) I ::= dI | d (String) STR ::= aSTR | a where d denotes a single digit and a denotes a single ascii character. For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages.","title":"Lexical Tokens"},{"location":"notes/syntax_analysis/#implementing-a-lexer-using-regular-expression","text":"Perhaps one easier way to implement a lexer is to make use of regular expression.","title":"Implementing a Lexer using Regular Expression"},{"location":"notes/syntax_analysis/#a-simple-example-of-using-scalautilmatchingregex","text":"We can specify a regex pattern as follows. This example was adopted from ( https://www.scala-lang.org/api/3.0.2/scala/util/matching/Regex.html ) val date = raw\"(\\d{4})-(\\d{2})-(\\d{2})\".r Next we can perform a match against the above regex pattern using the match expression. \"2004-01-20\" match { case date(year, month, day) => s\"$year was a good year for PLs.\" } The above expression is evaluated to 2004 was a good year for PLs. We could develop a simple lexer using the above trick. First we define the pattern for reach token. val integer = raw\"(\\d+)(.*)\".r val string = raw\"([^']*)(.*)\".r val squote = raw\"(')(.*)\".r val lbracket = raw\"(\\[)(.*)\".r val rbracket = raw\"(\\])(.*)\".r val lbrace = raw\"(\\{)(.*)\".r val rbrace = raw\"(\\})(.*)\".r val colon = raw\"(:)(.*)\".r val comma = raw\"(,)(.*)\".r For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration. Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned. import LToken.* type Error = String def lex_one(src:String):Either[String, (LToken, String)] = src match { case integer(s, rest) => Right((IntTok(s.toInt), rest)) case squote(_, rest) => Right((SQuote, rest)) case lbracket(_, rest) => Right((LBracket, rest)) case rbracket(_, rest) => Right((RBracket, rest)) case lbrace(_, rest) => Right((LBracket, rest)) case rbrace(_, rest) => Right((RBracket, rest)) case colon(_, rest) => Right((Colon, rest)) case comma(_, rest) => Right((Comma, rest)) case string(s, rest) => Right((StrTok(s), rest)) case _ => Left(s\"lexer error: unexpected token at ${src}\") } Note that the order of the Scala patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern string and the rest except for squote ). Lastly we define the top level lex function by calling lex_one in a recursive function. def lex(src:String):Either[Error, List[LToken]] = { def go(src:String, acc:List[LToken]):Either[Error, List[LToken]] = { if (src.length == 0) { Right(acc) } else { lex_one(src) match { case Left(error) => Left(error) case Right((ltoken, rest)) => go(rest, acc++List(ltoken)) } } } go(src, List()) }","title":"A simple example of using scala.util.matching.Regex"},{"location":"notes/syntax_analysis/#implementing-a-lexer-using-a-parser","text":"In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.) Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems.","title":"Implementing a Lexer using a Parser"},{"location":"notes/syntax_analysis/#parsing","text":"Input: Output from the Lexer Output: A parse tree representing parsed result according to the parse derivation Why tree representation? Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation. Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules.","title":"Parsing"},{"location":"notes/syntax_analysis/#parsing-derivation","text":"Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS. Consider the JSON grammar in its unabridged form, (1) J ::= i (2) J ::= 's' (3) J ::= [] (4) J ::= [IS] (5) J ::= {NS} (6) IS ::= J,IS (7) IS ::= J (8) NS ::= N,NS (9) NS ::= N (10) N ::= 's':J We take the output from our lexer example as the input, with some simplification by removing the Scala constructors { , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , } For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom. Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } ... (for the steps skipped, please refer to syntax_analysis_annex.md) (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS). This algorithm is easy to understand but it has some flaws. It does not terminate when the grammar contains left recursion. It involves some trial-and-error (back-tracking), hence it is not efficient","title":"Parsing Derivation"},{"location":"notes/syntax_analysis/#ambiguous-grammar","text":"A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees. Consider the following <<Grammar 3>> E ::= E + E E ::= E * E E ::= i Consider the input 1 + 2 * 3 . Parsing this input with the above grammar produces graph E-->E1[\"E\"] E-->+ E-->E2[\"E\"] E1-->i1[\"i(1)\"] E2-->E3[\"E\"] E2-->* E2-->E4[\"E\"] E3-->i2[\"i(2)\"] E4-->i3[\"i(3)\"]; or graph E-->E1[\"E\"] E-->* E-->E2[\"E\"] E1-->E3[\"E\"] E1-->+ E1-->E4[\"E\"] E2-->i3[\"i(3)\"] E3-->i1[\"i(1)\"] E4-->i2[\"i(2)\"]; To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that * should bind stronger than + . Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset <<Grammar 4>> E::= T + E E::= T T::= T * F T::= F F::= i As a result, the input 1 + 2 * 3 is parsed as graph E-->T1[\"T\"] E-->+ E-->E1[\"E\"] T1-->F1[\"F\"] F1-->i1[\"i(1)\"] E1-->T2[\"T\"] T2-->F2[\"F\"] F2-->i2[\"i(2)\"] T2-->* T2-->F3[\"F\"] F3-->i3[\"i(3)\"] ;","title":"Ambiguous Grammar"},{"location":"notes/syntax_analysis/#grammar-with-left-recursion","text":"Let's try to run a top-down recursive parsing algorithm over the following grammar <<Grammar 5>> E ::= E + T E ::= T T ::= i i and + are terminals, and E and T are the non-terminals. i denotes an integer. Consider applying the top-down recursive parsing mentioned above to the input 1 , if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation. Let N be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals) Left recursive grammar rules $$ \\begin{array}{rcl} N & ::= & N\\alpha_1 \\ & ... & \\ N & ::= & N\\alpha_n \\ N & ::= & \\beta_1 \\ & ... & \\ N & ::= & \\beta_m \\end{array} $$ can be transformed into \\[ \\begin{array}{rcl} N & ::= & \\beta_1 N' \\\\ & ... & \\\\ N & ::= & \\beta_m N' \\\\ N' & ::= & \\alpha_1 N' \\\\ & ... & \\\\ N' & ::= & \\alpha_n N' \\\\ N' & ::= & \\epsilon \\end{array} \\] Now apply the above to our running example. \\(N\\) is E and \\(\\alpha_1\\) is + T , T is \\(\\beta_1\\) . <<Grammar 6>> E ::= TE' E' ::= + TE' E' ::= epsilon T ::= i The resulting Grammar 6 is equivalent the original Grammar 5. Note that epsilon ( \\(\\epsilon\\) ) is a special terminal which denotes an empty sequence. There are few points to take note For indirect left recursion, some substitution steps are required before applying the above transformation. For instance <<Grammar 7>> G ::= H + G H ::= G + i H ::= i We need to substitute H into the first production rule. <<Grammar 8>> G ::= G + i + G G ::= i + G Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input 1 + 1 with Grammar 6 yields the following parse tree graph E-->T1[\"T\"] E-->Ep1[E'] T1-->i1[\"i(1)\"] Ep1-->+ Ep1-->T2[T] Ep1-->Ep2[E'] T2-->i2[\"i(1)\"] Ep2-->eps1[\u03b5] which needs to be transformed back to graph E-->E1[\"E\"] E-->+ E-->T1[\"T\"] E1-->T2[\"T\"] T1-->i1[\"i(1)\"] T2-->i2[\"i(1)\"]","title":"Grammar with Left Recursion"},{"location":"notes/syntax_analysis/#predictive-recursive-parsing","text":"Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking. In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as LL(k) grammar. Here k refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking. BTW, LL(k) stands for left-to-right, left-most derivation with k tokens look-ahead algorithm. Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\) , \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\) \\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence. $$ \\begin{array}{rcl} null(t,G) & = & false \\ null(\\epsilon,G) & = & true \\ null(N,G) & = & \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\ null(\\sigma_1...\\sigma_n,G) & = & null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} $$ \\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\) . \\[ \\begin{array}{rcl} first(\\epsilon, G) & = & \\{\\} \\\\ first(t,G) & = & \\{t\\} \\\\ first(N,G) & = & \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) & = & \\left [ \\begin{array}{ll} first(\\sigma,G) \\cup first(\\overline{\\sigma},G) & {\\tt if}\\ null(\\sigma,G) \\\\ first(\\sigma,G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] \\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\) . \\[ \\begin{array}{rcl} follow(\\sigma,G) & = & \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G} \\left [ \\begin{array}{ll} first(\\overline{\\gamma}, G) \\cup follow(N,G) & {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\ first(\\overline{\\gamma}, G) & {\\tt otherwise} \\end{array} \\right . \\end{array} \\] Sometimes, for convenience we omit the second parameter \\(G\\) . For example, let \\(G\\) be Grammar 6, then \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) = \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point . That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\) . We will discuss fix-point in-depth in some lesson later. Given \\(null\\) , \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in LL(k) . For simplicity, we check the case k = 1 , we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal. i + E E' T For each production rule \\(N ::= \\overline{\\sigma}\\) , we put the production rule in cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\) cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\) We fill up the table i + E E ::= TE' E' E' ::= + TE' T T ::= i We conclude that a grammar is in LL(1) if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a LL(1) grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol. In general, there are two kinds of conflicts found in grammar that violates the LL(1) grammar requirements. first-first conflict first-follow conflict","title":"Predictive Recursive Parsing"},{"location":"notes/syntax_analysis/#first-first-conflict","text":"Consider the grammar <<Grammar 9>> S ::= Xb S ::= Yc X ::= a Y ::= a We compute \\(null\\) , \\(first\\) and \\(follow\\) . \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] We fill up the following predictive parsing table a b c S S::=Xb, S::=Yc X X::=a Y Y::=a From the above we find that there are two production rules in the cell (S,a) , namely S::=Xb , and S::=Yc . This is a first-first conflict, since both production rules' first set contains a . This prevents us from constructing a predictable parser by observing the leading symbol from the input. First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion). From our running example, we find that the cell (S,a) has more than one production rule applicable. This is caused by the fact that both X::=a and Y::=a start with the non-terminal a . We could apply substitution to eliminate X and Y . <<Grammar 10>> S ::= ab S ::= ac Then we could introduce a new non-terminal Z which capture the following languages after a . <<Grammar 11>> S ::= aZ Z ::= b Z ::= c","title":"First-first Conflict"},{"location":"notes/syntax_analysis/#first-follow-conflict","text":"Consider the following grammar <<Grammar 12>> S ::= Xd X ::= C X ::= Ba C ::= epsilon B ::= d and the \\(null\\) , \\(first\\) and \\(follow\\) functions \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] We construct the predictive parsing table as follows a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d In the cell of (X,d) we find two production rules X::=Ba and X::=C (S::=Xd) . It is a first-follow conflict, because the first production rule is discovered through the first(X) set and the second one is from the follow(X) set. Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts Substitute B and C <<Grammar 13>> S ::= Xd X ::= epsilon X ::= da Substitute X <<Grammar 14>> S ::= d S ::= dad However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict.","title":"First-Follow Conflict"},{"location":"notes/syntax_analysis/#to-ll1-or-not-ll1","text":"Given a grammar, we could get a LL(1) grammar equivalent in most of the cases. Disambiguate the grammar if it is ambiguous Eliminate the left recursion Apply left-factoring if there exists some first-first conflict Apply substitution if there exists some first-follow conflict repeat 3 if first-first conflict is introduced Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm. Let's consider another example (a subset of Grammar 3). <<Grammar 15>> E ::= E + E E ::= i Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion <<Grammar 16>> E ::= iE' E' ::= + EE' E' ::= epsilon Next we compute the predictive parsing table. \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon As shown from the above, the grammar contains a first-follow conflict, therefore it is not a LL(1) . It is not possible to perform substitution to eliminate the first-follow conflict because it will lead to infinite expansion.","title":"To LL(1) or not LL(1)"},{"location":"notes/syntax_analysis/#a-short-summary-so-far-for-top-down-recursive-parsing","text":"Top-down parsing is simple, however might be inefficient. We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists. We need to eliminate left recursion so that the parsing will terminate. We construct the predictive parsing table to check whether the grammar is in LL(k) . If the grammar is in LL(k) we can always pick the right production rule given the first k leading symbols from the input. For most of the cases, LL(1) is sufficient for practical use. We also can conclude that a LL(k+1) grammar is also a LL(k) grammar, but the other way does not hold. Given a particular k and a grammar G , we can check whether G is LL(k) . However given a grammar G to find a k such that G is LL(k) is undecidable.","title":"A short summary so far for top-down recursive parsing"},{"location":"notes/syntax_analysis/#summary","text":"We have covered The roles and functionalities of lexers and parsers in a compiler pipeline There are two major types of parser, top-down parsing and bottom-up parsing How to eliminate left-recursion from a grammar, How to apply left-factoring How to construct a LL(1) predictive parsing table","title":"Summary"},{"location":"notes/syntax_analysis_2/","text":"50.054 - Syntax Analysis 2 Learning Outcome By the end of this lesson, you should be able to Construct a LR(0) parsing table Explain shift-reduce conflict Construct a SLR parsing table Bottom-up parsing An issue with LL(k) parsing is that we always need to make sure that we can pick the correct production rule by examining the first k tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking. What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing. LR(k) stands for left-to-right, right-most derivation with k lookahead tokens. In essence, LR(k) relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser. To understand LR(k) parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.) Let's recall Grammar 6 <<Grammar 6>> 1 S' ::= E$ 2 E ::= TE' 3 E' ::= + TE' 4 E' ::= epsilon 5 T ::= i We added number to each production rule, and we introduce a top level production rule S' ::= E$ where $ denotes the end of input symbol. Let's consider the following parsing table for Grammar 6. + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs, we assume that state IDs are having 2 digits, and state 10 is the starting state. In each cell, we find a set of parsing actions. shift s where s dentes a state ID. Given shift s in a cell ( s' , t ), we change the parser state from s' to s and consume the leading token t from the input and store it in the stack. accept . Given accept found in a cell ( s , $ ), the parsing is completed successfully. goto s where s denotes a state ID. Given goto s in a cell ( s' , t ), we change the parser's state to s . reduce p where p denotes a production rule ID. Given reduce p in a cell ( s , t ), lookup production rule LHS::=RHS from the grammar by p . We pop the items from top of the stack by reversing RHS . Given the state of the current top element of the stack, let's say s' , we lookup the goto action in cell ( s' , LHS ) and push LHS to the stack and perform the goto action. Consider the parsing the input 1+2+3 stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ We start with state (10) in the stack. Given the first token from the input is 1 (i.e. an i token), we look up the parsing table and find the shift 13 action in cell ( 10 , i ). By executing this action, we push i(13) in the stack. The next input is + . Given the current state is (13), we apply the smae strategy to find action reduce 5 in cell ( 13 , + ). Recall that the production rule with id 5 is T::=i , we pop the i(13) from the stack, and check for the correspondent action in cell ( 10 , T ), we find goto 11 . Hence we push T(11) into the stack. We follow the remaining steps to parse the input when we meet the accept action. One interesting observation is that the order of the rules found in the rule column is the reverse order of the list of rules we used in LL(k) parsing. Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for LR(k) grammars. LR(0) Parsing We first consider the simplest parsing table where we ignore the leading token from the input, LR(0) . The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine. From this point onwards, we use pseudo Scala syntax illustrate the algorithm behind the parsing table construction. Let . denote a meta symbol which indicate the current parsing context in a production rule. For instance for production rule 3 E' ::= +TE' , we have four possible contexts E' ::= .+TE' E' ::= +.TE' E' ::= +T.E' E' ::= +TE'. We call each of these possible contexts an Item . We define Items to be a set of Item s, Grammar to be a set of production rules (whose definition is omitted, we use the syntax LHS::=RHS directly in the pseudo-code.) type Items = Set[Item] type Grammar = Set[Prod] We consider the following operations. def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta) <- I (X ::= gamma) <- G } yield ( X::= . gamma ).union( for { (N ::= . epsilon ) <- I } yield ( N::= epsilon .) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta) <- I } yield (N ::= alpha X . beta) closure(J)(G) } Function closure takes an item set I and a grammar then returns the closure of I . For each item of shape N::=alpha . X beta in I , we look for the correspondent production rule X ::= gamma in G if X is a non-terminal, add X::= . gamma to the new item sets if it is not yet included in I . Noe that Scala by default does not support pattern such as (N ::= alpha . X beta) and (X::= gamma) . In this section, let's pretend that these patterns are allowed in Scala so that we can explain the algorithm in Scala style pseudo-code. Function goto takes an item set I and searches for item inside of shape N::= alpha . X beta then add N::=alpha X. beta as the next set J . We compute the closure of J and return it as result. type State = Items type Transition = (State, Symbol, State) case class StateMachine(states:Set[State], transitions:Set[Transition], accepts:Set[State]) def buildSM(init:State)(G:Grammar):StateMachine = { def step1(states:Set[State])(trans:Set[Transition]):(Set[State], Set[Transition]) = { // compute all states and transitions val newStateTrans = for { I <- states (A ::= alpha . X beta) <- I J <- pure(goto(I)(G)(X)) } yield (J, (I,X,J)) if newStateTrans.forall( st => st match { case (new_state, _) => states.contains(new_state) }) { (states, trans) } else { val newStates = newStateTrans.map( x => x._1) val newTrans = newStateTrans.map( x => x._2) step1(states.union(newStates))(trans.union(newTrans)) } } def step2(states:Set[State]):Set[State] = { // compute all final states states.filter( I => I.exists( item => item match { case (N ::= alpha . $) => true case _ => false })) } step1(Set(init))(Set()) match { case (states, trans) => { val finals = step2(states) StateMachine(states, trans, finals) } } } Function buildSM consists of two steps. In step1 we start with the initial state init and compute all possible states and transitions by applying goto . In step2 , we compute all final states. By applying buildSM to Grammar 6 yields the following state diagram. graph State10[\"State10 <br/> S'::=.E$ <br/> E::=.TE' <br/> T::=i \"]--T-->State11[\"State11 <br/> E::=T.E' <br/> E'::=+TE' <br/> E'::= . epsilon <br/> E'::= epsilon.\"] State10--E-->State12[\"State12 <br/> S'::=E.$\"] State10--i-->State13[\"State13 <br/> T::=i.\"] State11--+-->State14[\"State14 <br/> E'::= +.TE' <br/> T::=.i\"] State11--E'-->State17[\"State17 <br/> S'::=E.$\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E' <br/> E'::=.+TE' <br/> E'::=.epsilon <br/> E'::=epsilon . \"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'.\"] def reduce(states:List[State]):List[(Items, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items,Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a.append(List((I, N::=alpha))) case (a, _) => a } } } Function reduce takes a list of states and search for item set that contains an item of shape N::= alpha . . enum Action { case Shift(i:State) case Reduce(p:Prod) case Accept case Goto(i:State) } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, N::=alpha) <- reduce(states) x <- allTerminals(G) } yield (I, x, Reduce(N::=alpha)) val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Function ptable computes the LR(0) parsing table by making use of the functions defined earlier. Applying ptable to Grammar 6 yields + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol ( shift ) and a non-terminal symbol ( goto ). SLR parsing One issue with the above LR(0) parsing table is that we see conflicts in cells with multiple actions, e.g. cell ( 11 , + ). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the ptable function. In the ptable function, we blindly assign reduce actions to current state w.r.t. to all symbols. A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal. def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items, Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a ++ (follow(N).map( s => (I, s N::=alpha))) // fix case (a, _) => a } } } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, x, N::=alpha) <- reduce(states) } yield (I, x, Reduce(N::=alpha)) // fix val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section. LR(1) Parsing (Bonus materials) Besides SLR , LR(1) parsing also eliminates many conflicts found in LR(0) . The idea is to re-define item to include the look ahead token. For instance for production rule 3 E' ::= +TE' , we have 12 possible items ( E' ::= .+TE' , + ) ( E' ::= +.TE' , + ) ( E' ::= +T.E' , + ) ( E' ::= +TE'. , + ) ( E' ::= .+TE' , i ) ( E' ::= +.TE' , i ) ( E' ::= +T.E' , i ) ( E' ::= +TE'. , i ) ( E' ::= .+TE' , $ ) ( E' ::= +.TE' , $ ) ( E' ::= +T.E' , $ ) ( E' ::= +TE'. , $ ) We adjust the definition of closure and goto def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta, t) <- I (X ::= gamma) <- G w <- first(beta t) } yield ( X::= . gamma, w).union( for { (N ::= . epsilon, t) <- I } yield ( N::= epsilon ., t) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta, t) <- I } yield (N ::= alpha X . beta, t) closure(J)(G) } When computing the closure of an item (N ::= alpha . X beta, t) , we look up production rule X ::= gamma , to add X ::= .gamma into the new item set, we need to consider the possible leading terminal tokens coming from beta , and t in case beta accepts epsilon. Applying the adjusted definition, we have the follow state diagram graph State10[\"State10 <br/> S'::=.E$, ? <br/> E::=.TE', $ <br/> T::=i, $ <br/> T::=i, + \"]--T-->State11[\"State11 <br/> E::=T.E', $ <br/> E'::=+TE', $ <br/> E'::= . epsilon, $ <br/> E'::= epsilon., $\"] State10--E-->State12[\"State12 <br/> S'::=E.$, ?\"] State10--i-->State13[\"State13 <br/> T::=i., + <br/> T::=i., $\"] State11--+-->State14[\"State14 <br/> E'::= +.TE', $ <br/> T::=.i, + \"] State11--E'-->State17[\"State17 <br/> S'::=E.$, ?\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E', $ <br/> E'::=.+TE', $ <br/> E'::=.epsilon, $ <br/> E'::=epsilon., $\"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'., $\"] For the top-most production rule, there is no leading token, we put a special symbol ? , which does not affect the parsing. To incorporate item's new definition, we adjust the reduce function as follows def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha ., t)) => a.append(List((I, t, N::=alpha))) case (a, _) => a } } } buildSM and ptable function remain unchanged as per SLR parsing. By applying ptable we obtain the same parsing table as SLR parsing. SLR vs LR(1) LR(1) covers a larger set of grammar than SLR . For example consider the following grammar. <<Grammar 16>> 1 S' ::= S$ 2 S ::= A a 3 S ::= b A c 4 S ::= d c 5 S ::= b d a 6 A ::= d SLR produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$<br/> S::=.Aa <br/> S::= .bAc<br/> S::=.dc <br/> S::=.bda <br/> A::=.d \"]--S-->State11[\"State11 <br/> S'::=S.$\"] State10--A-->State12[\"State12 <br/> S::A.a\"] State10--b-->State13[\"State13 <br/> S::=b.Ac<br/> S::=b.da <br/> A::=.d\"] State10--d-->State14[\"State14 <br/> S::= d.c <br/> A::=d.\"] State12--a-->State15[\"State15 <br/> S::=Aa.\"] State13--A-->State16[\"State16 <br/> S::=bA.c\"] State13--d-->State17[\"State17 <br/> A::=d. <br/> S::=bd.a\"] State14--c-->State18[\"State18 <br/> S::=dc.\"] State16--c-->State19[\"State19 <br/> S::=bAc.\"] State17--a-->State20[\"State20 <br/> S::=bda.\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 There exist shift-reduce conflict. This is because in the closure computation when the item X::= . gamma is added to the closure, we approximate the next leading token by follow(X) . However there might be other alternative production rule for X in the grammar. This introduces extraneous reduce actions. LR(1) produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$, ? <br/> S::=.Aa, $ <br/> S::= .bAc, $ <br/> S::=.dc, $ <br/> S::=.bda, $ <br/> A::=.d, a \"]--S-->State11[\"State11 <br/> S'::=S.$, ?\"] State10--A-->State12[\"State12 <br/> S::A.a, $\"] State10--b-->State13[\"State13 <br/> S::=b.Ac, $<br/> S::=b.da, $ <br/> A::=.d, c\"] State10--d-->State14[\"State14 <br/> S::= d.c, $ <br/> A::=d., a\"] State12--a-->State15[\"State15 <br/> S::=Aa., $\"] State13--A-->State16[\"State16 <br/> S::=bA.c,$\"] State13--d-->State17[\"State17 <br/> A::=d.,c <br/> S::=bd.a, $\"] State14--c-->State18[\"State18 <br/> S::=dc., $\"] State16--c-->State19[\"State19 <br/> S::=bAc., $\"] State17--a-->State20[\"State20 <br/> S::=bda., $\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 In which the shift-reduce conflicts are eliminated because when given an item (N ::= alpha . X beta, t) , we add X ::= . gamma into the closure, by computing first(beta t) . This is only specific to this production rule X::= gamma and not other alternative. LR(1) and left recursion LR(1) can't handle all grammar with left recursion. For example processing Grammar 5 (from the previous unit) with LR(1) will result in some shift-reduce conflict. Summary We have covered How to construct a LR(0) parsing table How to construct a SLR parsing table","title":"50.054 - Syntax Analysis 2"},{"location":"notes/syntax_analysis_2/#50054-syntax-analysis-2","text":"","title":"50.054 - Syntax Analysis 2"},{"location":"notes/syntax_analysis_2/#learning-outcome","text":"By the end of this lesson, you should be able to Construct a LR(0) parsing table Explain shift-reduce conflict Construct a SLR parsing table","title":"Learning Outcome"},{"location":"notes/syntax_analysis_2/#bottom-up-parsing","text":"An issue with LL(k) parsing is that we always need to make sure that we can pick the correct production rule by examining the first k tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking. What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing. LR(k) stands for left-to-right, right-most derivation with k lookahead tokens. In essence, LR(k) relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser. To understand LR(k) parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.) Let's recall Grammar 6 <<Grammar 6>> 1 S' ::= E$ 2 E ::= TE' 3 E' ::= + TE' 4 E' ::= epsilon 5 T ::= i We added number to each production rule, and we introduce a top level production rule S' ::= E$ where $ denotes the end of input symbol. Let's consider the following parsing table for Grammar 6. + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs, we assume that state IDs are having 2 digits, and state 10 is the starting state. In each cell, we find a set of parsing actions. shift s where s dentes a state ID. Given shift s in a cell ( s' , t ), we change the parser state from s' to s and consume the leading token t from the input and store it in the stack. accept . Given accept found in a cell ( s , $ ), the parsing is completed successfully. goto s where s denotes a state ID. Given goto s in a cell ( s' , t ), we change the parser's state to s . reduce p where p denotes a production rule ID. Given reduce p in a cell ( s , t ), lookup production rule LHS::=RHS from the grammar by p . We pop the items from top of the stack by reversing RHS . Given the state of the current top element of the stack, let's say s' , we lookup the goto action in cell ( s' , LHS ) and push LHS to the stack and perform the goto action. Consider the parsing the input 1+2+3 stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ We start with state (10) in the stack. Given the first token from the input is 1 (i.e. an i token), we look up the parsing table and find the shift 13 action in cell ( 10 , i ). By executing this action, we push i(13) in the stack. The next input is + . Given the current state is (13), we apply the smae strategy to find action reduce 5 in cell ( 13 , + ). Recall that the production rule with id 5 is T::=i , we pop the i(13) from the stack, and check for the correspondent action in cell ( 10 , T ), we find goto 11 . Hence we push T(11) into the stack. We follow the remaining steps to parse the input when we meet the accept action. One interesting observation is that the order of the rules found in the rule column is the reverse order of the list of rules we used in LL(k) parsing. Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for LR(k) grammars.","title":"Bottom-up parsing"},{"location":"notes/syntax_analysis_2/#lr0-parsing","text":"We first consider the simplest parsing table where we ignore the leading token from the input, LR(0) . The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine. From this point onwards, we use pseudo Scala syntax illustrate the algorithm behind the parsing table construction. Let . denote a meta symbol which indicate the current parsing context in a production rule. For instance for production rule 3 E' ::= +TE' , we have four possible contexts E' ::= .+TE' E' ::= +.TE' E' ::= +T.E' E' ::= +TE'. We call each of these possible contexts an Item . We define Items to be a set of Item s, Grammar to be a set of production rules (whose definition is omitted, we use the syntax LHS::=RHS directly in the pseudo-code.) type Items = Set[Item] type Grammar = Set[Prod] We consider the following operations. def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta) <- I (X ::= gamma) <- G } yield ( X::= . gamma ).union( for { (N ::= . epsilon ) <- I } yield ( N::= epsilon .) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta) <- I } yield (N ::= alpha X . beta) closure(J)(G) } Function closure takes an item set I and a grammar then returns the closure of I . For each item of shape N::=alpha . X beta in I , we look for the correspondent production rule X ::= gamma in G if X is a non-terminal, add X::= . gamma to the new item sets if it is not yet included in I . Noe that Scala by default does not support pattern such as (N ::= alpha . X beta) and (X::= gamma) . In this section, let's pretend that these patterns are allowed in Scala so that we can explain the algorithm in Scala style pseudo-code. Function goto takes an item set I and searches for item inside of shape N::= alpha . X beta then add N::=alpha X. beta as the next set J . We compute the closure of J and return it as result. type State = Items type Transition = (State, Symbol, State) case class StateMachine(states:Set[State], transitions:Set[Transition], accepts:Set[State]) def buildSM(init:State)(G:Grammar):StateMachine = { def step1(states:Set[State])(trans:Set[Transition]):(Set[State], Set[Transition]) = { // compute all states and transitions val newStateTrans = for { I <- states (A ::= alpha . X beta) <- I J <- pure(goto(I)(G)(X)) } yield (J, (I,X,J)) if newStateTrans.forall( st => st match { case (new_state, _) => states.contains(new_state) }) { (states, trans) } else { val newStates = newStateTrans.map( x => x._1) val newTrans = newStateTrans.map( x => x._2) step1(states.union(newStates))(trans.union(newTrans)) } } def step2(states:Set[State]):Set[State] = { // compute all final states states.filter( I => I.exists( item => item match { case (N ::= alpha . $) => true case _ => false })) } step1(Set(init))(Set()) match { case (states, trans) => { val finals = step2(states) StateMachine(states, trans, finals) } } } Function buildSM consists of two steps. In step1 we start with the initial state init and compute all possible states and transitions by applying goto . In step2 , we compute all final states. By applying buildSM to Grammar 6 yields the following state diagram. graph State10[\"State10 <br/> S'::=.E$ <br/> E::=.TE' <br/> T::=i \"]--T-->State11[\"State11 <br/> E::=T.E' <br/> E'::=+TE' <br/> E'::= . epsilon <br/> E'::= epsilon.\"] State10--E-->State12[\"State12 <br/> S'::=E.$\"] State10--i-->State13[\"State13 <br/> T::=i.\"] State11--+-->State14[\"State14 <br/> E'::= +.TE' <br/> T::=.i\"] State11--E'-->State17[\"State17 <br/> S'::=E.$\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E' <br/> E'::=.+TE' <br/> E'::=.epsilon <br/> E'::=epsilon . \"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'.\"] def reduce(states:List[State]):List[(Items, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items,Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a.append(List((I, N::=alpha))) case (a, _) => a } } } Function reduce takes a list of states and search for item set that contains an item of shape N::= alpha . . enum Action { case Shift(i:State) case Reduce(p:Prod) case Accept case Goto(i:State) } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, N::=alpha) <- reduce(states) x <- allTerminals(G) } yield (I, x, Reduce(N::=alpha)) val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Function ptable computes the LR(0) parsing table by making use of the functions defined earlier. Applying ptable to Grammar 6 yields + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol ( shift ) and a non-terminal symbol ( goto ).","title":"LR(0) Parsing"},{"location":"notes/syntax_analysis_2/#slr-parsing","text":"One issue with the above LR(0) parsing table is that we see conflicts in cells with multiple actions, e.g. cell ( 11 , + ). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the ptable function. In the ptable function, we blindly assign reduce actions to current state w.r.t. to all symbols. A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal. def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items, Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha .)) => a ++ (follow(N).map( s => (I, s N::=alpha))) // fix case (a, _) => a } } } def ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match { case (S::= X$) => { val init = Set(closure(Set(S ::=.X$))(G)) buildSM(init)(G) match { case StateMachine(states, trans, finals) => { val shifts = for { (I, x, J) <- trans if isTerminal(x) } yield (I, x, Shift(J)) val gotos = for { (I, x, J) <- trans if !isTerminal(x) yield (I, x, Goto(J))) } val reduces = for { (I, x, N::=alpha) <- reduce(states) } yield (I, x, Reduce(N::=alpha)) // fix val accepts = for { I <- finals } yield (I, $, Accept) shifts ++ gotos ++ reduces ++ accepts } } } } Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section.","title":"SLR parsing"},{"location":"notes/syntax_analysis_2/#lr1-parsing-bonus-materials","text":"Besides SLR , LR(1) parsing also eliminates many conflicts found in LR(0) . The idea is to re-define item to include the look ahead token. For instance for production rule 3 E' ::= +TE' , we have 12 possible items ( E' ::= .+TE' , + ) ( E' ::= +.TE' , + ) ( E' ::= +T.E' , + ) ( E' ::= +TE'. , + ) ( E' ::= .+TE' , i ) ( E' ::= +.TE' , i ) ( E' ::= +T.E' , i ) ( E' ::= +TE'. , i ) ( E' ::= .+TE' , $ ) ( E' ::= +.TE' , $ ) ( E' ::= +T.E' , $ ) ( E' ::= +TE'. , $ ) We adjust the definition of closure and goto def closure(I:Items)(G:Grammar):Items = { val newItems = for { (N ::= alpha . X beta, t) <- I (X ::= gamma) <- G w <- first(beta t) } yield ( X::= . gamma, w).union( for { (N ::= . epsilon, t) <- I } yield ( N::= epsilon ., t) ) if (newItems.forall(newItem => I.contains(newItem))) { I } else { closure(I.union(newItems))(G)} def goto(I:Items)(G:Grammar)(sym:Symbol):Items = { val J = for { (N ::= alpha . X beta, t) <- I } yield (N ::= alpha X . beta, t) closure(J)(G) } When computing the closure of an item (N ::= alpha . X beta, t) , we look up production rule X ::= gamma , to add X ::= .gamma into the new item set, we need to consider the possible leading terminal tokens coming from beta , and t in case beta accepts epsilon. Applying the adjusted definition, we have the follow state diagram graph State10[\"State10 <br/> S'::=.E$, ? <br/> E::=.TE', $ <br/> T::=i, $ <br/> T::=i, + \"]--T-->State11[\"State11 <br/> E::=T.E', $ <br/> E'::=+TE', $ <br/> E'::= . epsilon, $ <br/> E'::= epsilon., $\"] State10--E-->State12[\"State12 <br/> S'::=E.$, ?\"] State10--i-->State13[\"State13 <br/> T::=i., + <br/> T::=i., $\"] State11--+-->State14[\"State14 <br/> E'::= +.TE', $ <br/> T::=.i, + \"] State11--E'-->State17[\"State17 <br/> S'::=E.$, ?\"] State14--i-->State13 State14--T-->State15[\"State15 <br/> E'::= +T.E', $ <br/> E'::=.+TE', $ <br/> E'::=.epsilon, $ <br/> E'::=epsilon., $\"] State15--+-->State14 State15--E'-->State16[\"State16 <br/> E'::=+TE'., $\"] For the top-most production rule, there is no leading token, we put a special symbol ? , which does not affect the parsing. To incorporate item's new definition, we adjust the reduce function as follows def reduce(states:List[State]):List[(Items, Symbol, Prod)] = { states.foldLeft(List())((accI:(List[(Items,Symbol, Prod)], Items)) => accI match { case (acc,I) => I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) => ai match { case (a, ( N::= alpha ., t)) => a.append(List((I, t, N::=alpha))) case (a, _) => a } } } buildSM and ptable function remain unchanged as per SLR parsing. By applying ptable we obtain the same parsing table as SLR parsing.","title":"LR(1) Parsing (Bonus materials)"},{"location":"notes/syntax_analysis_2/#slr-vs-lr1","text":"LR(1) covers a larger set of grammar than SLR . For example consider the following grammar. <<Grammar 16>> 1 S' ::= S$ 2 S ::= A a 3 S ::= b A c 4 S ::= d c 5 S ::= b d a 6 A ::= d SLR produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$<br/> S::=.Aa <br/> S::= .bAc<br/> S::=.dc <br/> S::=.bda <br/> A::=.d \"]--S-->State11[\"State11 <br/> S'::=S.$\"] State10--A-->State12[\"State12 <br/> S::A.a\"] State10--b-->State13[\"State13 <br/> S::=b.Ac<br/> S::=b.da <br/> A::=.d\"] State10--d-->State14[\"State14 <br/> S::= d.c <br/> A::=d.\"] State12--a-->State15[\"State15 <br/> S::=Aa.\"] State13--A-->State16[\"State16 <br/> S::=bA.c\"] State13--d-->State17[\"State17 <br/> A::=d. <br/> S::=bd.a\"] State14--c-->State18[\"State18 <br/> S::=dc.\"] State16--c-->State19[\"State19 <br/> S::=bAc.\"] State17--a-->State20[\"State20 <br/> S::=bda.\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 There exist shift-reduce conflict. This is because in the closure computation when the item X::= . gamma is added to the closure, we approximate the next leading token by follow(X) . However there might be other alternative production rule for X in the grammar. This introduces extraneous reduce actions. LR(1) produces the following state diagram and parsing table. graph State10[\"State10 <br/> S'::=.S$, ? <br/> S::=.Aa, $ <br/> S::= .bAc, $ <br/> S::=.dc, $ <br/> S::=.bda, $ <br/> A::=.d, a \"]--S-->State11[\"State11 <br/> S'::=S.$, ?\"] State10--A-->State12[\"State12 <br/> S::A.a, $\"] State10--b-->State13[\"State13 <br/> S::=b.Ac, $<br/> S::=b.da, $ <br/> A::=.d, c\"] State10--d-->State14[\"State14 <br/> S::= d.c, $ <br/> A::=d., a\"] State12--a-->State15[\"State15 <br/> S::=Aa., $\"] State13--A-->State16[\"State16 <br/> S::=bA.c,$\"] State13--d-->State17[\"State17 <br/> A::=d.,c <br/> S::=bd.a, $\"] State14--c-->State18[\"State18 <br/> S::=dc., $\"] State16--c-->State19[\"State19 <br/> S::=bAc., $\"] State17--a-->State20[\"State20 <br/> S::=bda., $\"] a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 In which the shift-reduce conflicts are eliminated because when given an item (N ::= alpha . X beta, t) , we add X ::= . gamma into the closure, by computing first(beta t) . This is only specific to this production rule X::= gamma and not other alternative.","title":"SLR vs LR(1)"},{"location":"notes/syntax_analysis_2/#lr1-and-left-recursion","text":"LR(1) can't handle all grammar with left recursion. For example processing Grammar 5 (from the previous unit) with LR(1) will result in some shift-reduce conflict.","title":"LR(1) and left recursion"},{"location":"notes/syntax_analysis_2/#summary","text":"We have covered How to construct a LR(0) parsing table How to construct a SLR parsing table","title":"Summary"},{"location":"notes/syntax_analysis_annex/","text":"Rule Parse tree Symbols Input (5) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] { NS } { ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (8) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N,NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } (10) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->s N-->RQ['] N-->: N-->J2[J] ' s':J, NS } ' k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->s N-->RQ['] N-->: N-->J2[J] s ':J, NS } k 1 ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] ' :J, NS } ' : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] : J, NS } : 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J]; J, NS } 1 , ' k 2 ' : [ ] } (1) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i; i , NS } 1 , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"]; , NS } , ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"]; NS } ' k 2 ' : [ ] } (9) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N]; N } ' k 2 ' : [ ] } (10) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[s] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; ' s':J } ' k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[s] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; s ':J } k 2 ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; ' :J } ' : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; : J } : [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J]; J } [ ] } (3) graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"]; [ ] } [ ] } graph J-->LB[\"{\"] J-->NS J-->RB[\"}\"] NS-->N NS-->, NS-->NS2[\"NS\"] N-->LQ['] N-->S1(\"s(k1)\") N-->RQ['] N-->: N-->J2[J] J2-->i[\"i(1)\"] NS2-->N2[N] N2-->LQ2['] N2-->S2[\"s(k2)\"] N2-->RQ2['] N2-->CL2[\":\"] N2-->J3[J] J3-->LSQ[\"[\"] J3-->RSQ[\"]\"];","title":"Syntax analysis annex"}]}